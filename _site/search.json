[
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/learning-r.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/learning-r.html",
    "title": "Learning R",
    "section": "",
    "text": "Data science is an exciting discipline that allows you to transform raw data into understanding, insight, and knowledge.\n\n— Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund in R for Data Science\nA data scientist is someone who creates understanding, insight, and knowledge from raw data with programming. Programming is an essential tool in nearly every part of a data science project because it allows you to do data science efficiently and reproducibly.\nThere are many different programming languages you can use to do data science, but here we cover my favourite programming language: R."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#whats-r",
    "href": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#whats-r",
    "title": "Learning R",
    "section": "What’s R?",
    "text": "What’s R?\nR is an open source programming language for wrangling, visualizing, modelling, and communicating data, and so much more. It has a strong community behind it and is widely used among researchers, statisticians, and data scientists in a variety of fields."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#where-do-i-start",
    "href": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#where-do-i-start",
    "title": "Learning R",
    "section": "Where do I start?",
    "text": "Where do I start?\nI believe every R user should work through these two books (in order):\n\nHands-On Programming with R by Garrett Grolemund\nR for Data Science by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund\n\nTogether these two books teach you a core set of tools that you can use to accomplish the vast majority of tasks in any data science project:\n\nHands-On Programming with R is a short and friendly introduction to the fundamentals of R programming written for non-programmers who want to become data scientists.\nR for Data Science is a friendly introduction to the tools useful for doing data science with R.\n\nAs a companion to these two books I also recommend reading the tidyverse style guide, which provides a style guide for writing R code that is easy to read.\n\n\n\n\n\n\nHelping yourself\n\n\n\nPart of the challenge of learning a programming language is building up a vocabulary of commands to accomplish different tasks. Posit (formerly RStudio) maintains a number of cheat sheets to remind you how to do common tasks in your favourite R packages.\n\n\nFinally, I recommend learning to write reports and presentations in Quarto, a tool for integrating prose, code, and results into reproducible documents. This is covered in the Communicate sections of R for Data Science, but I’d like to emphasize it here because it’s a useful, beginner-friendly skill that goes a long way.\nQuarto is a successor to R Markdown that has ironed-out some of the friction points to make writing reproducible documents an even better experience than it already was. Because Quarto is relatively new, you might find R Markdown currently supports certain use cases better (like APA style manuscripts). In cases like these it’s perfectly fine to use R Markdown; it isn’t going away and is still actively maintained.\nIf you do use R Markdown, the Communicate sections of the first edition of R for Data Science provide a a short and friendly introduction, and the R Markdown: The Definitive Guide and R Markdown Cookbook books provide more comprehensive coverage."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#what-else-should-i-learn",
    "href": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#what-else-should-i-learn",
    "title": "Learning R",
    "section": "What else should I learn?",
    "text": "What else should I learn?\nHands-On Programming with R and R for Data Science provide an excellent foundation in R programming for data science, but there are a number of topics these books don’t cover that are equally important for doing data science successfully:\n\nContent knowledge\nInterpersonal skills\nResearch skills\nStatistical modelling\n\nHaving content knowledge about the problem you are using data science to answer allows you to ask better questions, identify data problems, and develop solutions that are meaningful to your audience. Content knowledge is something you can build over time through experience, interactions with the people affected by the problem you are trying to answer, books and courses, and so forth. You don’t always need to be an expert on the problem at hand, but learning more about it will help you avoid mistakes, build confidence in your solutions, and connect with your audience.\nThis also underscores the importance of interpersonal skills for practicing data scientists. Data science problems are ultimately people problems: Much of our data is about people. All of our data is communicated to people. And the solutions we develop using data science will only make an impact if our audience chooses to adopt them. You have to speak for the data, because the data doesn’t speak for itself. Developing strong leadership, teamwork, empathy, and communication skills will help you navigate the human side of data science.\nComplimenting content knowledge and interpersonal skills are research skills and statistical modelling, which are embodied in the term data science. Research skills such as observation, measurement, experiment design, and survey design allow us to collect data that addresses a problem we care about; and statistical modelling helps us transform that raw data into understanding, insight, and knowledge through estimation and testing. Research and statistics, along with programming, are core skills in data science that can be challenging to learn. Partly because developing these skills takes time, practice, and humility. And partly because pedagogy on research and statistics—whether in books or courses—has a lot of gaps for people who aren’t “real” statisticians.1"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#how-do-i-learn-statistics",
    "href": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#how-do-i-learn-statistics",
    "title": "Learning R",
    "section": "How do I learn statistics?",
    "text": "How do I learn statistics?\nMy answer to this question makes the following assumptions about you:\n\nYou are not a formally trained statistician\nYou have received some prior training in statistics\nYou have (re-)discovered your own ignorance of statistics\nYou want to address your ignorance of statistics\nYou want to use statistics to solve problems\n\nIf this describes you then I hope the resources that follow help you on your journey like they have for me. If this doesn’t describe you, stick around anyway, you might find something new.\nLet’s start with some book recommendations:\n\nRegression and Other Stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari (Source)\nData Analysis Using Regression and Multilevel/Hierarchical Models by Andrew Gelman and Jennifer Hill2\nBayes Rules! by Alicia A. Johnson, Miles Q. Ott, and Mine Dogucu (GitHub)\nAn introduction to statistical learning with applications in R by Gareth James, Daniela Witten, Trevor Hastie, and Rob Tibshirani\nStatistical Rethinking by Richard McElreath\nImproving Your Statistical Inferences by Daniël Lakens\n\nAll of these books are well-written, engaging, and have examples in R.3 Together they cover applications of fundamental and state-of-the-art tools in statistical modelling. But most importantly—they encourage statistical thinking. They’re ideal books for self-study and I can’t recommend them enough.\n\n\n\n\n\n\nAll you need is the linear model\n\n\n\nMost people have the unfortunate experience of learning statistics through arcane rituals with numerous statistical tests that appear to have no fundamental framework to tie them together. This is a failure of pedagogy rather than a feature of statistics. Fortunately, there is a fundamental framework to tie all these statistical tests together: It’s called the linear model.\nMost common statistical tests are either special cases or extensions of the linear model; understanding this will improve your statistical thinking and make it easier to abandon the arcanum and statistical rituals in favour of a unified statistical framework.\nFor a short introduction to this concept I recommend reading Common statistical tests are linear models (or: how to teach stats) by Jonas Kristoffer Lindeløv (note: there are a few small errors that haven’t been fixed, so also see the GitHub Issues).\n\n\nI’d like to also give honourable mentions to the following books, which have all the great qualities of the books above, but cover more specialized topics:\n\nHandbook of Graphs and Networks in People Analytics: With Examples in R and Python by Keith McNulty (Source)\nDoing Meta-Analysis in R: A Hands-on Guide by Mathias Harrer, Pim Cuijpers, Toshi A. Furukawa, and David D. Ebert (Source)\nText Mining with R by Julia Silge and David Robinson (Source)\n\nThese are just a few choice examples. There are a lot of statistics books written for R users, and you can almost always find a book covering whatever topic you’re interested in.\n\n\n\n\n\n\nSocial media\n\n\n\nThere are strong #RStats communities on most social media platforms, where you can discover new people and follow or participate in conversations about R and statistics. The #RStats community on Mastodon (formerly Twitter; RIP) has been an invaluable learning tool and helped me discover things about R and statistics I wouldn’t have on my own. Frank Harrell also created the Data Methods Discussion Forum to provide a place for longer more in-depth discussions. Finally, Stack Overflow and Cross Validated are great public Q&A platforms for R programming and statistics, respectively.\n\n\nA lot of people in the R community also have their own R programming, data science, and statistics blogs, vlogs, or websites. Some of my favourite authors include:\n\nAndrew Heiss\nA. Solomon Kurz\nMichael Clark\nJulia Silge\n\nStarting your own blog or website is also a great way to learn, and gives you a place to share your work! Quarto, the open-source scientific and technical publishing system, makes the process of creating and publishing a website simple and friendly. It’s what I use for Tidy Tales (and lots of other projects).\nFinally, a lot of people in the R community who have taught courses or workshops make their materials openly available (including myself and some of the people listed above). One source I want to highlight in particular is psyTeachR from the University of Glasgow School of Psychology and Neuroscience, which covers an entire curriculum of courses for doing reproducible data science."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#where-do-i-learn-more-about-r-programming",
    "href": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#where-do-i-learn-more-about-r-programming",
    "title": "Learning R",
    "section": "Where do I learn more about R programming?",
    "text": "Where do I learn more about R programming?\n\nYou don’t need to be an expert programmer to be a successful data scientist, but learning more about programming pays off, because becoming a better programmer allows you to automate common tasks, and solve new problems with greater ease.\n\n— Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund in R for Data Science\nIf you want to become a better R programmer, I think these books are a good place to start:\n\nR Packages by Hadley Wickham and Jenny Bryan\nAdvanced R by Hadley Wickham\nggplot2: Elegant Graphics for Data Analysis by Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen\nMastering Shiny by Hadley Wickham\n\nR packages is a friendly introduction teaching you how to develop and publish your own R packages.\nAdvanced R is written for intermediate R users who want to improve their programming skills and understanding of the language, teaching you useful tools, techniques, and idioms that can help solve many types of problems.\nggplot2: Elegant Graphics for Data Analysis is written for R users who want to understand the details of the theory underlying the ggplot2 R package, teaching you the elements of ggplot2’s grammar and how they fit together, and giving you the power to tailor any plot specifically to your needs.\nMastering Shiny is a comprehensive introduction teaching you how to easily create rich, interactive web apps with the shiny R package.\n\n\n\n\n\n\nThe R Manuals\n\n\n\n\n\nThe R Manuals are manuals for the R language written by the R Development Core team. I mention them here because it’s good to know about them, but the book recommendations I’ve already made largely cover the contents of these manuals in a friendlier way. Posit also maintain nicely formatted HTML versions of the manuals."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#how-do-i-make-my-work-reproducible",
    "href": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#how-do-i-make-my-work-reproducible",
    "title": "Learning R",
    "section": "How do I make my work reproducible?",
    "text": "How do I make my work reproducible?\nThis series is called Reproducible Data Science, so I should probably talk more about that. If you’ve learned even a portion of what’s covered above then you already have a lot of the skills needed to do reproducible data science; but you are likely missing out on some essential tools and a stable framework for making reproducible data products.\nA data product is the combination of data with code that wrangles, visualizes, models, or communicates data, whose outputs will be shared with some end-user. Some common examples of data products are quarterly earnings presentations, scientific papers, and interactive dashboards. For these examples the end-user might be your coworkers, other scientists, or members of the public, respectively. You will even be the end-user of your data products sometimes. Because data products will be used by others (or yourself), it’s good practice to do quality assurance so your end-users (that includes you!) can be confident in the quality of your data product. Making your data products reproducible is one small but important step you can take to ensure they meet the expectations of your end-user.4\n\n\n\n\n\n\nShades of reproducibility\n\n\n\nThe basic idea behind a reproducible data product is that the steps, processes, and procedures that went into making it can be repeated exactly by yourself and others, resulting in the exact same outcome every time. Ideally, there is no expiration date for reproducibility—the reproducibility of a data product could be tested tomorrow or in ten decades and should give the exact same outcome both times. If the outcomes were different, the data product is no longer reproducible (and perhaps we should no longer trust the original results). Realistically, it might be okay if a data product stops being reproducible, so long as this change happens after the data product has outlived its purpose.\n\n\nThere are three core components that need to be accessible for a data product to be reproducible:\n\nData\nSoftware\nDocumentation\n\nThe data and software should be packaged together somewhere, like an R project stored in a GitHub repository or Docker container, with documentation on how to reproduce the data product. Reproducing the data product should be convenient for the end-user, without being disruptive. Ideally the entire pipeline can be run with a single command, and it should not install packages into someone’s local library or change settings on their computer without their permission. Achieving this requires new tools and a stable framework to glue them together.\nIn particular, I think the following R packages are essential for reproducibility:\n\nrenv for creating reproducible environments in R projects\ntargets for creating reproducible workflows\ntestthat for testing the reproducibility of results\nsessioninfo for getting system and R session information\n\nBut there are tools beyond R that are also essential for reproducibility:\n\nQuarto for reproducible documents\nGit for version control\nGitHub or GitLab for hosting Git repositories\nDocker for packaging data products and their dependencies into reproducible containers\n\nEach of these R packages and tools plays a different role in making a data product reproducible. Together they create a system for making reproducible data products. Depending on how long you hope a data product will be reproducible for, you might use all these R packages and tools or you might only use some.\nThe best place to learn each of these R packages and tools individually is their respective documentation. To learn how to use these R packages and tools as a system for making reproducible data products, the following are a good place to start:\n\nReproducible Analytical Pipelines by Bruno Rodrigues\nAutomating Computational Reproducibility in R using renv, Docker, and GitHub Actions by Nathaniel Haines\nCombining R and Python with {reticulate} and Quarto by Nicola Rennie\n\nYou might also like:\n\nOpen source is a hard requirement for reproducibility by Bruno Rodrigues\nFunctional programming explains why containerization is needed for reproducibility by Bruno Rodrigues\nCode longevity of the R programming language by Bruno Rodrigues\nMRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum? by Bruno Rodrigues"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#what-did-you-forget-to-teach-me-about-r",
    "href": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#what-did-you-forget-to-teach-me-about-r",
    "title": "Learning R",
    "section": "What did you forget to teach me about R?",
    "text": "What did you forget to teach me about R?\nSee What They Forgot to Teach You About R by Jenny Bryan and Jim Hester.\nSee also Happy Git and GitHub for the useR by Jenny Bryan, the STAT 545 TAs, and Jim Hester."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#parting-words",
    "href": "series/2023-01-24_reproducible-data-science/posts/learning-r.html#parting-words",
    "title": "Learning R",
    "section": "Parting words",
    "text": "Parting words\n\nData science is not 100% about writing code. There’s a human side to it.\n\n— Hadley Wickham in Designing Data Science\nI discussed this earlier, but it bears repeating: the human side of data science is really important if you want to solve problems successfully. One of the reasons R is my favourite language is because it’s been designed to make statistical thinking and computing accessible to anyone. This accessibility has had a big impact on me—I doubt I would be doing data science without R—and I think it’s why we have such a strong, diverse community of R users and programmers.\nSo I try to make all my work as accessible as it can be, and I recommend you do too. It makes a difference."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/technical-writing.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/technical-writing.html",
    "title": "Technical Writing",
    "section": "",
    "text": "Literate programming is the practice of mixing text and executable code in the same document, making it possible to write reproducible documents. There are four engines for executing code in Quarto documents:\n\nknitr (supports R, Python, Julia, and more)\njupyter (Python only)\nIJulia (Julia only)\nObservable JS (Observable JS only)\n\nAll engines support executing code in code blocks within a document, which can be used to execute code and include its output in a document. Code blocks can produce a wide variety of outputs, including plots, tabular output from data frames, and plain text. The behaviour of code execution and output can be set with Quarto’s Execution Options.\nknitr, jupyter, and Observable JS also support executing inline code within markdown text, which can be used to allow narrative to automatically use the most up to date computations. The syntax for inline code varies across the engines.\nThe remainder of this post covers some useful literate programming practices when writing reproducible documents with the knitr engine.\n\n\n\n\n\n\nProject management\n\n\n\nLiterate programming is a powerful tool for writing reproducible documents, but it can also become unwieldy if your computations require a lot of code. Rather than writing all the code within a document, it is often better to source the required code within a document, then include output using the objects created by the sourced code. The simplest way to adopt this approach is by using the source() function to source R script files containing the computations for a document; a better way is to use the literate programming approach described in the targets R package."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/technical-writing.html#plain-text-outputs",
    "href": "series/2023-01-24_reproducible-data-science/posts/technical-writing.html#plain-text-outputs",
    "title": "Technical Writing",
    "section": "Plain text outputs",
    "text": "Plain text outputs\nMost code you write will be output as plain text. Depending on the purpose of a document, it might be fine to leave that output as is, or you might want to format the text, include it inline, and so forth.\nFor example, say you wanted to report the number of participants in a study you ran. Rather than writing a sentence like this:\n\nThere were twelve participants in the study.\n\nYou could get the number of participants computationally, format that number into the word “twelve”, and include it in the sentence using inline code:\n\nn_participants <- xfun::numbers_to_words(12)\n\n\nThere were `r n_participants` participants in the study.\n\nThis sentence would become “There were twelve participants in the study.” when you render the document. Likewise, if the number of participants changed, the number reported in the sentence would change the next time the document rendered (as long as the R object storing the number of participants knew about the change).\nIf there are many values you need to report inline, storing them in a list is a good practice. For an overview of this approach, see:\n\nLists are my secret weapon for reporting stats with knitr by Tristan Mahr"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/technical-writing.html#figure-and-table-outputs",
    "href": "series/2023-01-24_reproducible-data-science/posts/technical-writing.html#figure-and-table-outputs",
    "title": "Technical Writing",
    "section": "Figure and table outputs",
    "text": "Figure and table outputs\nFigures and tables produced by code blocks have unique parameters you can use to adjust their layout, add captions, or make them cross-referenceable. See the Quarto documentation for details:\n\nFigures documentation\nTables documentation\n\n\n\n\n\n\n\nAdjusting figure appearance\n\n\n\nWhen making figures with ggplot2 it’s best to know the final size of the figure before attempting to adjust the size of plot text or geometry."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/technical-writing.html#equations",
    "href": "series/2023-01-24_reproducible-data-science/posts/technical-writing.html#equations",
    "title": "Technical Writing",
    "section": "Equations",
    "text": "Equations\nQuarto features extensive support for writing beautiful math equations with LaTeX math expressions authored using standard Pandoc markdown syntax: Use $ delimiters for inline math and $$ delimiters for display math. Provide an #eq- label immediately after a display math equation to make it referenceable.\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\ninline math: $E = mc^{2}$\ninline math: E=mc^{2}\n\n\ndisplay math:\n$$E = mc^{2}$$\ndisplay math:\nE = mc^{2}\n\n\ndisplay math with label:\n$$E = mc^{2}$${#eq-mc-squared}\ndisplay math with label:\nE = mc^{2} \\tag{1}\n\n\n\nMath expressions can be rendered in any of Quarto’s output formats, but different rendering methods are used depending on the format:\n\nhtml offers several math rendering methods\npdf uses LaTeX (including raw LaTeX)\ndocx uses Microsoft Word’s equation environment\n\nTo learn more about writing LaTeX math expressions, see:\n\nMathJax basic tutorial and quick reference1\nCheatsheet for LaTeX Math Commands\n\n\n\n\n\n\n\nInspecting equations rendered by MathJax\n\n\n\nEquations rendered by MathJax can be inspected by right clicking them. This is useful if you want to view or copy the underlying TeX code from an equation Show Math As > TeX Commands or Copy to Clipboard > TeX Commands. Note that Tidy Tales uses the KaTeX renderer, so it won’t work on this site.\n\n\n\nUsing inline R code in math equations\nInline R code `r ` can be used within inline or display math to include code output in math equations.\n\nmtcars_fit <- lm(mpg ~ am, data = mtcars)\ncoef_int   <- coef(mtcars_fit)[\"(Intercept)\"]\ncoef_am    <- coef(mtcars_fit)[\"am\"]\n\nInline math: $\\mathrm{\\widehat{mpg}} = `r coef_int` + `r coef_am`(\\mathrm{am})$\nInline math: \\mathrm{\\widehat{mpg}} = 17.1473684 + 7.2449393(\\mathrm{am})\nThe same approach also works for display math.\n\n\n\n\n\n\nWriting model equations\n\n\n\nThe equatiomatic package can be used to write equations from a fitted model. Learn more on the package website.\n\nlibrary(equatiomatic)\n\nBy default the model equation uses math symbols.\n\nextract_eq(mtcars_fit)\n\n\n\n#> $$\n#> \\operatorname{mpg} = \\alpha + \\beta_{1}(\\operatorname{am}) + \\epsilon\n#> $$\n\n\nBut model estimates can be used too.\n\nextract_eq(mtcars_fit, use_coefs = TRUE)\n\n\n\n#> $$\n#> \\operatorname{\\widehat{mpg}} = 17.15 + 7.24(\\operatorname{am})\n#> $$\n\n\n\n\n\n\nUsing math equations in plots\nBoth base R and ggplot2 plots feature support for writing beautiful math equations with plotmath expressions. Equations can be used in plot labels, legends, and text.\nIf you would rather write equations using LaTeX math expressions, the latex2exp package can be used to parse and convert LaTeX to plotmath expressions.2 Learn more on the package website.\nggplot2 also includes some convenience functions for using plotmath in plot labels:\n\nlabel_parsed() interprets labels as plotmath expressions\nlabel_bquote() offers a flexible way of labelling facet rows or columns with plotmath expressions"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html",
    "title": "Productivity Tips",
    "section": "",
    "text": "RStudio includes a variety of features to make you more productive working with R and RStudio. These features are covered in detail in the RStudio User Manual. Here I highlight some of the features I think are particularly helpful for working productively."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#customize-rstudio",
    "href": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#customize-rstudio",
    "title": "Productivity Tips",
    "section": "Customize RStudio",
    "text": "Customize RStudio\nRStudio provides a number of preferences you can customize to change the look and feel of the IDE, provide certain defaults, and so forth. Go through them and find what works for you.\nMy own preferences look something like:\n\n\n~/.config/rstudio/rstudio-prefs.json\n\n{\n    \"show_margin\": false,\n    \"save_files_before_build\": true,\n    \"wrap_tab_navigation\": false,\n    \"save_workspace\": \"never\",\n    \"reuse_sessions_for_project_links\": true,\n    \"jobs_tab_visibility\": \"shown\",\n    \"rainbow_parentheses\": true,\n    \"restore_source_documents\": false,\n    \"restore_last_project\": false,\n    \"load_workspace\": false,\n    \"scroll_past_end_of_document\": true,\n    \"syntax_color_console\": true,\n    \"panes\": {\n        \"quadrants\": [\n            \"Source\",\n            \"TabSet1\",\n            \"Console\",\n            \"TabSet2\"\n        ],\n        \"tabSet1\": [\n            \"History\",\n            \"Connections\",\n            \"Packages\",\n            \"Presentation\"\n        ],\n        \"tabSet2\": [\n            \"Files\",\n            \"Environment\",\n            \"Plots\",\n            \"Viewer\",\n            \"Build\",\n            \"VCS\",\n            \"Help\"\n        ],\n        \"hiddenTabSet\": [\n            \"Tutorial\"\n        ],\n        \"console_left_on_top\": false,\n        \"console_right_on_top\": true,\n        \"additional_source_columns\": 0\n    },\n    \"show_indent_guides\": true,\n    \"highlight_r_function_calls\": true,\n    \"auto_append_newline\": true,\n    \"strip_trailing_whitespace\": true,\n    \"code_completion_characters\": 2,\n    \"graphics_backend\": \"ragg\",\n    \"rmd_viewer_type\": \"pane\",\n    \"show_help_tooltip_on_idle\": true,\n    \"source_with_echo\": true,\n    \"rmd_chunk_output_inline\": false\n}"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#use-the-command-palette",
    "href": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#use-the-command-palette",
    "title": "Productivity Tips",
    "section": "Use the command palette",
    "text": "Use the command palette\nRStudio comes equipped with a command palette that gives instant, searchable access to all of RStudio’s commands. Bring it up with Command-Shift-P."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#use-keyboard-shortcuts",
    "href": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#use-keyboard-shortcuts",
    "title": "Productivity Tips",
    "section": "Use keyboard shortcuts",
    "text": "Use keyboard shortcuts\nRStudio comes equipped with over 100 keyboard shortcuts to speed up your workflow, each of which you can modify to suit your preferences. You can even write your own custom keyboard shortcuts to execute RStudio application commands, editor commands, or user-defined R functions.\n\nCommon shortcuts\n\n\n\n\n\n\n\nDescription\nCommand\n\n\n\n\nCopy\nCtrl+C\n\n\nPaste\nCtrl+V\n\n\nCut\nCtrl+X\n\n\nSave\nCtrl+S\n\n\nUndo\nCtrl+Z\n\n\nRedo\nCtrl+Shift+Z\n\n\n\n\n\nNavigation shortcuts\n\n\n\n\n\n\n\nDescription\nCommand\n\n\n\n\nMove cursor to Source Editor\nCtrl+1\n\n\nNavigate Source Editor tabs (left)\nCtrl+Option+Left\n\n\nNavigate Source Editor tabs (right)\nCtrl+Option+Right\n\n\nMove cursor to Console\nCtrl+2\n\n\nNavigate Console and Terminal command history\nUp/Down\n\n\nMove cursor to beginning of line\nCommand+Left\n\n\nMove cursor to end of line\nCommand+Right\n\n\n\n\n\nEditing shortcuts\n\n\n\n\n\n\n\nDescription\nCommand\n\n\n\n\nIndent (at beginning of line)\nTab\n\n\nInsert pipe operator\nShift+Command+M\n\n\nInsert code block\nCommand+Option+I\n\n\nInsert additional cursor on line above\nCtrl+Option+Up\n\n\nInsert additional cursor on line below\nCtrl+Option+Down\n\n\nInsert additional cursor at click position\nCommand+Option+Click\n\n\n\n\n\nMac shortcuts\n\n\n\n\n\n\n\nDescription\nCommand\n\n\n\n\nSwitch between open apps\nCommand+Tab\n\n\nNavigate app switcher (while holding Command)\nLeft/Right"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#write-your-own-code-snippets",
    "href": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#write-your-own-code-snippets",
    "title": "Productivity Tips",
    "section": "Write your own code snippets",
    "text": "Write your own code snippets\nRStudio supports text macros, called code snippets, useful for quickly inserting common snippets of code or text. If you find yourself writing the same boilerplate over and over again, it might be time to turn it into a code snippet.\nFor inspiration, here are code snippets written by others in the R community:\n\nMarkdown snippets by Tom Mock"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#manage-r-with-.rprofile-and-.renviron",
    "href": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#manage-r-with-.rprofile-and-.renviron",
    "title": "Productivity Tips",
    "section": "Manage R with .Rprofile and .Renviron",
    "text": "Manage R with .Rprofile and .Renviron\nThe .Rprofile and .Renviron dotfiles can be used to modify the startup behaviour of an R session on a global or per-project basis:\n\n.Rprofile contains R code to be run when R starts up\n.Renviron contains environment variables to be set when R starts up\n\nThe behaviour of these files is described in more detail in the R Startup chapter in What They Forgot to Teach You About R by Jenny Bryan and Jim Hester.\n\n\n\n\n\n\nReproducibility\n\n\n\nAs Jenny Bryan and Jim Hester write in What They Forgot to Teach You About R: “A good rule of thumb is you should only put things in your .Rprofile that you run interactively in the R terminal. If it ever appears in a R script or R Markdown file it should not be in your .Rprofile.”\n\n\nFor inspiration, here are .Rprofiles written by others in the R community:\n\nStack Overflow thread\nRProfile Essentials by Kevin Ushey\n\nNote that some of them contain code that could break the rule of thumb for reproducibility."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#related-reading",
    "href": "series/2023-01-24_reproducible-data-science/posts/productivity-tips.html#related-reading",
    "title": "Productivity Tips",
    "section": "Related Reading",
    "text": "Related Reading\n\n\nRStudio User Manual by Posit\nRStudio How To Articles by Posit Support"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/system-setup.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/system-setup.html",
    "title": "System Setup",
    "section": "",
    "text": "The following will install, build, and configure the tools used in the remaining posts in this series (and some other things). This is my personal setup for macOS and is intended to get a new computer up and running with the tools and preferences I like to use. A lot of this is general to any data science setup on macOS, but some of it is specific to me.\nYou are free to use or modify this for your own setup, but should do so thoughtfully. Don’t run any of the scripts below without understanding what they do, and backup your system if you are trying this on an existing setup. To test drive my system setup safely, consider using a virtualized instance of macOS with UTM.\n\n\n\n\n\n\nBefore you begin\n\n\n\nMy system setup is done mainly done through the shell prompt in Terminal, which is the easiest recommended way to get a shell prompt on macOS. You can find Terminal in the Utilities directory inside your Applications directory.\nIf you have never used Terminal before, Apple has a nice shell scripting primer and command line primer book in their archives.\n\n\n\n\nXcode command line tools are needed to build certain packages from source. The command line tools provide a lighter alternative to installing the full Xcode release from the App Store, which contains a lot that isn’t needed for data science use cases.\nInstall Xcode command line tools from the Terminal with:\nxcode-select --install\n\n\n\nHomebrew is an open source package manager for macOS that makes installing, removing, and managing software and dependencies simple.\nInstall homebrew from the Terminal with:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n\n\nchezmoi is an open source dotfile manager. Dotfiles are hidden files and hidden directories that store preferences and settings for packages and applications. Dotfiles can be made visible in Finder with the Command-Shift-. keyboard shortcut; learn more about dotfiles here and here.\nInstall chezmoi from the Terminal with Homebrew:\nbrew install chezmoi\nchezmoi relies on dotfiles being hosted in a public or private Git repository (e.g., GitHub) to share changes across multiple computers. For example, my private dotfiles repository is hosted at: https://github.com/mccarthy-m-g/dotfiles, where I’ve added my dotfiles and other setup scripts. For more information, see:\n\nchezmoi’s Quick Start guide\nAutomating the Setup of a New Mac by Moncef Belyamani"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/system-setup.html#my-setup",
    "href": "series/2023-01-24_reproducible-data-science/posts/system-setup.html#my-setup",
    "title": "System Setup",
    "section": "My setup",
    "text": "My setup\nWith the prerequisites installed, it is now possible to install, build, and configure the remaining tools for my setup. There are many ways to do this, but I use chezmoi’s script support so it’s easy to setup my dotfiles and install software with Homebrew on a new computer using the same command:\nchezmoi init --apply $GITHUB_USERNAME\nWhat this does:\n\nClones the dotfiles repository\nApplies the changes and runs any scripts\n\nBelow I cover how to integrate this approach with Homebrew.\n\nInstalling software with Homebrew\nHomebrew can be used to install almost any software or dependency, whether it’s open source or closed source. Homebrew’s installation commands include:\n\nbrew: Install software packages (also known as formulae)\ncask: Install application casks (RStudio, etc.)\ntap: Add package repositories (from GitHub, etc.)\nmas: Install Mac App Store applications (requires brew install mas)\nwhalebrew: Install Docker images as if they were Homebrew formulae (requires brew install whalebrew)\n\nSoftware and dependencies can be installed interactively from the command line, but a better approach for system setup is to use Homebrew Bundle, which allows you to install a list of dependencies located in a Brewfile using any of Homebrew’s installation commands.\nThe Brewfile to install all the dependencies covered in this series looks something like:\n\n\nBrewfile\n\n# Version control setup\nbrew \"git\"\ncask \"github\"\nbrew \"gh\"\ntap \"microsoft/git\"\ncask \"git-credential-manager-core\"\n\n# R setup\ntap \"r-lib/rig\"\ncask \"rig\"\ncask \"rstudio\"\ncask \"quarto\"\n\n# Python setup\nbrew \"pyenv\"\nbrew \"openssl\"\nbrew \"readline\"\nbrew \"sqlite3\"\nbrew \"xz\"\nbrew \"zlib\"\nbrew \"tcl-tk\"\nbrew \"pipenv\"\n\n# Reference manager setup\ncask \"zotero\"\n\nAfter making a Brewfile, install its dependency list with:\nbrew bundle\nUse the optional --file argument to specify the path to the Brewfile.\n\n\n\n\n\n\nHomebrew Bundle tricks\n\n\n\n\n\nCreate a Brewfile of your current Homebrew installations in the current working directory with:\nbrew bundle dump\nUninstall all software and dependencies not listed in a Brewfile with:\nbrew bundle cleanup\n\n\n\nTo integrate Homebrew Bundle with chezmoi, create a run_once_ script containing the Brewfile as a Here document in your dotfiles repository:\n\n\nrun_once_before_install-packages-darwin.sh.tmpl\n\n{{- if eq .chezmoi.os \"darwin\" -}}\n#!/bin/bash\n\nbrew bundle --no-lock --file=/dev/stdin << Brewfile\n# Version control setup\nbrew \"git\"\ncask \"github\"\nbrew \"gh\"\n\n# R setup\ntap \"r-lib/rig\"\ncask \"rig\"\ncask \"rstudio\"\ncask \"quarto\"\n\n# Python setup\nbrew \"pyenv\"\nbrew \"openssl\"\nbrew \"readline\"\nbrew \"sqlite3\"\nbrew \"xz\"\nbrew \"zlib\"\nbrew \"tcl-tk\"\nbrew \"pipenv\"\n\n# Reference manager setup\ncask \"zotero\"\nBrewfile\n{{ end -}}\n\nThis script will be run the first time you initialize chezmoi (as shown earlier):\nchezmoi init --apply $GITHUB_USERNAME"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-getting-help.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-getting-help.html",
    "title": "Getting Help with R",
    "section": "",
    "text": "If you need help solving a problem, the first step is to create a reproducible example. The goal of a reproducible example is to make it easier for others to help you, or for you to help yourself, by packaging your problematic code in such a way that anyone can exactly reproduce your problem. A good reproducible example is:\n\nSelf-contained: Your code includes everything needed to reproduce your problem\nMinimal: Your example only includes code directly related to your problem\n\nThe work you put into writing a good reproducible example usually pays for itself:\n\nOften you will solve your own problem in the process of writing a reproducible example\nYou improve your chances of getting help when your problem is easy to understand and reproduce\n\nFor tips on writing good reproducible examples, see:\n\nWhat’s a reproducible example and how do I create one?\nHow to make a great R reproducible example\nReprex do’s and don’ts\ntidyverse: Get help!\n\n\n\n\n\n\n\nCreate an R project for writing reproducible examples\n\n\n\nBecause reproducible examples are self-contained and minimal, it’s good practice to create them separately from the project that inspired your problem. A nice way to do this is to create a new reprex R project that contains empty files where you can write reproducible examples like:\nreprex/\n├─reprex.Rproj\n├─reprex.R\n├─reprex.Rmd\n├─reprex.qmd\nThe contents of these files are meant to be ephemeral. After you write a reproducible example and share it, it’s safe to delete the contents of the file or overwrite it in the future. If you do want to save the reproducible example somewhere, consider turning it into a gist on GitHub or a question on Stack Overflow, Cross Validated, or the Posit forum.\n\n\n\n\nUse the reprex R package to test, render, and copy your reproducible examples to your clipboard. Then share them when asking for help on:\n\nStack Overflow\nCross Validated\nThe Posit forum\nAnd more!"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-getting-help.html#debugging-interactively",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-getting-help.html#debugging-interactively",
    "title": "Getting Help with R",
    "section": "Debugging interactively",
    "text": "Debugging interactively\nSee Debugging with the RStudio IDE by Jonathan McPherson and the Debugging chapter in Advanced R by Hadley Wickham.\n\n\n\n\n\n\nR package versions and debugging\n\n\n\nSometimes the bugs you run into aren’t caused by your code, but rather the internal code of the package(s) you’re using. If you’re encountering a bug that’s associated with a package that has a newer version available, updating the package might be the only thing you need to do to fix the bug."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-getting-help.html#debugging-targets-pipelines",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-getting-help.html#debugging-targets-pipelines",
    "title": "Getting Help with R",
    "section": "Debugging targets pipelines",
    "text": "Debugging targets pipelines\nSee the Debugging chapter in the targets R package user manual"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-getting-help.html#debugging-github-actions-workflows",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-getting-help.html#debugging-github-actions-workflows",
    "title": "Getting Help with R",
    "section": "Debugging GitHub Actions workflows",
    "text": "Debugging GitHub Actions workflows\nIf you encounter problems in a GitHub Actions workflow but not locally, good luck. GitHub provides monitoring and troubleshooting documentation to get you started, but the lack of an interactive debugger can make debugging difficult. The typical approach to debugging GitHub Actions workflows is to:\n\nView which step caused the failure in the workflow’s logs\nGuess what caused the problem and commit the changes attempting to fix it\nPush those changes to trigger the workflow\nHope the workflows succeeds (or repeat the process until it does)\n\nThis isn’t great. But there are a few things you can do to make it somewhat better:\n\nDebug the workflow in a Pull Request so you don’t make a mess of your commit history\nUse act to run GitHub Actions workflows locally in Docker containers\nUse tmate to access a workflow’s runner with SSH or Web shell for interactive debugging"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/zotero-setup.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/zotero-setup.html",
    "title": "Reference Manager Setup",
    "section": "",
    "text": "Zotero is an open-source reference manager that styles itself as “your personal research assistant.”\nIf you are using another reference manager you should switch to Zotero, full stop. It’s easy to migrate your data from other reference management tools to Zotero, and will improve your workflow.\n\n\n\n\n\n\nUse Zotero’s built-in PDF reader\n\n\n\nZotero has its own built-in PDF reader and it’s really good. Much better than any external PDF readers. Learn more about it in the Zotero documentation.\n\n\n\n\nInstall Zotero from the Terminal with Homebrew:\nbrew install --cask zotero\n\n\n\nZotero Connector is a browser plugin that allows you to save web content (blogs, articles, etc.) to Zotero with a single click.\nThe Zotero Connector for Safari is bundled with Zotero. You can enable it from the Extensions pane of the Safari preferences.\n\n\n\n\n\n\nUse custom PDF resolvers\n\n\n\nZotero also supports custom PDF resolvers for automatically retrieving PDFs from other sources when Zotero can’t find or access a PDF. You can find some useful examples in Brenton Wiernik’s zotero-tools repository, or by searching something like “zotero pdf resolver” in your favourite search engine.\n\n\n\n\n\nThese are the core extensions I use with Zotero. See their documentation for installation instructions:\n\nBetter BibTeX\nZotFile\n\nI also use:\n\nscite\nPubPeer\n\nYou can find more extensions on the Zotero website.\n\n\n\nAs far as I’m aware, it isn’t possible to configure preferences in Zotero with dotfiles, so this has to be done manually.\nThe main preference I need to configure is the citation key formula for Better BibTeX:\nauthEtAl.lower+\"_\"+shorttitle(3,3)+\"_\"+year\nWhich means:\n\nLast name of the authors (with et al. for references with three or more authors)\nAn underscore\nFirst three words of the reference’s title in camel case\nAnother underscore\nYear of publication (if any)\n\nI use this over a authEtAl.lower+\"_\"+year citation key formula because the titles are a helpful reminder of what I’m citing. However, this citation key formula can make the markdown of a document hard to read, particularly when there are multiple citations in the same sentence or paragraph.\n\n\n\nZotero for iOS is an open source application for working with your Zotero library on an iPad or iPhone. Install it from the App Store."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/zotero-setup.html#configuring-zotero",
    "href": "series/2023-01-24_reproducible-data-science/posts/zotero-setup.html#configuring-zotero",
    "title": "Reference Manager Setup",
    "section": "Configuring Zotero",
    "text": "Configuring Zotero\nThings to do after installing Zotero and extensions.\n\nCloud syncing\nUse Zotero’s sync functionality to sync your Zotero library across devices. Zotero syncing has two parts:\n\nData syncing\nFile syncing\n\nData syncing syncs library items, but doesn’t sync attached files (PDFs, images, etc.). Set up data syncing in the Zotero app by signing into your account.\nFile syncing syncs attached files of library items. Files can be synced using either:\n\nWebDAV\nZotero Storage\n\nWebDAV is a standard protocol for transferring files over the web and can be used to sync files in your personal library. There are a number of WebDAV storage providers known to work with Zotero.\nZotero Storage is the file sync option recommended by Zotero. Zotero Storage includes all the features of WebDAV syncing, plus file syncing in group libraries, web-based access to file attachments, easier setup, and guaranteed compatibility.\n\n\n\n\n\n\nWebDAV or Zotero Storage?\n\n\n\n\n\nI currently use WebDAV syncing through 4shared’s free plan, which gives me 15 GB of free space. I chose this option because I don’t need any of the additional features of Zotero Storage, and 15 GBs is more than enough for my personal library.\n\n\n\n\n\nTurn off automatic tagging\nGo to General > Miscellaneous and turn off “Automatically tag items with keywords and subject headings”. I personally found this feature more annoying than helpful, since different sources would use different tags for the same thing, resulting in messy metadata. Instead I manually add tags to items in my library using the following scheme:\n\nSubject: name (focus area or field)\nTopic: name (topic keywords)\nData: source (open data set name)\nPopulation: characteristic (age group, sex, gender, species, location, etc.)\nVariable: name (variable measured in the study)\nMethod: name (experimental, observational, literature review, etc.)\nAnalysis: method (statistical method used for analysis)\nSource: name (for where or how I discovered the item)\nStatus: reading status (unread, read partially, or read)\n\nI like this approach because it makes it easy to drill down a library collection to the items I need or to rediscover an item I forget the name of. It also reduces some of the cognitive overhead of working with tags by taking advantage of Zotero’s autocomplete when adding tags manually, and alphabetic tag sorting."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/zotero-setup.html#using-zotero-for-citations-in-quarto-and-r-markdown-documents",
    "href": "series/2023-01-24_reproducible-data-science/posts/zotero-setup.html#using-zotero-for-citations-in-quarto-and-r-markdown-documents",
    "title": "Reference Manager Setup",
    "section": "Using Zotero for citations in Quarto and R Markdown documents",
    "text": "Using Zotero for citations in Quarto and R Markdown documents\nQuarto and R Markdown can use Pandoc to automatically generate formatted citations and bibliography entries in a number of citation styles, specified with a CSL file. The open source Citation Style Language (CSL) project maintains a crowdsourced GitHub repository with over 10,000 free CSL citation styles.\n\n\n\n\n\n\nUse CSL JSON for bibliography files\n\n\n\nThe native format for Pandoc’s citation processor, citeproc, CSL JSON and it is essential to use .json formatted bibliography files to get correctly formatted citations and bibliography entries. All other bibliography file types (e.g., .bib) are internally converted to CSL JSON—a complex and often lossy process that can result in incorrectly formatted citations.\n\n\nThere are two options for generating a bibliography file from Zotero personal or group libraries in Quarto and R Markdown:\n\nRStudio’s native Zotero integration (visual editor only)\nThe rbbt R package (works anywhere)\n\nI prefer rbbt because allows you to generate a CSL JSON bibliography file dynamically from a Quarto or R Markdown document, rather than using a persistent file. To insert citations in a document interactively I use rbbt’s “Insert Zotero Citation” RStudio addin from a custom Alt-Command-R keyboard shortcut."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/version-control-setup.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/version-control-setup.html",
    "title": "Version Control Setup",
    "section": "",
    "text": "Git is an open source distributed version control system for tracking and managing changes to files.\n\n\nInstall Git from the Terminal with Homebrew:\nbrew install git"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/version-control-setup.html#github",
    "href": "series/2023-01-24_reproducible-data-science/posts/version-control-setup.html#github",
    "title": "Version Control Setup",
    "section": "GitHub",
    "text": "GitHub\nGitHub is an internet hosting service for projects using Git that makes collaboration easy. It provides the distributed version control of Git, plus issue tracking, project management tools, continuous integration, and more for every project.\n\nInstalling GitHub Desktop\nGitHub Desktop is an open source application for interacting with GitHub using a GUI instead of the command line or a web browser.\nInstall GitHub Desktop from the Terminal with Homebrew:\nbrew install --cask github\n\n\nInstalling GitHub CLI\nGitHub CLI is a command line interface to access GitHub from the Terminal.\nInstall GitHub CLI from the Terminal with Homebrew:\nbrew install gh\n\n\nInstalling Git Credential Manager\nGit Credential Manager is a secure Git credential helper that makes it easy to store credentials securely and connect to GitHub over HTTPS.\nInstall Git Credential Manager from the Terminal with Homebrew:\nbrew tap microsoft/git\nbrew install --cask git-credential-manager-core"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/version-control-setup.html#version-control-in-rstudio",
    "href": "series/2023-01-24_reproducible-data-science/posts/version-control-setup.html#version-control-in-rstudio",
    "title": "Version Control Setup",
    "section": "Version Control in RStudio",
    "text": "Version Control in RStudio\nSee the following:\n\nHappy Git and GitHub for the useR by Jenny Bryan\nusethis: Managing Git(Hub) Credentials"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/version-control-setup.html#related-reading",
    "href": "series/2023-01-24_reproducible-data-science/posts/version-control-setup.html#related-reading",
    "title": "Version Control Setup",
    "section": "Related Reading",
    "text": "Related Reading\n\n\nGit documentation\n\nPro Git by Scott Chacon and Ben Straub\nGit command list\n\nGit and GitHub learning resources\n\nGitHub Skills: Learn how to use GitHub with interactive courses designed for beginners and experts"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/python-setup.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/python-setup.html",
    "title": "Python Setup",
    "section": "",
    "text": "Python is an open source programming language that is the “second best language for everything.” It’s worth setting up in case you need it, but for data science R is almost always simpler and better.\n\n\npyenv is an open source Python installation manager for installing, removing, and switching between multiple Python versions.\n\nInstall pyenv from the Terminal with Homebrew:\nbrew install pyenv\nSet up your shell environment for pyenv.\nInstall Python build dependencies with Homebrew:\nbrew install openssl readline sqlite3 xz zlib tcl-tk\n\n\n\n\nInstall the latest major version of Python 3 with shared library support1 from the Terminal with pyenv:\nenv PYTHON_CONFIGURE_OPTS=\"--enable-shared\" pyenv install 3:latest\n\n\n\npipenv is an open-source dependency manager for Python projects.\nInstall pipenv from the Terminal with Homebrew:\nbrew install pipenv\npoetry is an alternative open-source dependency manager for Python projects. I’m currently experimenting between poetry and pipenv to decide which I prefer.\nInstall poetry from the Terminal with:\ncurl -sSL https://install.python-poetry.org | python3 -\n\n\n\npipx is open-source package manager for installing and running command line applications written in Python in separate environments from your Python libraries. It’s like Homebrew, but for Python applications.\nInstall pipx from the Terminal with:\nbrew install pipx\npipx ensurepath"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/python-setup.html#calling-python-from-r",
    "href": "series/2023-01-24_reproducible-data-science/posts/python-setup.html#calling-python-from-r",
    "title": "Python Setup",
    "section": "Calling Python from R",
    "text": "Calling Python from R\nUse the reticulate R package to interface R and Python in the same project. reticulate has support for projects using pipenv or poetry."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/python-setup.html#related-reading",
    "href": "series/2023-01-24_reproducible-data-science/posts/python-setup.html#related-reading",
    "title": "Python Setup",
    "section": "Related Reading",
    "text": "Related Reading\n\n\nHow to Manage your Python Projects with Pipenv and Pyenv by Bruno Michetti\nPyenv, poetry and other rascals—modern Python dependency and version management by Olaf Górski"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-setup.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-setup.html",
    "title": "R Setup",
    "section": "",
    "text": "R is an open source programming language for wrangling, visualizing, modelling, and communicating data, and so much more. It has a strong community behind it and is widely used among researchers, statisticians, and data scientists in a variety of fields.\n\n\nrig is an open source R installation manager for installing, removing, configuring, and switching between multiple R versions and user-level package libraries.\nInstall rig from the Terminal with Homebrew:\nbrew tap r-lib/rig\nbrew install --cask rig\n\n\n\nInstall the latest version of R from the Terminal with rig:\nrig add release\nR can also be installed from CRAN (the comprehensive R archive network) using the following link https://cloud.r-project.org. A new major version of R comes out once a year, and there are 2-3 minor releases each year.\n\n\n\nRStudio is an open source integrated development environment, or IDE, for R programming made by Posit.\nInstall RStudio from the Terminal with Homebrew:\nbrew install --cask rstudio\nRStudio can also be installed from Posit using the following link https://posit.co/download/rstudio-desktop/. RStudio is updated a couple of times a year. When a new version is available, RStudio will let you know. It’s a good idea to upgrade regularly so you can take advantage of the latest and greatest features.\nLearn more from the RStudio User Guide.\n\n\n\nQuarto is an open source scientific and technical publishing system built on Pandoc.\nInstall Quarto from the Terminal with Homebrew:\nbrew install --cask quarto\n\n\n\n\n\n\nQuarto version manager\n\n\n\n\n\nIf you need to manage and switch between versions of Quarto you can also install qvm, the Quarto version manager."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-setup.html#related-reading",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-setup.html#related-reading",
    "title": "R Setup",
    "section": "Related Reading",
    "text": "Related Reading\n\n\nSetting up macOS as an R data science rig in 2023 by Isabella Velásquez and Gustavo E. Velásquez"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/start-here.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/start-here.html",
    "title": "Start Here",
    "section": "",
    "text": "This series is about learning and doing reproducible data science. It’s a mix between light tutorial content, discussion, and references to more comprehensive learning material that tries to:\n\nProvide a jumping off place for beginners\nServe as a quick reference for more experienced users\nUnearth some of the “hidden curriculum” you might not have been taught while learning R (whether or not you were taught in the classroom or are self-taught)"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/start-here.html#who-is-this-series-for",
    "href": "series/2023-01-24_reproducible-data-science/posts/start-here.html#who-is-this-series-for",
    "title": "Start Here",
    "section": "Who is this series for?",
    "text": "Who is this series for?\nI’ve tried to write this series so anyone can use it to learn or improve their approach to doing reproducible data science. That said, I’ve written the series in an order that makes sense logistically, but not necessarily pedagogically:\n\nIf you are completely new to programming, data science, or R, I recommend starting from the Learning R post and working through the Where do I start? section first\nIf you are setting up a new computer, I recommend reading the posts with setup in the title in order\nFor anyone else, the posts are mostly self-contained and can be read in any order\n\nSince this series is based on my personal experience and approach, some of the setup sections may not be relevant to all readers. In particular, I use macOS, and any examples using the shell prompt will be specific to macOS. Most of the software I use is cross-platform though, so you can refer to the software’s documentation for Linux or Windows instructions."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/start-here.html#where-else-can-i-learn",
    "href": "series/2023-01-24_reproducible-data-science/posts/start-here.html#where-else-can-i-learn",
    "title": "Start Here",
    "section": "Where else can I learn?",
    "text": "Where else can I learn?\nFor complementary and alternative approaches to doing reproducible data science, see:\n\nHandbook to reproducible, ethical and collaborative data science by The Turing Way\nPosit Solutions\n\nIf you need to do data science at scale, Posit’s commercial enterprise solutions are probably the best option."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-packages.html",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-packages.html",
    "title": "R Packages",
    "section": "",
    "text": "The standard way to install R packages is with the install.packages() function that comes with base R.\ninstall.packages(\"tibble\")\nA fresh way to install R packages is with the pkg_install() function from the pak package.\npkg_install(\"tibble\")\npak is fast, safe, and convenient. You can learn more about it on the package website.\n\n\n\n\n\n\n“Do you want to install from sources the package which needs compilation?”\n\n\n\nWhen installing a package, you will sometimes encounter the above question along with the message “There are binary versions available but the source versions are later”. What this question is asking is whether you would like to have your computer build (compile) the package binary from source code then install it (yes) or have your computer install a pre-built binary (no). If you choose yes, you will get the newest version of the package; if you choose no you will get the newest binary version of the package (typically the previous release).\nUnless you need the newest version of the package for a specific feature or bug fix, I recommend choosing no—the binary version will install faster, and you can simply wait a few days for the binary of the newest version to be built on the repository you’re installing packages from, then install the package again to get the newest version. If you need the newest version of the package right now then choose yes, but you will need to set up your R build toolchain first.\n\n\nTo learn more about package installation, see the Package structure and state chapter in R Packages by Hadley Wickham and Jenny Bryan."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-packages.html#where-can-i-install-r-packages-from",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-packages.html#where-can-i-install-r-packages-from",
    "title": "R Packages",
    "section": "Where can I install R packages from?",
    "text": "Where can I install R packages from?\nRelease versions of R packages can be installed from:\n\nCRAN\nBioconductor\n\nDevelopment versions of R packages can be installed from:\n\nGitHub\nR-Universe\n\nPrevious release versions of R packages can be installed from:\n\nPosit Public Package Manager\nMRAN (shut down on July 1st, 2023)"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-packages.html#where-can-i-find-new-r-packages",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-packages.html#where-can-i-find-new-r-packages",
    "title": "R Packages",
    "section": "Where can I find new R packages?",
    "text": "Where can I find new R packages?\n\n\n\nThere were 18954 R packages available on CRAN in 2022, and that number only continues to grow over time. That’s not to mention the 2165 R packages available on Bioconductor, the 1704 packages available on R-universe but not CRAN or Bioconductor, and the unknown amount of packages only available on GitHub.\n\n\n\n\n\nFortunately, there are many places to find curated content about R packages:\n\nYour favourite search engine\nCRAN Task Views\nR Views\nThe Posit Blog\nPosit Videos\ntidyverse blog\nTrending on GitHub\npkgsearch\n\nYour favourite search engine is often a good starting place. Include r and package in your search, plus the topic you’re interested in, and you’ll usually find something helpful. Use boolean operators to narrow down your search or to hide unrelated things like Reddit (-site:reddit.com).\nCRAN Task Views provide guidance on which R packages are relevant for tasks related to a certain topic. The task views are not meant to endorse the “best” packages for a given task; rather, they take an encyclopedic approach that serves as a good reference.\nR Views is an R community blog edited by Posit where you can learn about new R packages and see how to use them (plus some other goodies).\nThe Posit Blog is where you can get news and updates about R packages made by Posit. The blog is about “all things data science and the world that benefits from [Posit] products, community, and events”, so it has a wider scope than R packages alone; but it’s a good place to follow anyways to stay up to date with what’s happening in the R world.\nOn a similar note, you can find videos from events hosted by Posit, such as conference talks, data science hang outs, tutorials, and more at Posit Videos (and the Posit YouTube channel). This is a good place to see what R packages the community is using, and how they use them.\nThe tidyverse blog is where you can get news and updates about R packages in the tidyverse.\nFinally, if you don’t want to leave the comfort of your IDE, the pkgsearch package is a package you can use to find other packages..!\n\n\n\n\n\n\nSocial Media\n\n\n\nWord of mouth on social media is a great way to find new packages or learn about new ways to use your favourite packages. Follow your favourite developers to keep up to date with their work, and check out the #RStats and #TidyTuesday hashtags to see what the community is up to."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-packages.html#how-do-i-attach-r-packages",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-packages.html#how-do-i-attach-r-packages",
    "title": "R Packages",
    "section": "How do I attach R packages?",
    "text": "How do I attach R packages?\nThe standard way to attach R packages is with the library() function that comes with base R.\nlibrary(tibble)\nIf you want to attach multiple packages in a single function call, use the pkg_attach() function from the xfun package.\npkg_attach(\"tibble\", \"dplyr\")\n\nHow do I manage namespace conflicts?\nSometimes R packages will use the same name for different functions. A common example is dplyr::select() and MASS::select(). Under R’s default conflict resolution system, if both of these packages are loaded in the same session select() will come from the most recently loaded package.\nIf you want select() to refer to dplyr::select(), base R provides the following solutions:\n\nLoad MASS first and dplyr second\nExclude select() when loading MASS: library(MASS, exclude = \"select\")\nAlways namespace dplyr’s select() function: dplyr::select()\n\nAn alternative approach to managing namespace conflicts is with the conflicted package, which makes every conflict an error and forces you to choose which function to use. You can learn more about it on the package website."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-packages.html#how-do-i-update-r-packages",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-packages.html#how-do-i-update-r-packages",
    "title": "R Packages",
    "section": "How do I update R packages?",
    "text": "How do I update R packages?\nMost R packages available on CRAN are actively maintained and updated. For example, in 2022 more than half the packages available on CRAN had an update in the same or previous year. It’s good practice to keep up with these updates so you can get the latest features, improvements, and bug fixes.\n\n\n\n\n\nUse old.packages() to check which of the packages you’ve installed have updates available from the repositories listed in getOption(\"repos\").\nold.packages()\nUpdate specific packages to the most recent version by re-installing them with install.packages() or pak::pkg_install().\n# The standard way to update a package\ninstall.packages(\"tibble\")\n\n# A fresh way to update a package\npak::pkg_install(\"tibble\")\nUpdate all the packages listed by old.packages() at once with update.packages()1 or pak::pkg_install(old.packages()[,\"Package\"]). Note that update.packages() uses install.packages() under the hood, so it won’t be as fast as pak.\n# The standard way to update all packages at once\nupdate.packages()\n\n# A fresh way to update all packages at once\npak::pkg_install(old.packages()[,\"Package\"])\nYou can also install and update packages using RStudio, either from the Packages pane or from the menu bar Tools > Check for Package Updates.\n\n\n\n\n\n\nBreaking changes\n\n\n\nSometimes package updates will include breaking changes, which are named as such because they are expected to break code using older versions of the package. Most package updates won’t contain breaking changes, but occasionally they will. This should not discourage you from updating packages, but it should encourage you to update thoughtfully—don’t do your updates before important deadlines!\nI recommend creating multiple user libraries with rig that can be used for different tasks. This way different libraries can be updated more or less frequently, and it’s easier to make the choice whether or not to update. For example, you could have a devel library for package development that is updated weekly, a general main library that is updated monthly or yearly, and so forth. Another good practice—especially for long-running projects—is to create project-specific libraries with renv. The project libraries created by renv are isolated from your user libraries, so you can continue to update your user libraries as normal without worrying about breaking changes affecting the projects using renv."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/posts/r-packages.html#how-do-i-restore-my-current-library-into-a-new-library",
    "href": "series/2023-01-24_reproducible-data-science/posts/r-packages.html#how-do-i-restore-my-current-library-into-a-new-library",
    "title": "R Packages",
    "section": "How do I restore my current library into a new library?",
    "text": "How do I restore my current library into a new library?\nA new library is created whenever you install a new major or minor version of R (the “x” or “y” in version “x.y.z”), so all your favourite packages need to be re-installed to this new library if you want to use them. This is also true if you create a new library in the current version of R (e.g., with rig library add).\n\n\n\n\n\n\nTransferring a library from one computer to another\n\n\n\n\n\nThe approach shown below can also be used to transfer a library from one computer to another. All you need to do is:\n\nFollow the steps to create the character vector of packages in your current library (pkgs)\nSave the pkgs R object with: saveRDS(pkgs, file = \"pkgs.rds\")\nCopy the pkgs.rds file to the other computer*\nLoad the pkgs R object with: pkgs <- readRDS(“path/to/pkgs.rds”)\nInstall pak on the other computer with: install.packages(\"pak\")\nInstall the packages with: pkg_install(pkgs)\n\n*For added convenience, store pkgs.rds in a GitHub repository so you can access it wherever you go!\n\n\n\nIf you are using rig (and you should be), restoring your current library into a new library is easy with some help from pak and dplyr.2\nFirst—in your current library—get the path to your user library. Copy this down somewhere.\n.libPaths()[1]\nYou can now switch to your new library. The easiest way to switch on macOS is with the menu bar app; otherwise use the shell prompt. Open a new RStudio window to start a session with the new library.3\n# Switch to a different R version\nrig default <version>\n\n# Switch to a different library\nrig library default <lib-name>\nIn your new library, install and attach pak and dplyr.\n# install.packages(c(\"pak\", \"dplyr\"))\nlibrary(pak)\nlibrary(dplyr)\nUse pak::pkg_list() to get a data frame containing data about the packages installed in your current library.\npkgs_tbl <- pkg_list(lib = \"path/to/your/user/library\")\nWrangle this data down into a character vector specifying the package source and package, following the package reference syntax used by pak.\npkgs <- pkgs_tbl |>\n  select(package, remoteusername, repotype) |>\n  mutate(pkg = case_when(\n    # GitHub\n    !is.na(remoteusername) ~ paste0(remoteusername, \"/\", package), \n    # CRAN and Bioconductor\n    repotype %in% c(\"cran\", \"bioc\") ~ paste0(repotype, \"::\", package),\n    # Default to the `standard` package source\n    TRUE ~ paste0(\"standard::\", package)\n  )) |>\n  pull(pkg)\nThen install all your packages.\npkg_install(pkgs)\n\n\n\n\n\n\nDefault library locations and .Renviron\n\n\n\nIf you have previously set R_LIBS_USER in your .Renviron to change the default library path, you may need to remove this variable or change its path if there are issues locating your various user libraries. A quick way to open .Renviron is with usethis::edit_r_environ()."
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/index.html",
    "href": "series/2023-01-24_reproducible-data-science/index.html",
    "title": "Reproducible Data Science",
    "section": "",
    "text": "Learn what this series is about and how to use it.\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling and configuring the basics.\n\n\n\nJan 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling and configuring Git and GitHub.\n\n\n\nJan 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling and configuring R.\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling and configuring Python.\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling and configuring Zotero.\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn how to program and do data science with R.\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling, managing, and finding R Packages.\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting reproducible examples and debugging errors with R.\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnical writing and literate programming in Quarto documents.\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecome more productive working with R and RStudio.\n\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "series/2023-01-24_reproducible-data-science/index.html#artwork",
    "href": "series/2023-01-24_reproducible-data-science/index.html#artwork",
    "title": "Reproducible Data Science",
    "section": "Artwork",
    "text": "Artwork\n\nArtwork by Allison Horst."
  },
  {
    "objectID": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html",
    "href": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html",
    "title": "Shared axis labels in patchwork plots",
    "section": "",
    "text": "To access the datasets, help pages, and functions that we will use in this code snippet, load the following packages:\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\nThen make some data and ggplot2 plots to be used in the patchwork.\n\nhuron <- data.frame(year = 1875:1972, level = as.vector(LakeHuron))\nh <- ggplot(huron, aes(year))\n\nh1 <- h +\n  geom_ribbon(aes(ymin = level - 1, ymax = level + 1), fill = \"grey70\") +\n  geom_line(aes(y = level))\n\nh2 <- h + geom_area(aes(y = level))"
  },
  {
    "objectID": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#shared-x-axis-labels",
    "href": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#shared-x-axis-labels",
    "title": "Shared axis labels in patchwork plots",
    "section": "Shared x-axis labels",
    "text": "Shared x-axis labels\nWe set the bottom margin to 0 so the tag is in the same vertical position that the x-axis would otherwise be in.\n\n# Create the patchwork, dropping the x-axis labels from the plots, and setting\n# the margins\nh_patch <- h1 + h2 & xlab(NULL) & theme(plot.margin = margin(5.5, 5.5, 0, 5.5))\n\n# Use the tag label as an x-axis label\nwrap_elements(panel = h_patch) +\n  labs(tag = \"year\") +\n  theme(\n    plot.tag = element_text(size = rel(1)),\n    plot.tag.position = \"bottom\"\n  )"
  },
  {
    "objectID": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#shared-y-axis-labels",
    "href": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#shared-y-axis-labels",
    "title": "Shared axis labels in patchwork plots",
    "section": "Shared y-axis labels",
    "text": "Shared y-axis labels\nWe set the left margin to 0 so the tag is in the same horizontal position that the y-axis would otherwise be in.\n\n# Create the patchwork, dropping the y-axis labels from the plots, and setting\n# the margins\nh_patch <- h1 / h2 & ylab(NULL) & theme(plot.margin = margin(5.5, 5.5, 5.5, 0))\n\n# Use the tag label as a y-axis label\nwrap_elements(h_patch) +\n  labs(tag = \"level\") +\n  theme(\n    plot.tag = element_text(size = rel(1), angle = 90),\n    plot.tag.position = \"left\"\n  )"
  },
  {
    "objectID": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#shared-axis-labels-without-using-patchwork",
    "href": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#shared-axis-labels-without-using-patchwork",
    "title": "Shared axis labels in patchwork plots",
    "section": "Shared axis labels without using patchwork",
    "text": "Shared axis labels without using patchwork\nElio Campitelli shared a solution on Mastodon that accomplishes the same results as above, but without patchwork. It uses the magic tilde notation to create functions in the data argument of each geom that adds a grouping variable var that can be faceted on.\n\nh <- ggplot(huron, aes(year)) +\n  geom_ribbon(\n    data = ~ transform(.x, var = \"a\"),\n    aes(ymin = level - 1, ymax = level + 1),\n    fill = \"grey70\"\n  ) +\n  geom_line(data = ~ transform(.x, var = \"a\"), aes(y = level)) +\n  geom_area(data = ~ transform(.x, var = \"b\"), aes(y = level)) +\n  # Since we don't care about the facet strips here, we can remove them.\n  theme(\n    strip.text = element_blank(),\n    strip.background = element_blank()\n  )\n\nFacet by rows for a shared x-axis.\n\nh +\n  facet_wrap(vars(var), scales = \"free_y\")\n\n\n\n\nFacet by columns for a shared y-axis.\n\nh +\n  facet_wrap(vars(var), scales = \"free_y\", ncol = 1)"
  },
  {
    "objectID": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#section",
    "href": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#section",
    "title": "Shared axis labels in patchwork plots",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#michael-mccarthy",
    "href": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#michael-mccarthy",
    "title": "Shared axis labels in patchwork plots",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#comments",
    "href": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#comments",
    "title": "Shared axis labels in patchwork plots",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#session-info",
    "href": "snippets/2022-12-22_patchwork-shared-axis-labels/index.html#session-info",
    "title": "Shared axis labels in patchwork plots",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2022-12-22\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.280 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n ggplot2     * 3.4.0   2022-11-04 [1] CRAN (R 4.2.0)\n patchwork   * 1.1.2   2022-08-19 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n\n [1] /Users/Michael/Library/R/x86_64/4.2/library/__tidytales\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html",
    "href": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html",
    "title": "Directly labeling ggdist lineribbons with geomtextpath",
    "section": "",
    "text": "To access the datasets, help pages, and functions that we will use in this code snippet, load the following packages:\n\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(geomtextpath)"
  },
  {
    "objectID": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html#directly-labeling-lineribbons",
    "href": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html#directly-labeling-lineribbons",
    "title": "Directly labeling ggdist lineribbons with geomtextpath",
    "section": "Directly labeling lineribbons",
    "text": "Directly labeling lineribbons\nFirst make some data.\n\nset.seed(1234)\nn = 5000\n\ndf <- tibble(\n  .draw = 1:n,\n  intercept = rnorm(n, 3, 1),\n  slope = rnorm(n, 1, 0.25),\n  x = list(-4:5),\n  y = map2(intercept, slope, ~ .x + .y * -4:5)\n) %>%\n  unnest(c(x, y))\n\nThen plot it.\n\ndf %>%\n  group_by(x) %>%\n  median_qi(y, .width = c(.50, .80, .95)) %>%\n  ggplot(aes(x = x, y = y, ymin = .lower, ymax = .upper)) +\n  # Hide the line from geom_lineribbon() by setting `size = 0`\n  geom_lineribbon(size = 0) +\n  scale_fill_brewer() +\n  # Replace the hidden line with a labelled line\n  geom_textline(label = \"label\")"
  },
  {
    "objectID": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html#section",
    "href": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html#section",
    "title": "Directly labeling ggdist lineribbons with geomtextpath",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html#michael-mccarthy",
    "href": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html#michael-mccarthy",
    "title": "Directly labeling ggdist lineribbons with geomtextpath",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html#comments",
    "href": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html#comments",
    "title": "Directly labeling ggdist lineribbons with geomtextpath",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html#session-info",
    "href": "snippets/2022-10-29_geomtextpath-with-ggdist/index.html#session-info",
    "title": "Directly labeling ggdist lineribbons with geomtextpath",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.1.1 (2021-08-10)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2022-11-23\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.269 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n dplyr        * 1.0.10     2022-09-01 [1] CRAN (R 4.1.2)\n forcats      * 0.5.1      2021-01-27 [1] CRAN (R 4.1.0)\n geomtextpath * 0.1.1      2022-08-30 [1] CRAN (R 4.1.2)\n ggdist       * 3.1.1.9001 2022-06-19 [1] Github (mjskay/ggdist@25a813d)\n ggplot2      * 3.3.6      2022-05-03 [1] CRAN (R 4.1.2)\n purrr        * 0.3.4      2020-04-17 [1] CRAN (R 4.1.0)\n readr        * 2.1.2      2022-01-30 [1] CRAN (R 4.1.2)\n sessioninfo  * 1.2.2      2021-12-06 [1] CRAN (R 4.1.0)\n stringr      * 1.4.0      2019-02-10 [1] CRAN (R 4.1.0)\n tibble       * 3.1.6      2021-11-07 [1] CRAN (R 4.1.0)\n tidyr        * 1.2.0      2022-02-01 [1] CRAN (R 4.1.2)\n tidyverse    * 1.3.1      2021-04-15 [1] CRAN (R 4.1.0)\n\n [1] /Users/Michael/Library/R/4.1/library\n [2] /Library/Frameworks/R.framework/Versions/4.1/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html",
    "href": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html",
    "title": "Histogram raincloud plots",
    "section": "",
    "text": "To access the datasets, help pages, and functions that we will use in this code snippet, load the following packages:\n\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(palettes)\nlibrary(forcats)"
  },
  {
    "objectID": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#rationale",
    "href": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#rationale",
    "title": "Histogram raincloud plots",
    "section": "Rationale",
    "text": "Rationale\nLikert scales are a commonly used measurement tool in surveys. A typical Likert scale is made of multiple items measuring respondent’s attitudes towards different statements (e.g., “The prime minister is doing a good job”, “The senate is doing a good job”, etc.).\nAttitudes towards each statement are then measured with a rating scale like:\n\n\nPlease indicate how much you agree or disagree with each of these statements:\n\n\n\n\n\n\n\n\n\n\n\n\nStrongly disagree\nSomewhat disagree\nNeither agree nor disagree\nSomewhat agree\nStrongly agree\n\n\n\n\nThe prime minister is doing a good job.\n1\n2\n3\n4\n5\n\n\nThe senate is doing a good job.\n1\n2\n3\n4\n5\n\n\n\n\n\nBecause items in a Likert scale are numeric but discrete, a density histogram is an ideal way to visualize the distribution of responses to each item (as opposed to the density curve typically used in raincloud plots with continuous data).\n\nWhy not a density curve?\nWhile it is possible to use a density curve, doing so should make it immediately obvious why it isn’t a great approach for discrete numeric data like this:\n\nThe density curve masks notable differences in density between different scores\nThe outermost fills in the density curve are cut off when it is trimmed to the range of the input data\nThe density curve goes far beyond the possible values of the data when it isn’t trimmed1\n\n\n\n\n\n\n\n\n\nCode\nggplot(likert_scores, aes(x = score, y = item)) +\n  stat_slab(\n    aes(fill = cut(after_stat(x), breaks = breaks(x))),\n    justification = -.2,\n    height = 0.7,\n    slab_colour = \"black\",\n    slab_linewidth = 0.5,\n    trim = TRUE\n  ) +\n  geom_boxplot(\n    width = .2,\n    outlier.shape = NA\n  ) +\n  geom_jitter(width = .1, height = .1, alpha = .3) +\n  scale_fill_manual(\n    values = pal_ramp(met_palettes$Hiroshige, 5, -1),\n    labels = 1:5,\n    guide = guide_legend(title = \"score\", reverse = TRUE)\n  )\nggplot(likert_scores, aes(x = score, y = item)) +\n  stat_slab(\n    justification = -.2,\n    height = 0.7,\n    slab_colour = \"black\",\n    slab_linewidth = 0.5,\n    trim = FALSE\n  ) +\n  geom_boxplot(\n    width = .2,\n    outlier.shape = NA\n  ) +\n  geom_jitter(width = .1, height = .1, alpha = .3) +\n  scale_x_continuous(breaks = 1:5)\n\n\n\n\n\n\n\ntrim = TRUE\n\n\n\n\n\n\n\ntrim = FALSE\n\n\n\n\n\n\nHowever, each of these problems is easily solved by using a density histogram instead."
  },
  {
    "objectID": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#histogram-raincloud-plots",
    "href": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#histogram-raincloud-plots",
    "title": "Histogram raincloud plots",
    "section": "Histogram raincloud plots",
    "text": "Histogram raincloud plots\nFirst make some data.\n\nset.seed(123)\n\nlikert_scores <- data.frame(\n  item = rep(letters[1:2], times = 33),\n  score = sample(1:5, 66, replace = TRUE)\n)\n\nIt’s straightforward to make density histograms for each item with ggplot2.\n\nggplot(likert_scores, aes(x = score, y = after_stat(density))) +\n  geom_histogram(\n    aes(fill = after_stat(x)),\n    bins = 5,\n    colour = \"black\"\n  ) +\n  scale_fill_gradientn(\n    colours = pal_ramp(met_palettes$Hiroshige, 5, -1),\n    guide = guide_legend(title = \"score\", reverse = TRUE)\n  ) +\n  facet_wrap(vars(fct_rev(item)), ncol = 1)\n\n\n\n\nHowever, the density histograms in this plot can’t be vertically justified to give space for the box and whiskers plot and points used in a typical raincloud plot. For that we need the stat_slab() function from the ggdist package and a small helper function to determine where to put breaks in the histogram.\n\n#' Set breaks so bins are centred on each score\n#'\n#' @param x A vector of values.\n#' @param width Any value between 0 and 0.5 for setting the width of the bins.\nbreaks <- function(x, width = 0.49999999) {\n  rep(1:max(x), each = 2) + c(-width, width)\n}\n\nThe default slab type for stat_slab() is a probability density (or mass) function (\"pdf\"), but it can also calculate density histograms (\"histogram\"). To match the appearance of geom_histogram(), the breaks argument needs to be given the location of each bin’s left and right edge; this also necessitates using cut() with the fill aesthetic so the fill breaks correctly align with each bin.\n\nggplot(likert_scores, aes(x = score, y = item)) +\n  stat_slab(\n    # Divide fill into five equal bins\n    aes(fill = cut(after_stat(x), breaks = 5)),\n    slab_type = \"histogram\",\n    breaks = \\(x) breaks(x),\n    # Justify the histogram upwards\n    justification = -.2,\n    # Reduce the histogram's height so it doesn't cover geoms from other items\n    height = 0.7,\n    # Add black outlines because they look nice\n    slab_colour = \"black\",\n    outline_bars = TRUE,\n    slab_linewidth = 0.5\n  ) +\n  geom_boxplot(\n    width = .2,\n    # Hide outliers since the raw data will be plotted\n    outlier.shape = NA\n  ) +\n  geom_jitter(width = .1, height = .1, alpha = .3) +\n  # Cutting the fill into bins puts it on a discrete scale\n  scale_fill_manual(\n    values = pal_ramp(met_palettes$Hiroshige, 5, -1),\n    labels = 1:5,\n    guide = guide_legend(title = \"score\", reverse = TRUE)\n  )"
  },
  {
    "objectID": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#section",
    "href": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#section",
    "title": "Histogram raincloud plots",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#michael-mccarthy",
    "href": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#michael-mccarthy",
    "title": "Histogram raincloud plots",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#comments",
    "href": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#comments",
    "title": "Histogram raincloud plots",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#session-info",
    "href": "snippets/2023-01-19_ggdist-histogram-rainclouds/index.html#session-info",
    "title": "Histogram raincloud plots",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2023-01-20\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.313 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n forcats     * 0.5.2   2022-08-19 [1] CRAN (R 4.2.0)\n ggdist      * 3.2.1   2023-01-18 [1] CRAN (R 4.2.2)\n ggplot2     * 3.4.0   2022-11-04 [1] CRAN (R 4.2.0)\n palettes    * 0.1.0   2022-12-19 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n\n [1] /Users/Michael/Library/R/x86_64/4.2/library/__tidytales\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html",
    "href": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html",
    "title": "Longitudinal Measurement Invariance",
    "section": "",
    "text": "To access the datasets, help pages, and functions that we will use in this code snippet, load the following packages:\n\nlibrary(lavaan)\nlibrary(semTools)\n\nAnd read in the data:\n\nsocial_exchanges <- read.csv(here(\"data\", \"2021-11-01_social-exchanges.csv\"))\n\nThe data contains simulated values for several indicators of positive and negative social exchanges, measured on two occasions (w1 and w2). There are three continuous indicators that measure perceived companionship (vst1, vst2, vst3), and three binary indicators that measure unwanted advice (unw1, unw2, unw3). The data and some of the examples come from Longitudinal Structural Equation Modeling: A Comprehensive Introduction by Jason Newsom."
  },
  {
    "objectID": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#configural-invariance",
    "href": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#configural-invariance",
    "title": "Longitudinal Measurement Invariance",
    "section": "Configural Invariance",
    "text": "Configural Invariance\nUsing the lavaan package.\n\nconfigural_model_lav <- (\"\n  # Measurement model\n  w1comp =~ w1vst1 + w1vst2 + w1vst3\n  w2comp =~ w2vst1 + w2vst2 + w2vst3\n  \n  # Variances and covariances\n  w2comp ~~ w1comp\n  w1comp ~~ w1comp\n  w2comp ~~ w2comp\n\n  w1vst1 ~~ w1vst1\n  w1vst2 ~~ w1vst2\n  w1vst3 ~~ w1vst3\n  w2vst1 ~~ w2vst1\n  w2vst2 ~~ w2vst2\n  w2vst3 ~~ w2vst3\n\n  w1vst1 ~~ w2vst1\n  w1vst2 ~~ w2vst2\n  w1vst3 ~~ w2vst3\n\")\n\nconfigural_model_lav_fit <- sem(configural_model_lav, data = social_exchanges)\n\nUsing the semTools package.\n\n# First, define the configural model, using the repeated measures factors and\n# indicators.\nconfigural_model_smt <- (\"\n  # Measurement model\n  w1comp =~ w1vst1 + w1vst2 + w1vst3\n  w2comp =~ w2vst1 + w2vst2 + w2vst3\n\")\n\n# Second, create a named list indicating which factors are actually the same\n# latent variable measured repeatedly.\nlongitudinal_factor_names <- list(\n  comp = c(\"w1comp\", \"w2comp\")\n)\n\n# Third, generate the lavaan model syntax using semTools.\nconfigural_model_smt <- measEq.syntax(\n  configural.model = configural_model_smt,\n  longFacNames = longitudinal_factor_names,\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  data = social_exchanges\n)\nconfigural_model_smt <- as.character(configural_model_smt)\n\n# Finally, fit the model using lavaan.\nconfigural_model_smt_fit <- sem(configural_model_smt, data = social_exchanges)\n\nCompare lavaan and semTools fit measures.\n\n\nConfigural invariance is met if the model fits well, indicators load on the same factors, and loadings are all of acceptable magnitude. An alternative way of testing longitudinal configural invariance is to fit separate confirmatory factor models at each time point; configural invariance is met if the previously stated criteria hold and the measure has the same factor structure at each time point.\n\nfitMeasures(configural_model_lav_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\"))\n\n#>  chisq     df pvalue    cfi  rmsea \n#>  9.911  5.000  0.078  0.997  0.041\n\nfitMeasures(configural_model_smt_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\"))\n\n#>  chisq     df pvalue    cfi  rmsea \n#>  9.911  5.000  0.078  0.997  0.041"
  },
  {
    "objectID": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#weak-invariance",
    "href": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#weak-invariance",
    "title": "Longitudinal Measurement Invariance",
    "section": "Weak Invariance",
    "text": "Weak Invariance\nUsing the lavaan package.\n\nweak_model_lav <- (\"\n  # Measurement model\n  w1comp =~ w1vst1 + a*w1vst2 + b*w1vst3 # Factor loading equality constraint\n  w2comp =~ w2vst1 + a*w2vst2 + b*w2vst3 # Factor loading equality constraint\n\n  # Variances and covariances\n  w2comp ~~ w1comp\n  w1comp ~~ w1comp\n  w2comp ~~ w2comp\n\n  w1vst1 ~~ w1vst1\n  w1vst2 ~~ w1vst2\n  w1vst3 ~~ w1vst3\n  w2vst1 ~~ w2vst1\n  w2vst2 ~~ w2vst2\n  w2vst3 ~~ w2vst3\n\n  w1vst1 ~~ w2vst1\n  w1vst2 ~~ w2vst2\n  w1vst3 ~~ w2vst3\n\")\n\nweak_model_lav_fit <- sem(weak_model_lav, social_exchanges)\n\nUsing the semTools package.\n\nweak_model_smt <- measEq.syntax(\n  configural.model = configural_model_smt,\n  longFacNames = longitudinal_factor_names,\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  long.equal = c(\"loadings\"),\n  data = social_exchanges\n)\nweak_model_smt <- as.character(weak_model_smt)\n\nweak_model_smt_fit <- sem(weak_model_smt, data = social_exchanges)\n\nCompare lavaan and semTools fit measures.\n\nfitMeasures(weak_model_lav_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\"))\n\n#>  chisq     df pvalue    cfi  rmsea \n#> 12.077  7.000  0.098  0.997  0.036\n\nfitMeasures(weak_model_smt_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\"))\n\n#>  chisq     df pvalue    cfi  rmsea \n#> 12.077  7.000  0.098  0.997  0.036\n\n\nTest weak invariance.\n\nlavTestLRT(configural_model_lav_fit, weak_model_lav_fit)"
  },
  {
    "objectID": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#strong-invariance",
    "href": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#strong-invariance",
    "title": "Longitudinal Measurement Invariance",
    "section": "Strong Invariance",
    "text": "Strong Invariance\nUsing the lavaan package.\n\n\nEquality tests of factor variances should only be conducted when all factor loadings also are constrained to be equal over time. When all non-referent loadings are set equal in the constrained model, the chi-square is the same regardless of the referent.\n\nstrong_model_lav <- (\"\n  # Measurement model\n  w1comp =~ w1vst1 + a*w1vst2 + b*w1vst3 # Factor loading equality constraint\n  w2comp =~ w2vst1 + a*w2vst2 + b*w2vst3 # Factor loading equality constraint\n\n  # Variances and covariances\n  w2comp ~~ w1comp\n  w2comp ~~ v*w2comp # Factor variance equality constraint\n  w1comp ~~ v*w1comp # Factor variance equality constraint\n\n  w1vst1 ~~ w2vst1\n  w1vst2 ~~ w2vst2\n  w1vst3 ~~ w2vst3\n\")\n\nstrong_model_lav_fit <- sem(strong_model_lav, social_exchanges)\n\nUsing the semTools package.\n\n# Example 2.2\nstrong_model_smt <- measEq.syntax(\n  configural.model = configural_model_smt,\n  longFacNames = longitudinal_factor_names,\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  long.equal = c(\"loadings\", \"lv.variances\"),\n  data = social_exchanges\n)\nstrong_model_smt <- as.character(strong_model_smt)\n\nstrong_model_smt_fit <- sem(strong_model_smt, social_exchanges)\n\nCompare lavaan and semTools fit measures.\n\nfitMeasures(strong_model_lav_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\"))\n\n#>  chisq     df pvalue    cfi  rmsea \n#> 37.553  8.000  0.000  0.983  0.080\n\nfitMeasures(strong_model_smt_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\"))\n\n#>  chisq     df pvalue    cfi  rmsea \n#> 37.553  8.000  0.000  0.983  0.080\n\n\nTest strong invariance.\n\nlavTestLRT(configural_model_lav_fit, weak_model_lav_fit, strong_model_lav_fit)"
  },
  {
    "objectID": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#strict-invariance",
    "href": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#strict-invariance",
    "title": "Longitudinal Measurement Invariance",
    "section": "Strict Invariance",
    "text": "Strict Invariance\nUsing the lavaan package.\n\nstrict_model_lav <- (\"\n  # Measurement model\n  w1comp =~ w1vst1 + a*w1vst2 + b*w1vst3 # Factor loading equality constraint\n  w2comp =~ w2vst1 + a*w2vst2 + b*w2vst3 # Factor loading equality constraint\n\n  # Variances & covariances\n  w2comp ~~ w1comp\n\n  w1comp ~~ c*w1comp # Factor variance equality constraint\n  w2comp ~~ c*w2comp # Factor variance equality constraint\n\n  w1vst1 ~~ w2vst1\n  w1vst2 ~~ w2vst2\n  w1vst3 ~~ w2vst3\n\n  w1vst1 ~~ d*w1vst1 # Measurement residual equality constraint\n  w1vst2 ~~ e*w1vst2 # Measurement residual equality constraint\n  w1vst3 ~~ f*w1vst3 # Measurement residual equality constraint\n\n  w2vst1 ~~ d*w2vst1 # Measurement residual equality constraint\n  w2vst2 ~~ e*w2vst2 # Measurement residual equality constraint\n  w2vst3 ~~ f*w2vst3 # Measurement residual equality constraint\n\")\n\nstrict_model_lav_fit <- sem(strict_model_lav, social_exchanges)\n\nUsing the semTools package.\n\nstrict_model_smt <- measEq.syntax(\n  configural.model = configural_model_smt,\n  longFacNames = longitudinal_factor_names,\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  long.equal = c(\"loadings\", \"lv.variances\", \"residuals\"),\n  data = social_exchanges\n)\nstrict_model_smt <- as.character(strict_model_smt)\n\nstrict_model_smt_fit <- sem(strict_model_smt, social_exchanges)\n\nCompare lavaan and semTools fit measures.\n\nfitMeasures(strict_model_lav_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\"))\n\n#>  chisq     df pvalue    cfi  rmsea \n#> 78.779 11.000  0.000  0.961  0.104\n\nfitMeasures(strict_model_smt_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\"))\n\n#>  chisq     df pvalue    cfi  rmsea \n#> 78.779 11.000  0.000  0.961  0.104\n\n\nTest strict invariance.\n\nlavTestLRT(\n  configural_model_lav_fit,\n  weak_model_lav_fit,\n  strong_model_lav_fit,\n  strict_model_lav_fit\n)"
  },
  {
    "objectID": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#section",
    "href": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#section",
    "title": "Longitudinal Measurement Invariance",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#michael-mccarthy",
    "href": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#michael-mccarthy",
    "title": "Longitudinal Measurement Invariance",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#comments",
    "href": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#comments",
    "title": "Longitudinal Measurement Invariance",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#session-info",
    "href": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#session-info",
    "title": "Longitudinal Measurement Invariance",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.1.1 (2021-08-10)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2022-11-23\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.269 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n here        * 1.0.1   2020-12-13 [1] CRAN (R 4.1.0)\n lavaan      * 0.6-11  2022-03-31 [1] CRAN (R 4.1.2)\n semTools    * 0.5-5   2021-07-07 [1] CRAN (R 4.1.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.1.0)\n\n [1] /Users/Michael/Library/R/4.1/library\n [2] /Library/Frameworks/R.framework/Versions/4.1/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#data",
    "href": "snippets/2021-11-01_longitudinal-measurement-invariance/index.html#data",
    "title": "Longitudinal Measurement Invariance",
    "section": "Data",
    "text": "Data\n\nDownload the data used in this post."
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html",
    "href": "posts/2022-12-20_palettes/index.html",
    "title": "Introducing the palettes package",
    "section": "",
    "text": "I am proud to announce that version 0.1.0 of the palettes package is now on CRAN. palettes is an R package for working with colour vectors and colour palettes. I made it with three main goals in mind, each described in a vignette on the package website:\n\nTo provide a new family of colour classes (palettes_colour and palettes_palette) that always print as hex codes with colour previews.\nTo provide a comprehensive library of methods for working with colour vectors and colour palettes, including methods for ggplot2, gt, biscale, and other colour packages.\nTo make it easy for anyone to make their own colour palette package. Colour palette packages made with palettes exist solely for the purpose of distributing colour palettes and get access to all the features of palettes for free.\n\nIf you just want to jump in and start using palettes, you can install it from CRAN with:\ninstall.packages(\"palettes\")\nThe package website is the best place to start: https://mccarthy-m-g.github.io/palettes/index.html\nIf you want to learn more about why you should be using palettes, read on to learn more about the motivation of the package and how it makes working with colour vectors and colour palettes easy and fun for everyone."
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html#origins",
    "href": "posts/2022-12-20_palettes/index.html#origins",
    "title": "Introducing the palettes package",
    "section": "Origins",
    "text": "Origins\nThe origins of palettes start with a different (in purgatory) package, BPRDcolours, which I was inspired to make while reading Mike Mignola’s Hellboy comics. Every page of Hellboy is a masterclass in colour and contrast, and I thought it would be fun to bring those colours into R:\n\n\n\nHellboy promo poster by Mike Mignola\n\n\nSo how does one make a colour palette package in R? My answer now is to read the Creating a colour palette package vignette and make it with palettes. My answer then was to read the source code of several other colour palette packages, then reimplement the relevant functions in BPRDcolours. Not a great answer, but it’s the approach everyone else was using.\nI managed to put together a rough development version, but got sidetracked with other projects soon after and put BPRDcolours on hold. And so my mind was left to brew on a better way to go about this whole colour palette package thing."
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html#enter-palettes",
    "href": "posts/2022-12-20_palettes/index.html#enter-palettes",
    "title": "Introducing the palettes package",
    "section": "Enter palettes",
    "text": "Enter palettes\nOne year after development on BPRDcolours stopped, I started palettes.\nThe main motivation behind palettes was to standardize and simplify the process of making a colour palette package. No more reading through other’s source code to figure out how to store colours or make them work with ggplot2 plots. No more unnecessary code duplication. I wanted the process of shipping a set of hex colour codes in an R package to be as simple as, well… shipping a set of hex colour codes in an R package. Anything extra like ggplot2 functionality should come for free.\nA secondary motivation was to provide pretty printing of colour palettes, with colour previews next to the hex colour codes. This was inspired by packages like taylor and prismatic, which provide their own pretty printing methods for colours using the crayon package. palettes uses the cli package for pretty printing, which has superseded crayon."
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html#just-show-me-some-colour-palettes-already",
    "href": "posts/2022-12-20_palettes/index.html#just-show-me-some-colour-palettes-already",
    "title": "Introducing the palettes package",
    "section": "Just show me some colour palettes already!",
    "text": "Just show me some colour palettes already!\nOkay, okay.\n\nlibrary(palettes)\n\nColour classes in palettes come in two forms:\n\nColour vectors (palettes_colour), which are created by pal_colour()\nColour palettes (palettes_palette), which are created by pal_palette()\n\nColour vectors can be thought of as a base type for colours, and colour palettes are just (named) lists of colour vectors. To illustrate, let’s use some colours from the MetBrewer package.\npal_colour() is a nice way to create a colour vector.\n\njava <- pal_colour(c(\"#663171\", \"#cf3a36\", \"#ea7428\", \"#e2998a\", \"#0c7156\"))\njava\n#> <palettes_colour[5]>\n#> • #663171\n#> • #CF3A36\n#> • #EA7428\n#> • #E2998A\n#> • #0C7156\n\n\npal_palette() is a nice way to create named colour palettes.\n\nmetbrewer_palettes <- pal_palette(\n  egypt = c(\"#dd5129\", \"#0f7ba2\", \"#43b284\", \"#fab255\"),\n  java  = java\n)\nmetbrewer_palettes\n#> <palettes_palette[2]>\n#> $egypt\n#> <palettes_colour[4]>\n#> • #DD5129\n#> • #0F7BA2\n#> • #43B284\n#> • #FAB255\n#> \n#> $java\n#> <palettes_colour[5]>\n#> • #663171\n#> • #CF3A36\n#> • #EA7428\n#> • #E2998A\n#> • #0C7156\n\n\nplot() is a nice way to showcase colour vectors and colour palettes. The appearance of the plot depends on the input.\n\nplot(metbrewer_palettes)\n\n\n\n\nCasting and coercion methods are also available to turn other objects (like character vectors or lists) into colour vectors and colour palettes.\nYou can even cast colour vectors and colour palettes into tibbles.\n\nmetbrewer_tbl <- as_tibble(metbrewer_palettes)\nmetbrewer_tbl\n#> # A tibble: 9 × 2\n#>   palette colour   \n#>   <chr>   <colour> \n#> 1 egypt   • #DD5129\n#> 2 egypt   • #0F7BA2\n#> 3 egypt   • #43B284\n#> 4 egypt   • #FAB255\n#> 5 java    • #663171\n#> 6 java    • #CF3A36\n#> 7 java    • #EA7428\n#> 8 java    • #E2998A\n#> 9 java    • #0C7156\n\n\nThis is useful if you want to wrangle the colours with dplyr.\n\nlibrary(dplyr)\n\nmetbrewer_tbl <- slice(metbrewer_tbl, -8)\nmetbrewer_tbl\n#> # A tibble: 8 × 2\n#>   palette colour   \n#>   <chr>   <colour> \n#> 1 egypt   • #DD5129\n#> 2 egypt   • #0F7BA2\n#> 3 egypt   • #43B284\n#> 4 egypt   • #FAB255\n#> 5 java    • #663171\n#> 6 java    • #CF3A36\n#> 7 java    • #EA7428\n#> 8 java    • #0C7156\n\n\nThen go back to a colour palette with the deframe() function from tibble.\n\nlibrary(tibble)\n\nmetbrewer_tbl %>%\n  group_by(palette) %>%\n  summarise(pal_palette(colour)) %>%\n  deframe()\n#> <palettes_palette[2]>\n#> $egypt\n#> <palettes_colour[4]>\n#> • #DD5129\n#> • #0F7BA2\n#> • #43B284\n#> • #FAB255\n#> \n#> $java\n#> <palettes_colour[4]>\n#> • #663171\n#> • #CF3A36\n#> • #EA7428\n#> • #0C7156"
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html#what-about-ggplot2-plots",
    "href": "posts/2022-12-20_palettes/index.html#what-about-ggplot2-plots",
    "title": "Introducing the palettes package",
    "section": "What about ggplot2 plots?",
    "text": "What about ggplot2 plots?\nJust use one of the scale_ functions!\n\nlibrary(ggplot2)\n\nhiroshige <- pal_colour(c(\n  \"#1e466e\", \"#376795\", \"#528fad\", \"#72bcd5\", \"#aadce0\",\n  \"#ffe6b7\", \"#ffd06f\", \"#f7aa58\", \"#ef8a47\", \"#e76254\"\n))\n\nggplot(faithfuld, aes(waiting, eruptions, fill = density)) +\n  geom_raster() +\n  coord_cartesian(expand = FALSE) +\n  scale_fill_palette_c(hiroshige)\n\n\n\n\nThere are scale_ functions for discrete, continuous, and binned data, and you can pass additional arguments to them for further customization."
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html#im-sold.-how-do-i-make-a-colour-palette-package",
    "href": "posts/2022-12-20_palettes/index.html#im-sold.-how-do-i-make-a-colour-palette-package",
    "title": "Introducing the palettes package",
    "section": "I’m sold. How do I make a colour palette package?",
    "text": "I’m sold. How do I make a colour palette package?\nSee the Creating a colour palette package vignette, which works through an example package showing the steps needed to make a colour palette package with palettes.\nIf you do make a package with palettes, let me know through email or Mastodon! In the future I’d like to make a “palettesverse” to showcase and collect all the packages using palettes in one place."
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html#whats-next",
    "href": "posts/2022-12-20_palettes/index.html#whats-next",
    "title": "Introducing the palettes package",
    "section": "What’s next?",
    "text": "What’s next?\nIn future versions I would like to:\n\nProvide an option to choose the symbol used for colour previews\nAdd functions for manipulating colour palettes\nAdd an interpolation function for bivariate colour scales\nMake a better hex sticker (looking for help on this one!)\n\nIf you have other suggestions or requests, please file an issue on GitHub."
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html#section",
    "href": "posts/2022-12-20_palettes/index.html#section",
    "title": "Introducing the palettes package",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html#michael-mccarthy",
    "href": "posts/2022-12-20_palettes/index.html#michael-mccarthy",
    "title": "Introducing the palettes package",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html#comments",
    "href": "posts/2022-12-20_palettes/index.html#comments",
    "title": "Introducing the palettes package",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/2022-12-20_palettes/index.html#session-info",
    "href": "posts/2022-12-20_palettes/index.html#session-info",
    "title": "Introducing the palettes package",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2022-12-21\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.280 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.0.10  2022-09-01 [1] CRAN (R 4.2.0)\n ggplot2     * 3.4.0   2022-11-04 [1] CRAN (R 4.2.0)\n palettes    * 0.1.0   2022-12-19 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n tibble      * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n\n [1] /Users/Michael/Library/R/x86_64/4.2/library/__tidytales\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-05-03_r-developers/index.html",
    "href": "posts/2023-05-03_r-developers/index.html",
    "title": "The Pareto Principle in R package development",
    "section": "",
    "text": "During my (ongoing) job search for a data science or developer-focused role where I get to do R programming, this question came to me: Just how many R developers are there? That’s the question that inspired this post. However, the data needed to answer this question can also be used to answer other interesting questions about R developers, such as how many packages they’ve contributed to, their roles in package development, and so forth. So that’s what we’ll be doing here.\nIf you just want to see the stats, you can skip to the R developer statistics section. Otherwise follow along to see how I retrieved and wrangled the data into a usable state."
  },
  {
    "objectID": "posts/2023-05-03_r-developers/index.html#prerequisites",
    "href": "posts/2023-05-03_r-developers/index.html#prerequisites",
    "title": "The Pareto Principle in R package development",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nlibrary(tidyverse)\nlibrary(stringi)\nlibrary(scales)\nlibrary(gt)\n\nI’ll be using the CRAN package repository data returned by tools::CRAN_package_db() to get package and author metadata for the current packages available on CRAN. This returns a data frame with character columns containing most metadata from the DESCRIPTION file of a given R package.\n\n\nSince this data will change over time, here’s when tools::CRAN_package_db() was run for reference: 2023-05-03.\n\ncran_pkg_db <- tools::CRAN_package_db()\n\nglimpse(cran_pkg_db)\n\n#> Rows: 19,473\n#> Columns: 67\n#> $ Package                   <chr> \"A3\", \"AalenJohansen\", \"AATtools\", \"ABACUS\",…\n#> $ Version                   <chr> \"1.0.0\", \"1.0\", \"0.0.2\", \"1.0.0\", \"0.1\", \"0.…\n#> $ Priority                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Depends                   <chr> \"R (>= 2.15.0), xtable, pbapply\", NA, \"R (>=…\n#> $ Imports                   <chr> NA, NA, \"magrittr, dplyr, doParallel, foreac…\n#> $ LinkingTo                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Rcp…\n#> $ Suggests                  <chr> \"randomForest, e1071\", \"knitr, rmarkdown\", N…\n#> $ Enhances                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ License                   <chr> \"GPL (>= 2)\", \"GPL (>= 2)\", \"GPL-3\", \"GPL-3\"…\n#> $ License_is_FOSS           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ License_restricts_use     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ OS_type                   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Archs                     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ MD5sum                    <chr> \"027ebdd8affce8f0effaecfcd5f5ade2\", \"d7eb2a6…\n#> $ NeedsCompilation          <chr> \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"n…\n#> $ Additional_repositories   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Author                    <chr> \"Scott Fortmann-Roe\", \"Martin Bladt [aut, cr…\n#> $ `Authors@R`               <chr> NA, \"c(person(\\\"Martin\\\", \\\"Bladt\\\", email =…\n#> $ Biarch                    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ BugReports                <chr> NA, NA, \"https://github.com/Spiritspeak/AATt…\n#> $ BuildKeepEmpty            <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ BuildManual               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ BuildResaveData           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ BuildVignettes            <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Built                     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ ByteCompile               <chr> NA, NA, \"true\", NA, NA, NA, NA, NA, NA, NA, …\n#> $ `Classification/ACM`      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ `Classification/ACM-2012` <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ `Classification/JEL`      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ `Classification/MSC`      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ `Classification/MSC-2010` <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Collate                   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Collate.unix              <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Collate.windows           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Contact                   <chr> NA, NA, NA, NA, NA, NA, NA, NA, \"Ian Morison…\n#> $ Copyright                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Eli…\n#> $ Date                      <chr> \"2015-08-15\", NA, NA, NA, \"2021-12-12\", NA, …\n#> $ `Date/Publication`        <chr> \"2015-08-16 23:05:52\", \"2023-03-01 10:42:09 …\n#> $ Description               <chr> \"Supplies tools for tabulating and analyzing…\n#> $ Encoding                  <chr> NA, \"UTF-8\", \"UTF-8\", \"UTF-8\", \"UTF-8\", NA, …\n#> $ KeepSource                <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Language                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ LazyData                  <chr> NA, NA, \"true\", \"true\", NA, \"true\", NA, NA, …\n#> $ LazyDataCompression       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ LazyLoad                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, \"yes\", N…\n#> $ MailingList               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Maintainer                <chr> \"Scott Fortmann-Roe <scottfr@berkeley.edu>\",…\n#> $ Note                      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Packaged                  <chr> \"2015-08-16 14:17:33 UTC; scott\", \"2023-02-2…\n#> $ RdMacros                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ StagedInstall             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ SysDataCompression        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ SystemRequirements        <chr> NA, NA, NA, NA, NA, NA, NA, NA, \"GNU make\", …\n#> $ Title                     <chr> \"Accurate, Adaptable, and Accessible Error M…\n#> $ Type                      <chr> \"Package\", \"Package\", \"Package\", NA, \"Packag…\n#> $ URL                       <chr> NA, NA, NA, \"https://shiny.abdn.ac.uk/Stats/…\n#> $ UseLTO                    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ VignetteBuilder           <chr> NA, \"knitr\", NA, \"knitr\", NA, \"knitr\", NA, N…\n#> $ ZipData                   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Path                      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ `X-CRAN-Comment`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ Published                 <chr> \"2015-08-16\", \"2023-03-01\", \"2022-08-12\", \"2…\n#> $ `Reverse depends`         <chr> NA, NA, NA, NA, NA, NA, \"abctools, EasyABC\",…\n#> $ `Reverse imports`         <chr> NA, NA, NA, NA, NA, NA, \"ecolottery, poems\",…\n#> $ `Reverse linking to`      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ `Reverse suggests`        <chr> NA, NA, NA, NA, NA, NA, \"coala\", \"abctools\",…\n#> $ `Reverse enhances`        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …"
  },
  {
    "objectID": "posts/2023-05-03_r-developers/index.html#wrangle",
    "href": "posts/2023-05-03_r-developers/index.html#wrangle",
    "title": "The Pareto Principle in R package development",
    "section": "Wrangle",
    "text": "Wrangle\nSince we only care about package and author metadata, a good first step is to remove everything else. This leaves us with a Package field and two author fields: Author and Authors@R. The difference between the two author fields is that Author is an unstructured text field that can contain any text in any format, and Authors@R is a structured text field containing R code that defines authors’ names and roles with the person() function.\n\ncran_pkg_db <- cran_pkg_db |>\n  select(package = Package, authors = Author, authors_r = `Authors@R`) |>\n  as_tibble()\n\nHere’s a comparison of the two fields, using the dplyr package as an example:\n\n# Author\ncran_pkg_db |>\n  filter(package == \"dplyr\") |>\n  pull(authors) |>\n  cat()\n\n#> Hadley Wickham [aut, cre] (<https://orcid.org/0000-0003-4757-117X>),\n#>   Romain François [aut] (<https://orcid.org/0000-0002-2444-4226>),\n#>   Lionel Henry [aut],\n#>   Kirill Müller [aut] (<https://orcid.org/0000-0002-1416-3412>),\n#>   Davis Vaughan [aut] (<https://orcid.org/0000-0003-4777-038X>),\n#>   Posit Software, PBC [cph, fnd]\n\n# Authors@R\ncran_pkg_db |>\n  filter(package == \"dplyr\") |>\n  pull(authors_r) |>\n  cat()\n\n#> c(\n#>     person(\"Hadley\", \"Wickham\", , \"hadley@posit.co\", role = c(\"aut\", \"cre\"),\n#>            comment = c(ORCID = \"0000-0003-4757-117X\")),\n#>     person(\"Romain\", \"François\", role = \"aut\",\n#>            comment = c(ORCID = \"0000-0002-2444-4226\")),\n#>     person(\"Lionel\", \"Henry\", role = \"aut\"),\n#>     person(\"Kirill\", \"Müller\", role = \"aut\",\n#>            comment = c(ORCID = \"0000-0002-1416-3412\")),\n#>     person(\"Davis\", \"Vaughan\", , \"davis@posit.co\", role = \"aut\",\n#>            comment = c(ORCID = \"0000-0003-4777-038X\")),\n#>     person(\"Posit Software, PBC\", role = c(\"cph\", \"fnd\"))\n#>   )\n\n\nAnd a glimpse at the data:\n\ncran_pkg_db\n\n#> # A tibble: 19,473 × 3\n#>    package       authors                                                 autho…¹\n#>    <chr>         <chr>                                                   <chr>  \n#>  1 A3            \"Scott Fortmann-Roe\"                                     <NA>  \n#>  2 AalenJohansen \"Martin Bladt [aut, cre],\\n  Christian Furrer [aut]\"    \"c(per…\n#>  3 AATtools      \"Sercan Kahveci [aut, cre]\"                             \"perso…\n#>  4 ABACUS        \"Mintu Nath [aut, cre]\"                                  <NA>  \n#>  5 abbreviate    \"Sigbert Klinke [aut, cre]\"                             \"\\n  p…\n#>  6 abbyyR        \"Gaurav Sood [aut, cre]\"                                \"perso…\n#>  7 abc           \"Csillery Katalin [aut],\\n  Lemaire Louisiane [aut],\\n… \"c( \\n…\n#>  8 abc.data      \"Csillery Katalin [aut],\\n  Lemaire Louisiane [aut],\\n… \"c( \\n…\n#>  9 ABC.RAP       \"Abdulmonem Alsaleh [cre, aut], Robert Weeks [aut], Ia…  <NA>  \n#> 10 ABCanalysis   \"Michael Thrun, Jorn Lotsch, Alfred Ultsch\"              <NA>  \n#> # … with 19,463 more rows, and abbreviated variable name ¹​authors_r\n\n\nFrom the output above you can see that every package uses the Author field, but not all packages use the Authors@R field. This is unfortunate, because it means that the names and roles of authors need to be extracted from the unstructured text in the Author field for a subset of packages, which is difficult to do and somewhat error-prone. Just for consideration, here’s how many packages don’t use the Authors@R field.\n\ncran_pkg_db |>\n  filter(is.na(authors_r)) |>\n  nrow()\n\n#> [1] 6361\n\n\nSo roughly one-third of all packages. From the output above it’s also clear that although there are similarities in how different packages populate the Author field, it does vary; so a simple rule like splitting the text on commas isn’t sufficient. These are fairly tame examples—some packages even use multiple sentences describing each author’s roles and affiliations, or contain other comments such as copyright disclaimers. All of these things make it more difficult to extract names and roles without errors.\nConversely, for the Authors@R field, all that’s needed is to parse and evaluate the R code stored there as a character string; this will return a person vector that has format() methods to get authors’ names and roles into an analysis-ready format. This removes the possibility for me to introduce errors into the data, although it doesn’t solve things like Authors using an inconsistent name across packages (e.g., sometimes including their middle initial and sometimes not, or just generally writing their name differently).\nBecause there are two fields, I’ll make two helper functions to get name and role data from each field. Regardless of the field, the end goal is to tidy cran_pkg_db into a data frame with three columns: package, person, and roles, with one package/person combination per row.\n\nExtracting from Authors@R\nGetting the data we want from the Authors@R field is pretty straightforward. For the packages where this is used, each one has a vector of person objects stored as a character string like:\n\nmm_string <- \"person(\\\"Michael\\\", \\\"McCarthy\\\", , role = c(\\\"aut\\\", \\\"cre\\\"))\"\n\nmm_string\n\n#> [1] \"person(\\\"Michael\\\", \\\"McCarthy\\\", , role = c(\\\"aut\\\", \\\"cre\\\"))\"\n\n\nWhich can be parsed and evaluated as R code like:\n\nmm_eval <- eval(parse(text = mm_string))\n\nclass(mm_eval)\n\n#> [1] \"person\"\n\n\nThen the format() method for the person class can be used to get names and roles into the format I want simply and accurately.\n\nmm_person <- format(mm_eval, include = c(\"given\", \"family\"))\nmm_roles  <- format(mm_eval, include = c(\"role\"))\ntibble(person = mm_person, roles = mm_roles)\n\n#> # A tibble: 1 × 2\n#>   person           roles     \n#>   <chr>            <chr>     \n#> 1 Michael McCarthy [aut, cre]\n\n\nI’ve wrapped this up into a small helper function, authors_r(), that includes some light tidying steps just to deal with a couple small discrepancies I noticed in a subset of packages.\n\n# Get names and roles from \"person\" objects in the Authors@R field\nauthors_r <- function(x) {\n  # Some light preprocessing is needed to replace the unicode symbol for line\n  # breaks with the regular \"\\n\". This is an edge case from at least one\n  # package.\n  code <- str_replace_all(x, \"\\\\<U\\\\+000a\\\\>\", \"\\n\")\n  persons <- eval(parse(text = code))\n  person <- str_trim(format(persons, include = c(\"given\", \"family\")))\n  roles <- format(persons, include = c(\"role\"))\n  tibble(person = person, roles = roles)\n}\n\nHere’s an example of it with dplyr:\n\ncran_pkg_db |>\n  filter(package == \"dplyr\") |>\n  pull(authors_r) |>\n  # Normalizing names leads to more consistent results with summary statistics\n  # later on, since some people use things like umlauts and accents\n  # inconsistently.\n  stri_trans_general(\"latin-ascii\") |>\n  authors_r()\n\n#> # A tibble: 6 × 2\n#>   person              roles     \n#>   <chr>               <chr>     \n#> 1 Hadley Wickham      [aut, cre]\n#> 2 Romain Francois     [aut]     \n#> 3 Lionel Henry        [aut]     \n#> 4 Kirill Muller       [aut]     \n#> 5 Davis Vaughan       [aut]     \n#> 6 Posit Software, PBC [cph, fnd]\n\n\n\n\nExtracting from Author\nAs I mentioned before, getting the data we want from the Author field is more complicated since there’s no common structure between all packages. I tried a few approaches, including:\n\nChatGPT\nNamed Entity Extraction\nRegular expressions (regex)\n\nChatGPT worked excellently in the few examples I tried; however, OpenAI doesn’t provide free API access, so I had no way of using this with R without paying (which I didn’t want to do). Here’s the prompt I used (note that it would need to be expanded to deal with more edge cases):\n\nSeparate these names with commas and do not include any other information (including a response to the request); if any names are within person() they belong to one person: \n\nNamed Entity Extraction, which is a natural language processing (NLP) method that extracts entities (like peoples’ names) from text, didn’t work very well in the few examples I tried. It didn’t recognize certain names even when the only thing in a sentence was names separated by commas. This is probably my fault more than anything—I’ve never used this method before and didn’t want to spend too much time learning it just for this post, so I used a pre-trained model that probably wasn’t trained on a diverse set of names.\nFortunately, regular expressions actually worked pretty well, so this is the solution I settled on. I tried two approaches to this. First I tried to split the names (and roles) up by commas (and eventually other punctuation as I ran into edge cases). This worked alright; there were clearly errors in the data with this method, but since most packages use a simple structure in the Author field it correctly extracted names from most packages.\nSecond I tried to extract the names (and roles) directly with a regular expression that could match a variety of names. This is the solution I settled on. It still isn’t perfect, but the data is cleaner than with the other method. Regardless, the difference in number of observations between both methods was only in the mid hundreds—so I think any statistics based on this data, although not completely accurate, are still sufficient to get a good idea of the R developer landscape on CRAN.\n\n# This regex was adapted from <https://stackoverflow.com/a/7654214/16844576>.\n# It's designed to capture a wide range of names, including those with\n# punctuation in them. It's tailored to this data, so I don't know how well\n# it would generalize to other situations, but feel free to try it.\npersons_roles <- r\"((\\'|\\\")*[A-Z]([A-Z]+|(\\'[A-Z])?[a-z]+|\\.)(?:(\\s+|\\-)[A-Z]([a-z]+|\\.?))*(?:(\\'?\\s+|\\-)[a-z][a-z\\-]+){0,2}(\\s+|\\-)[A-Z](\\'?[A-Za-z]+(\\'[A-Za-z]+)?|\\.)(?:(\\s+|\\-)[A-Za-z]([a-z]+|\\.))*(\\'|\\\")*(?:\\s*\\[(.*?)\\])?)\"\n# Some packages put the person() code in the wrong field, but it's also\n# formatted incorrectly and throws an error when evaluated, so the best we can\n# do is just extract the whole thing for each person.\nperson_objects <- r\"(person\\((.*?)\\))\"\n\n# Get names and roles from character strings in the Author field\nauthors <- function(x) {\n  # The Author field is unstructured and there are idiosyncrasies between\n  # different packages. The steps here attempt to fix the idiosyncrasies so\n  # authors can be extracted with as few errors as possible.\n  persons <- x |>\n    # Line breaks should be replaced with spaces in case they occur in the\n    # middle of a name.\n    str_replace_all(\"\\\\n|\\\\<U\\\\+000a\\\\>|\\\\n(?=[:upper:])\", \" \") |>\n    # Periods should always have a space after them so initials will be\n    # recognized as part of a name.\n    str_replace_all(\"\\\\.\", \"\\\\. \") |>\n    # Commas before roles will keep them from being included in the regex.\n    str_remove_all(\",(?= \\\\[)\") |>\n    # Get persons and their roles.\n    str_extract_all(paste0(persons_roles, \"|\", person_objects)) |>\n    unlist() |>\n    # Multiple spaces can be replaced with a single space for cleaner names.\n    str_replace_all(\"\\\\s+\", \" \")\n\n  tibble(person = persons) |>\n    mutate(\n      roles  = str_extract(person, \"\\\\[(.*?)\\\\]\"),\n      person = str_remove(\n        str_remove(person, \"\\\\s*\\\\[(.*?)\\\\]\"),\n        \"^('|\\\")|('|\\\")$\" # Some names are wrapped in quotations\n      )\n    )\n}\n\nHere’s an example of it with dplyr. If you compare it to the output from authors_r() above you can see the data quality is still good enough for rock ‘n’ roll, but it isn’t perfect; Posit’s roles are no longer defined because the comma in their name cut off the regex before it captured the roles. So there are some edge cases like this that will create measurement error in the person or roles columns, but I don’t think it’s bad enough to invalidate the results.\n\ncran_pkg_db |>\n  filter(package == \"dplyr\") |>\n  pull(authors) |>\n  stri_trans_general(\"latin-ascii\") |>\n  authors()\n\n#> # A tibble: 6 × 2\n#>   person          roles     \n#>   <chr>           <chr>     \n#> 1 Hadley Wickham  [aut, cre]\n#> 2 Romain Francois [aut]     \n#> 3 Lionel Henry    [aut]     \n#> 4 Kirill Muller   [aut]     \n#> 5 Davis Vaughan   [aut]     \n#> 6 Posit Software  <NA>\n\n\n\n\nExtracting roles\nFrom the example dplyr output above, we can see that the roles column is currently a character string with the role codes, which isn’t super useful. Later on I’ll split these out into indicator columns with a TRUE or FALSE for whether someone had a given role. I also wanted the full names for the roles, since some of the codes aren’t very obvious.\nKurt Hornik, Duncan Murdoch and Achim Zeileis published a nice article in The R Journal explaining the roles of R package authors and where they come from. Briefly, they come from the “Relator and Role” codes and terms from MARC (MAchine-Readable Cataloging, Library of Congress, 2012) here: https://www.loc.gov/marc/relators/relaterm.html.\nThere are a lot of roles there; I just took the ones that were present in the data at the time I wrote this post.\n\nmarc_roles <- c(\n  analyst = \"anl\",\n  architecht = \"arc\",\n  artist = \"art\",\n  author = \"aut\",\n  author_in_quotations = \"aqt\",\n  author_of_intro = \"aui\",\n  bibliographic_antecedent = \"ant\",\n  collector = \"col\",\n  compiler = \"com\",\n  conceptor = \"ccp\",\n  conservator = \"con\",\n  consultant = \"csl\",\n  consultant_to_project = \"csp\",\n  contestant_appellant = \"cot\",\n  contractor = \"ctr\",\n  contributor = \"ctb\",\n  copyright_holder = \"cph\",\n  corrector = \"crr\",\n  creator = \"cre\",\n  data_contributor = \"dtc\",\n  degree_supervisor = \"dgs\",\n  editor = \"edt\",\n  funder = \"fnd\",\n  illustrator = \"ill\",\n  inventor = \"inv\",\n  lab_director = \"ldr\",\n  lead = \"led\",\n  metadata_contact = \"mdc\",\n  musician = \"mus\",\n  owner = \"own\",\n  presenter = \"pre\",\n  programmer = \"prg\",\n  project_director = \"pdr\",\n  scientific_advisor = \"sad\",\n  second_party = \"spy\",\n  sponsor = \"spn\",\n  supporting_host = \"sht\",\n  teacher = \"tch\",\n  thesis_advisor = \"ths\",\n  translator = \"trl\",\n  research_team_head = \"rth\",\n  research_team_member = \"rtm\",\n  researcher = \"res\",\n  reviewer = \"rev\",\n  witness = \"wit\",\n  woodcutter = \"wdc\"\n)\n\n\n\nTidying the data\nWith all the explanations out of the way we can now tidy the data with our helper functions.\n\ncran_authors <- cran_pkg_db |>\n  mutate(\n    # Letters with accents, etc. should be normalized so that names including\n    # them are picked up by the regex.\n    across(c(authors, authors_r), \\(.x) stri_trans_general(.x, \"latin-ascii\")),\n    # The extraction functions aren't vectorized so they have to be mapped over.\n    # This creates a list column.\n    persons = if_else(\n      is.na(authors_r),\n      map(authors, \\(.x) authors(.x)),\n      map(authors_r, \\(.x) authors_r(.x))\n    )\n  ) |>\n  select(-c(authors, authors_r)) |>\n  unnest(persons) |>\n  # If a package only has one author then they must be the author and creator,\n  # so it's safe to impute this when it isn't there.\n  group_by(package) |>\n  mutate(roles = if_else(\n    is.na(roles) & n() == 1, \"[aut, cre]\", roles\n  )) |>\n  ungroup()\n\nThen add the indicator columns for roles. Note the use of the walrus operator (:=) here to create new columns from the full names of MARC roles on the left side of the walrus, while detecting the MARC codes with str_detect() on the right side. I’m mapping over this because the left side can’t be a vector.\n\ncran_authors_tidy <- cran_authors |>\n  # Add indicator columns for all roles.\n  bind_cols(\n    map2_dfc(\n      names(marc_roles), marc_roles,\n      function(.x, .y) {\n        cran_authors |>\n          mutate(!!.x := str_detect(roles, .y)) |>\n          select(!!.x)\n      }\n    )\n  ) |>\n  # Not everyone's role is known.\n  mutate(unknown = is.na(roles))\n\nThis all leaves us with a tidy (mostly error free) data frame about R developers and their roles that is ready to explore:\n\nglimpse(cran_authors_tidy)\n\n#> Rows: 52,719\n#> Columns: 50\n#> $ package                  <chr> \"A3\", \"AalenJohansen\", \"AalenJohansen\", \"AATt…\n#> $ person                   <chr> \"Scott Fortmann-Roe\", \"Martin Bladt\", \"Christ…\n#> $ roles                    <chr> \"[aut, cre]\", \"[aut, cre]\", \"[aut]\", \"[aut, c…\n#> $ analyst                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ architecht               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ artist                   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ author                   <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU…\n#> $ author_in_quotations     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ author_of_intro          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ bibliographic_antecedent <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ collector                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ compiler                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ conceptor                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ conservator              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ consultant               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ consultant_to_project    <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ contestant_appellant     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ contractor               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ contributor              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ copyright_holder         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ corrector                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ creator                  <lgl> TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FA…\n#> $ data_contributor         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ degree_supervisor        <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ editor                   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ funder                   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ illustrator              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ inventor                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ lab_director             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ lead                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ metadata_contact         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ musician                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ owner                    <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ presenter                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ programmer               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ project_director         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ scientific_advisor       <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ second_party             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ sponsor                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ supporting_host          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ teacher                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ thesis_advisor           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ translator               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ research_team_head       <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ research_team_member     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ researcher               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ reviewer                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ witness                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ woodcutter               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#> $ unknown                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…"
  },
  {
    "objectID": "posts/2023-05-03_r-developers/index.html#r-developer-statistics",
    "href": "posts/2023-05-03_r-developers/index.html#r-developer-statistics",
    "title": "The Pareto Principle in R package development",
    "section": "R developer statistics",
    "text": "R developer statistics\nI’ll start with person-level stats, mainly because some of the other stats are further summaries of these statistics. Nothing fancy here, just the number of packages a person has contributed to, role counts, and nominal and percentile rankings. Both the ranking methods used here give every tie the same (smallest) value, so if two people tied for second place both their ranks would be 2, and the next person’s rank would be 4.\n\ncran_author_pkg_counts <- cran_authors_tidy |>\n  group_by(person) |>\n  summarise(\n    n_packages = n(),\n    across(analyst:unknown, function(.x) sum(.x, na.rm = TRUE))\n  ) |>\n  mutate(\n    # Discretizing this for visualization purposes later on\n    n_pkgs_fct = case_when(\n      n_packages == 1 ~ \"One\",\n      n_packages == 2 ~ \"Two\",\n      n_packages == 3 ~ \"Three\",\n      n_packages >= 4 ~ \"Four+\"\n    ),\n    n_pkgs_fct = factor(n_pkgs_fct, levels = c(\"One\", \"Two\", \"Three\", \"Four+\")),\n    rank = min_rank(desc(n_packages)),\n    percentile = percent_rank(n_packages) * 100,\n    .after = n_packages\n  ) |>\n  arrange(desc(n_packages))\n\nHere’s an interactive gt table of the person-level stats so you can find yourself, or ask silly questions like how many other authors share a name with you. If you page or search through it you can also get an idea of the data quality (e.g., try “Posit” under the person column and you’ll see that they don’t use a consistent organization name across all packages, which creates some measurement error here).\n\n\nCode\ncran_author_pkg_counts |>\n  select(-n_pkgs_fct) |>\n  gt() |>\n  tab_header(\n    title = \"R Developer Contributions\",\n    subtitle = \"CRAN Package Authorships and Roles\"\n  ) |>\n  text_transform(\n    \\(.x) str_to_title(str_replace_all(.x, \"_\", \" \")),\n    locations = cells_column_labels()\n  ) |>\n  fmt_number(\n    columns = percentile\n  ) |>\n  fmt(\n    columns = rank,\n    fns = \\(.x) label_ordinal()(.x)\n  ) |>\n  cols_width(everything() ~ px(120)) |>\n  opt_interactive(use_sorting = FALSE, use_filters = TRUE)\n\n\n\n\n\n\nR Developer Contributions\nCRAN Package Authorships and Roles\n\n\n\n\n\n\nSo there are around 29453 people who have some type of authorship on at least one currently available CRAN package at the time this post was published. I’ve emphasized “around” because of the measurement error from extracting names from the Author field of DESCRIPTION and from people writing their names in multiple ways across packages, but also because this number will fluctuate over time as new packages are published, unmaintained packages are archived, and so forth.\nTo try to put this number into perspective, Ben Ubah, Claudia Vitolo, and Rick Pack put together a dashboard with data on how many R users there are worldwide belonging to different R user groups. At the time of writing this post there were:\n\nAround 775,000 members of R user groups organized on Meetup\nAround 100,000 R-Ladies members\n\nThe R Consortium also states on their website that there are more than two million R users worldwide (although they don’t state when or where this number comes from). Regardless of the exact amount, it’s apparent that there are many more R users than R developers.\n\nPackage contributions\nThe title of this post probably gave this away, but around 90% of R developers have worked on one to three packages, and only around 10% have worked on four or more packages.\n\ncran_author_pkg_counts |>\n  group_by(n_pkgs_fct) |>\n  summarise(n_people = n()) |>\n  ggplot(mapping =  aes(x = n_pkgs_fct, y = n_people)) +\n    geom_col() +\n    scale_y_continuous(\n      sec.axis = sec_axis(\n        trans = \\(.x) .x / nrow(cran_author_pkg_counts),\n        name = \"Percent of sample\",\n        labels = label_percent(),\n        breaks = c(0, .05, .10, .15, .70)\n      )\n    ) +\n    labs(\n      x = \"Package contributions\",\n      y = \"People\"\n    )\n\n\n\n\nNotably, in the group that have worked on four or more packages, the spread of package contributions is huge. This vast range is mostly driven by people who do R package development as part of their job (e.g., if you look at the cran_author_pkg_counts table above, most of the people at the very top are either professors of statistics or current or former developers from Posit, rOpenSci, or the R Core Team).\n\ncran_author_pkg_counts |>\n  filter(n_pkgs_fct == \"Four+\") |>\n  group_by(rank, n_packages) |>\n  summarise(n_people = n()) |>\n  ggplot(mapping = aes(x = n_packages, y = n_people)) +\n    geom_segment(aes(xend = n_packages, yend = 0)) +\n    geom_point() +\n    scale_y_continuous(\n      sec.axis = sec_axis(\n        trans = \\(.x) .x / nrow(cran_author_pkg_counts),\n        name = \"Percent of sample\",\n        labels = label_percent()\n      )\n    ) +\n    labs(\n      x = \"Package contributions\",\n      y = \"People\"\n    )\n\n\n\n\nHere are some subsample summary statistics to compliment the plots above.\n\ncran_author_pkg_counts |>\n  group_by(n_packages >= 4) |>\n  summarise(\n    n_developers = n(),\n    n_pkgs_mean = mean(n_packages),\n    n_pkgs_sd = sd(n_packages),\n    n_pkgs_median = median(n_packages),\n    n_pkgs_min = min(n_packages),\n    n_pkgs_max = max(n_packages)\n  )\n\n#> # A tibble: 2 × 7\n#>   `n_packages >= 4` n_developers n_pkgs_mean n_pkgs_sd n_pkgs_…¹ n_pkg…² n_pkg…³\n#>   <lgl>                    <int>       <dbl>     <dbl>     <dbl>   <int>   <int>\n#> 1 FALSE                    27107        1.27     0.562         1       1       3\n#> 2 TRUE                      2346        7.78     8.63          5       4     202\n#> # … with abbreviated variable names ¹​n_pkgs_median, ²​n_pkgs_min, ³​n_pkgs_max\n\n\n\n\nRole distributions\nNot every contribution to an R package involves code. For example, two authors of the wiad package were woodcutters! The package is for wood image analysis, so although it’s surprising a role like that exists, it makes a lot of sense in context. Anyways, neat factoids aside, the point of this section is to look at the distribution of different roles in R package development.\nTo start, let’s get an idea of how many people were involved in programming-related roles. This won’t be universally true, but most of the time the following roles will involve programming:\n\nprogramming_roles <-\n  c(\"author\", \"creator\", \"contributor\", \"compiler\", \"programmer\")\n\nHere’s the count:\n\ncran_author_pkg_counts |>\n  filter(if_any(!!programming_roles, \\(.x) .x > 0)) |>\n  nrow()\n\n#> [1] 24170\n\n\nThere were also 5434 whose role was unknown (either because it wasn’t specified or wasn’t picked up by my regex method). Regardless, most people have been involved in programming-related roles, and although other roles occur they’re relatively rare.\nHere’s a plot to compliment this point:\n\ncran_authors_tidy |>\n  summarise(across(analyst:unknown, function(.x) sum(.x, na.rm = TRUE))) |>\n  pivot_longer(cols = everything(), names_to = \"role\", values_to = \"n\") |>\n  arrange(desc(n)) |>\n  ggplot(mapping = aes(x = n, y = reorder(role, n))) +\n    geom_segment(aes(xend = 0, yend = role)) +\n    geom_point() +\n    labs(\n      x = \"Count across packages\",\n      y = \"Role\"\n    )\n\n\n\n\n\n\nRanking contributions\nThe interactive table above already contains this information, but to compliment David Smith’s post from 5 years ago, here’s the current Top 20 most prolific authors on CRAN.\n\n\nThis is why Hadley is on the cover of Glamour magazine and we’re not.   \n\ncran_author_pkg_counts |>\n  # We don't want organizations or groups here\n  filter(!(person %in% c(\"RStudio\", \"R Core Team\", \"Posit Software, PBC\"))) |>\n  head(20) |>\n  select(person, n_packages) |>\n  gt() |>\n  tab_header(\n    title = \"Top 20 R Developers\",\n    subtitle = \"Based on number of CRAN package authorships\"\n  ) |>\n  text_transform(\n    \\(.x) str_to_title(str_replace_all(.x, \"_\", \" \")),\n    locations = cells_column_labels()\n  ) |>\n  cols_width(person ~ px(140))\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Top 20 R Developers\n    \n    \n      Based on number of CRAN package authorships\n    \n    \n      Person\n      N Packages\n    \n  \n  \n    Hadley Wickham\n159\n    Jeroen Ooms\n89\n    Gabor Csardi\n82\n    Kurt Hornik\n78\n    Scott Chamberlain\n76\n    Dirk Eddelbuettel\n75\n    Martin Maechler\n74\n    Stephane Laurent\n73\n    Achim Zeileis\n68\n    Winston Chang\n51\n    Max Kuhn\n50\n    Yihui Xie\n47\n    Jim Hester\n46\n    Henrik Bengtsson\n45\n    John Muschelli\n45\n    Roger Bivand\n43\n    Ben Bolker\n42\n    Bob Rudis\n42\n    Brian Ripley\n42\n    Michel Lang\n41"
  },
  {
    "objectID": "posts/2023-05-03_r-developers/index.html#conclusion",
    "href": "posts/2023-05-03_r-developers/index.html#conclusion",
    "title": "The Pareto Principle in R package development",
    "section": "Conclusion",
    "text": "Conclusion\nMy main takeaway from all of this is that if you know how to write and publish an R package on CRAN (or contribute to existing packages), you have a valuable skill that not a lot of other R users have. If you do want to learn, I recommend reading R Packages by Hadley Wickham and Jenny Bryan.\nMy other takeaway is that the Author field should be dropped from DESCRIPTION so my eyesore of a regular expression never has to extract a name again. (This still wouldn’t remove all the measurement error I discussed, since some people and organizations don’t write their names consistently across packages. Oh well.).\nOne thing I am curious about, but which would be hard to get good data on, is how many people have R package development experience who haven’t published on CRAN; or, of the people who have published on CRAN, how many packages have they worked on that aren’t (yet) on CRAN (for me it’s five).\nAnyways, that’s it for now. If you think this data could answer other interesting questions I didn’t cover, let me know down below and I’ll consider adding more to the post."
  },
  {
    "objectID": "posts/2023-05-03_r-developers/index.html#section",
    "href": "posts/2023-05-03_r-developers/index.html#section",
    "title": "The Pareto Principle in R package development",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "posts/2023-05-03_r-developers/index.html#michael-mccarthy",
    "href": "posts/2023-05-03_r-developers/index.html#michael-mccarthy",
    "title": "The Pareto Principle in R package development",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "posts/2023-05-03_r-developers/index.html#comments",
    "href": "posts/2023-05-03_r-developers/index.html#comments",
    "title": "The Pareto Principle in R package development",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/2023-05-03_r-developers/index.html#session-info",
    "href": "posts/2023-05-03_r-developers/index.html#session-info",
    "title": "The Pareto Principle in R package development",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2023-05-03\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.313 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.1.0   2023-01-29 [1] CRAN (R 4.2.0)\n forcats     * 0.5.2   2022-08-19 [1] CRAN (R 4.2.0)\n ggplot2     * 3.4.0   2022-11-04 [1] CRAN (R 4.2.0)\n gt          * 0.9.0   2023-03-31 [1] CRAN (R 4.2.0)\n purrr       * 0.3.5   2022-10-06 [1] CRAN (R 4.2.0)\n readr       * 2.1.3   2022-10-01 [1] CRAN (R 4.2.0)\n scales      * 1.2.1   2022-08-20 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi     * 1.7.8   2022-07-11 [1] CRAN (R 4.2.0)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.2.0)\n tibble      * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyr       * 1.2.1   2022-09-08 [1] CRAN (R 4.2.0)\n tidyverse   * 1.3.2   2022-07-18 [1] CRAN (R 4.2.0)\n\n [1] /Users/Michael/Library/R/x86_64/4.2/library/__tidytales\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html",
    "href": "posts/2021-06-15_demons-souls/index.html",
    "title": "Go forth, slayer of Demons",
    "section": "",
    "text": "On the first day\nMan was granted a soul\nAnd with it, clarity\nOn the second day\nupon Earth was planted\nan irrevocable poison\nA soul-devouring demon\n\nDemon’s Souls is an action role-playing video game set in the dark fantasy kingdom of Boletaria, a land cursed with a deep, terrible fog brought forth by an ancient soul-devouring demon called the Old One. To lift the curse and mend the world players must slay and absorb the souls of five powerful archdemons, whereafter they can face the Old One and lull it back to slumber. Demon’s Souls is renowned for its challenge and design, and has made a lasting impact on the video game industry. It is also the progenitor of what has become one of my favourite video game franchises."
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#theming-inspiration",
    "href": "posts/2021-06-15_demons-souls/index.html#theming-inspiration",
    "title": "Go forth, slayer of Demons",
    "section": "Theming Inspiration",
    "text": "Theming Inspiration\nHero text appears on the screen whenever the player performs a significant action in Demon’s Souls, such as slaying a demon or, infamously, dying themselves. These provide a great design reference for plot theming.\n\n\n\n\n\n\n\n\nIn-game screenshots of the hero text from Demon’s Souls.\n\n\n\n\nDemon’s Souls also has a unique logo whose design I want to reference.\n\n\n\n\n\nThe Demon’s Souls logo.\n\n\n\n\nI want to translate these design elements to my plot like so:\n\nThe Optimus Princeps font can be used for plot text.\nThe yellow and red colours for the hero text can be used to contrast successes and failures, respectively.\nThe distressed, broken style of the Demon’s Souls logo can be used for the plot’s title.\n\nApplying these elements to my plot will help it fit the Demon’s Souls aesthetic."
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#prerequisites",
    "href": "posts/2021-06-15_demons-souls/index.html#prerequisites",
    "title": "Go forth, slayer of Demons",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nlibrary(tidyverse)\nlibrary(ggfx)\nlibrary(magick)\n\nI’ll be using PlayStation Network trophy data for my plot. The data contains statistics for the percent of players who have slain a given boss in Demon’s Souls out of all the players who have ever played the game. I have constructed the data manually since Sony does not provide an API to access PlayStation Network trophy data programmatically. Demon’s Souls was released on February 5, 2009, so it is unlikely these stats will change much in the future.\n\n# Tribbles are not just useful for scaring Klingons, they make it easy to\n# create tibbles too\ndemons_souls <- tribble(\n  ~boss,            ~boss_type,  ~location,              ~archstone, ~percent_completed,\n  \"Phalanx\",        \"Demon\",     \"Boletarian Palace\",    \"1-1\",      63.1,               \n  \"Tower Knight\",   \"Demon\",     \"Boletarian Palace\",    \"1-2\",      46.6,               \n  \"Penetrator\",     \"Demon\",     \"Boletarian Palace\",    \"1-3\",      30.3,               \n  \"False King\",     \"Archdemon\", \"Boletarian Palace\",    \"1-4\",      24.2,               \n  \"Armor Spider\",   \"Demon\",     \"Stonefang Tunnel\",     \"2-1\",      43.9,               \n  \"Flamelurker\",    \"Demon\",     \"Stonefang Tunnel\",     \"2-2\",      35.1,               \n  \"Dragon God\",     \"Archdemon\", \"Stonefang Tunnel\",     \"2-3\",      33.1,               \n  \"Fool’s Idol\",    \"Demon\",     \"Tower of Latria\",      \"3-1\",      35.7,               \n  \"Maneater\",       \"Demon\",     \"Tower of Latria\",      \"3-2\",      28.7,               \n  \"Old Monk\",       \"Archdemon\", \"Tower of Latria\",      \"3-3\",      27.7,               \n  \"Adjudicator\",    \"Demon\",     \"Shrine of Storms\",     \"4-1\",      36.1,               \n  \"Old Hero\",       \"Demon\",     \"Shrine of Storms\",     \"4-2\",      28.8,               \n  \"Storm King\",     \"Archdemon\", \"Shrine of Storms\",     \"4-3\",      28.1,               \n  \"Leechmonger\",    \"Demon\",     \"Valley of Defilement\", \"5-1\",      32.5,               \n  \"Dirty Colossus\", \"Demon\",     \"Valley of Defilement\", \"5-2\",      27.2,               \n  \"Maiden Astraea\", \"Archdemon\", \"Valley of Defilement\", \"5-3\",      26.6\n) %>%\n  mutate(across(boss_type:archstone, as_factor))\n\ndemons_souls"
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#wrangle",
    "href": "posts/2021-06-15_demons-souls/index.html#wrangle",
    "title": "Go forth, slayer of Demons",
    "section": "Wrangle",
    "text": "Wrangle\nThe data is already structured the way I want it for my plot, but there are still some interesting things to explore through wrangling and summary stats.\nWithin each location, players have to slay each demon in the order specified by the archstones. For example, in the Boletarian Palace a player cannot face the Tower Knight before they have slain the Phalanx. So each location has a first, second, and third boss (and the Boletarian Palace has a fourth that can only be faced after slaying all the other demons). This can be used to get an imperfect idea of player attrition in the game.\n\n# Detect the order of bosses based on archstone suffix\ndemons_souls <- demons_souls %>%\n  mutate(\n    archstone_boss = case_when(\n      str_detect(archstone, \"-1\") ~ \"First\",\n      str_detect(archstone, \"-2\") ~ \"Second\",\n      str_detect(archstone, \"-3\") ~ \"Third\",\n      str_detect(archstone, \"-4\") ~ \"Fourth (False King)\"\n    ),\n    archstone_boss = as_factor(archstone_boss),\n    .after = archstone\n  )\n\ndemons_souls\n\n\n\n  \n\n\n\nNow, there are two ways to go about getting this imperfect idea of player attrition in the game. The first involves using the entire data set.\n\n# Calculate the average percent of players who have slain the first, second,\n# ..., archstone boss across locations. \ndemons_souls %>%\n  group_by(archstone_boss) %>%\n  summarise(average_completed = mean(percent_completed))\n\n\n\n  \n\n\n\nThe second involves removing the Phalanx from the data set due to its influential pull on the average for the first archstone boss. It has a much higher completion percent (63.1%) than the other bosses in the game, and the reason for this is that the Phalanx is the first boss in the game. Players must slay it before they can go to face the first archstone boss from other locations in the game. Removing the Phalanx might give a more accurate picture of average completion for first archstone bosses.\n\n# Trophy earned: Slayer of Demon \"Phalanx\"\ndemons_souls %>%\n  filter(boss != \"Phalanx\") %>%\n  group_by(archstone_boss) %>%\n  summarise(average_completed = mean(percent_completed))\n\n\n\n  \n\n\n\nWith the Phalanx’s influence removed, it looks like there is roughly a 4% drop in average completion for each successive archstone boss. In order to face the False King players must first slay every other demon and archdemon in the game, so it is interesting the drop stays consistent there. Most players who made it far enough to slay their first archdemon then went on to slay the rest.\n\n\nUmbassa.\nAbout one quarter of Demon’s Souls players persisted to the end of the game. But three quarters did not. Assuming most players at least attempted each location, then averaging by location can give an imperfect idea of their overall difficulty for players during their first playthrough.\n\n# Calculate the average completion rate by location, arranged from \"easiest\" to\n# \"hardest\"\ndemons_souls %>%\n  group_by(location) %>%\n  summarise(average_completed = mean(percent_completed)) %>%\n  arrange(desc(average_completed))\n\n\n\n  \n\n\n\nIt looks like there are two clusters here, an easier one with the Boletarian Palace and Stonefang Tunnel, and a harder one with Shrine of Storms, Tower of Latria, and the Valley of Defilement. I finished my first playthrough of the game in 2012, so I only have distant memories to reflect on, but this ranking looks sound to me. For experienced players I think this ranking is less relevant. Once you’re experienced most of the variability in difficulty comes down to the character build you choose."
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#visualize",
    "href": "posts/2021-06-15_demons-souls/index.html#visualize",
    "title": "Go forth, slayer of Demons",
    "section": "Visualize",
    "text": "Visualize\n\n# Define aliases for plot fonts and colours\noptimus <- \"OptimusPrinceps\"\noptimus_b <- \"OptimusPrincepsSemiBold\"\nyellow <- \"#ffaf24\" #  #fec056\n\nThe plot I want to make is inspired by this Tidy Tuesday plot by Georgios Karamanis. I used Georgios’ code as a starting point, then modified it to get the behaviour and result I wanted.\nThe centrepiece of the plot is the coloured text that shows the percent of Demon’s Souls players who have completed a given boss in yellow and who have not in red. This effect is achieved by applying a rectangular filter over the text that only allows the portion of the text within the filter’s borders to be shown. Doing this once for yellow text and once for red text allows the full string to appear, with the ratio of colours within a boss’s name reflecting the percent of players that have completed it. A few calculations are needed in order for the ratios to be accurate, and for the text to look aesthetically pleasing.\n\ndemons_souls_plot <- demons_souls %>%\n  mutate(\n    # Percentages need to be in decimal form for the calculations and plotting\n    # to work properly\n    percent_completed = percent_completed/100,\n    boss = fct_reorder(toupper(boss), percent_completed),\n    # In order to justify text to the same width, a ratio of how many times\n    # each string would fit into the widest string needs to be calculated. This\n    # can then be multiplied by an arbitrary value to determine the final size\n    # for each string of text.\n    str_width = strwidth(boss, family = optimus_b, units = \"inches\") * 25.4, # in millimetres\n    str_ratio = max(str_width)/str_width,\n    text_size = 4.9 * str_ratio,\n    # The division here is arbitrary, its effect is reflected in the scale of the\n    # y-axis\n    tile_height = text_size / 10\n  ) %>%\n  # Bosses will appear from top to bottom based on completion ratios. The\n  # calculation here accounts for the differences in text size for each string.\n  arrange(percent_completed) %>%\n  mutate(y = cumsum(lag(tile_height/2, default = 0) + tile_height/2))\n\nNow the plot can be constructed. The final code for the plot is roughly 100 lines long, so I’ve hidden it in the section below. However, there are a few parts of the code I want to highlight before showing the final plot.\n\n\nShow Code\n# The trick for geom spacing is to set the size of the plot from the start\nfile <- tempfile(fileext = '.png')\nragg::agg_png(file, width = 4, height = 5.5, res = 300, units = \"in\")\n\nggplot(demons_souls_plot) +\n  # Make it easier to see where 50% is using a vertical line. geom_segment() is\n  # used here instead of geom_vline() because the latter goes up into the title\n  # text. An empty data frame is supplied so that only one copy of the geom is\n  # drawn.\n  geom_segment(aes(\n    x = 0,\n    xend = 0,\n    y = 10.9,\n    yend = 0,\n    size = 0.6),\n    data = data.frame(),\n    alpha = 0.3,\n    colour = \"grey\",\n    lineend = \"round\",\n    linetype = \"twodash\"\n  ) +\n  scale_alpha_identity() +\n  \n  # Set bounding box for yellow portion of centrepiece text\n  as_reference(\n    geom_rect(aes(\n      xmin = -0.5,\n      xmax = -0.5 + ((percent_completed)),\n      ymin = y - (tile_height * 0.5),\n      ymax = y + (tile_height * 0.5)\n    )), \n    id = \"demon_vanquished\"\n  ) +\n  # Only show the portion of yellow centrepiece text located within the\n  # bounding box\n  with_blend(\n    geom_text(aes(\n      x = 0,\n      y = y,\n      label = boss,\n      size = text_size\n    ),\n    colour = yellow,\n    family = optimus_b),\n    bg_layer = \"demon_vanquished\",\n    blend_type = \"in\"\n  ) +\n  # Set bounding box for red portion of centrepiece text\n  as_reference(\n    geom_rect(aes(\n      xmin = 0.5 - ((1 - percent_completed)),\n      xmax = 0.5,\n      ymin = y - (tile_height * 0.5),\n      ymax = y + (tile_height * 0.5)\n    )), \n    id = \"you_died\"\n  ) +\n  # Only show the portion of red centrepiece text located within the bounding\n  # box\n  with_blend(\n    geom_text(aes(\n      x = 0,\n      y = y,\n      label = boss,\n      size = text_size\n    ),\n    colour = \"red\",\n    family = optimus_b),\n    bg_layer = \"you_died\",\n    blend_type = \"in\"\n  ) +\n  \n  # Draw \"axis\" for Demon Vanquished\n  annotate(\n    \"text\",\n    x = -0.65,\n    y = 7.75,\n    label = \"demon vanquished\",\n    angle = 90,\n    size = 5,\n    family = optimus,\n    colour = yellow\n  ) +\n  geom_segment(aes(\n    x = -0.645,\n    xend = -0.645,\n    y = 10.05,\n    yend = 10.45),\n    lineend = \"round\",\n    colour = yellow,\n    size = 0.3,\n    arrow = arrow(angle = 45, length = unit(1, \"mm\"), type = \"open\")\n  ) +\n  # Draw \"axis\" for You Died\n  annotate(\n    \"text\",\n    x = 0.65,\n    y = 4.65,\n    label = \"you died\",\n    angle = 270,\n    size = 5,\n    family = optimus,\n    colour = \"red\"\n  ) +\n  geom_segment(aes(\n    x = 0.645,\n    xend = 0.645,\n    y = 3.51,\n    yend = 3.01),\n    lineend = \"round\",\n    colour = \"red\",\n    size = 0.3,\n    arrow = arrow(angle = 45, length = unit(1, \"mm\"), type = \"open\")\n  ) +\n  \n  # Draw a title surrounded by line decorations at the top of the panel\n  geom_segment(aes(\n    x = -0.75,\n    xend = 0.75,\n    y = 13.2,\n    yend = 13.2,\n    size = 0.3),\n    lineend = \"round\",\n    colour = \"grey\"\n  ) +\n  annotate(\n    \"text\",\n    x = 0,\n    y = 12.325,\n    size = 7,\n    family = optimus_b,\n    colour = \"white\",\n    lineheight = 0.75,\n    label = \"DEMON’S SOULS\\nBOSS COMPLETION\"\n  ) +\n  geom_segment(aes(\n    x = -0.025,\n    xend = -0.75,\n    y = 11.4,\n    yend = 11.4,\n    size = 0.3),\n    lineend = \"round\",\n    colour = \"grey\"\n  ) +\n  geom_segment(aes(\n    x = 0.025,\n    xend = 0.75,\n    y = 11.4,\n    yend = 11.4,\n    size = 0.3),\n    lineend = \"round\",\n    colour = \"grey\"\n  ) +\n  annotate(\n    \"point\",\n    x  = 0,\n    y = 11.4,\n    colour = \"grey\",\n    shape = 5,\n    size = 2,\n    stroke = 0.6\n  ) +\n  annotate(\n    \"point\",\n    x  = 0,\n    y = 11.4,\n    colour = \"grey\",\n    shape = 5,\n    size = 0.75\n  ) +\n  \n  # Draw plot caption\n  annotate(\n    \"text\",\n    x = 1,\n    y = 10.33,\n    angle = 270,\n    hjust = 0,\n    size = 3,\n    alpha = 0.3,\n    label = \"SOURCE: PLAYSTATION NETWORK | GRAPHIC: MICHAEL MCCARTHY\",\n    family = optimus,\n    color = \"grey\"\n  ) +\n  \n  # Make sure the text size calculated for each string is used so that strings\n  # are justified\n  scale_size_identity() +\n  # Take axis limits exactly from data so there's no spacing around the panel,\n  # allow drawing outside of the panel for annotations, and set the axis limits\n  # to match the limits of the text.\n  coord_cartesian(expand = FALSE, clip = \"off\", xlim = c(-0.5, 0.5)) +\n  # Specify the panel size manually. This makes it easier to position plot\n  # elements with absolute positions.\n  ggh4x::force_panelsizes(rows = unit(5, \"in\"), # height\n                          cols = unit(1.8, \"in\")) + # width\n  theme_void() +\n  theme(\n    legend.position = \"none\",\n    plot.margin = unit(c(0.5, 4, 0.5, 4), \"in\"),\n    plot.background = element_rect(fill = \"black\", color = NA))\n\ninvisible(dev.off())\n\n# Apply a mask texture to the final image to mimic the style of the Demon's\n# Souls logo in the plot title\nmask <- image_read(\n  here(\"posts\", \"2021-06-15_demons-souls\", \"images\", \"texture.png\")\n  ) %>%\n  image_transparent(\"white\") %>%\n  image_threshold(\"black\", \"90%\")\n\nfinal_plot <- image_composite(image_read(file), mask, operator = \"Over\")\n\n\nFirst, the code behind the coloured centrepiece text. It uses ggfx::as_reference() and ggfx::with_blend() to selectively apply a filter over portions of the text, as I discussed earlier. The boundaries of the filter are defined by the ggplot2 geom inside of ggfx::as_reference(), then ggfx::with_blend() applies a filter specified by blend_type to the ggplot2 geom inside of it. By duplicating this process twice—once for yellow text and again for red text—but with different filter boundaries based on the percent completed and not completed, the entire boss name is displayed with accurate colour fills.\n\n  # Set bounding box for yellow portion of centrepiece text\n  as_reference(\n    geom_rect(aes(\n      xmin = -0.5,\n      xmax = -0.5 + ((percent_completed)),\n      ymin = y - (tile_height * 0.5),\n      ymax = y + (tile_height * 0.5)\n    )), \n    id = \"demon_vanquished\"\n  ) +\n  # Only show the portion of yellow centrepiece text located within the\n  # bounding box\n  with_blend(\n    geom_text(aes(\n      x = 0,\n      y = y,\n      label = boss,\n      size = text_size\n    ),\n    colour = yellow,\n    family = optimus_b),\n    bg_layer = \"demon_vanquished\",\n    blend_type = \"in\"\n  ) +\n   # Set bounding box for red portion of centrepiece text\n  as_reference(\n    geom_rect(aes(\n      xmin = 0.5 - ((1 - percent_completed)),\n      xmax = 0.5,\n      ymin = y - (tile_height * 0.5),\n      ymax = y + (tile_height * 0.5)\n    )), \n    id = \"you_died\"\n  ) +\n  # Only show the portion of red centrepiece text located within the bounding\n  # box\n  with_blend(\n    geom_text(aes(\n      x = 0,\n      y = y,\n      label = boss,\n      size = text_size\n    ),\n    colour = \"red\",\n    family = optimus_b),\n    bg_layer = \"you_died\",\n    blend_type = \"in\"\n  )\n\nSecond, the code behind the distressed, broken style of the title text. This one is actually quite simple. It uses magick::image_composite() to apply a texture mask I made in Krita over the composed plot. The mask has a transparent background with black lines located over the space where the plot title is. Both the composed plot and mask images have the same dimensions, so when they’re composed together the effect is applied exactly where I want it.\n\nimage_composite(plot, mask, operator = \"Over\")\n\nFinally, I just wanted to note that the decorative lines around the plot’s title text are actually made up of ggplot2 geoms. I used two ggplot2::geom_point() geoms with different sizes to create the diamond on the bottom line."
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#final-graphic",
    "href": "posts/2021-06-15_demons-souls/index.html#final-graphic",
    "title": "Go forth, slayer of Demons",
    "section": "Final Graphic",
    "text": "Final Graphic"
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#section",
    "href": "posts/2021-06-15_demons-souls/index.html#section",
    "title": "Go forth, slayer of Demons",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#michael-mccarthy",
    "href": "posts/2021-06-15_demons-souls/index.html#michael-mccarthy",
    "title": "Go forth, slayer of Demons",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#comments",
    "href": "posts/2021-06-15_demons-souls/index.html#comments",
    "title": "Go forth, slayer of Demons",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#session-info",
    "href": "posts/2021-06-15_demons-souls/index.html#session-info",
    "title": "Go forth, slayer of Demons",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2022-12-21\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.280 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.0.10  2022-09-01 [1] CRAN (R 4.2.0)\n forcats     * 0.5.2   2022-08-19 [1] CRAN (R 4.2.0)\n ggfx        * 1.0.1   2022-08-22 [1] CRAN (R 4.2.0)\n ggplot2     * 3.4.0   2022-11-04 [1] CRAN (R 4.2.0)\n here        * 1.0.1   2020-12-13 [1] CRAN (R 4.2.0)\n magick      * 2.7.3   2021-08-18 [1] CRAN (R 4.2.0)\n purrr       * 0.3.5   2022-10-06 [1] CRAN (R 4.2.0)\n readr       * 2.1.3   2022-10-01 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.2.0)\n tibble      * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyr       * 1.2.1   2022-09-08 [1] CRAN (R 4.2.0)\n tidyverse   * 1.3.2   2022-07-18 [1] CRAN (R 4.2.0)\n\n [1] /Users/Michael/Library/R/x86_64/4.2/library/__tidytales\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#data",
    "href": "posts/2021-06-15_demons-souls/index.html#data",
    "title": "Go forth, slayer of Demons",
    "section": "Data",
    "text": "Data\n\nDownload the data used in this post."
  },
  {
    "objectID": "posts/2021-06-15_demons-souls/index.html#fair-dealing",
    "href": "posts/2021-06-15_demons-souls/index.html#fair-dealing",
    "title": "Go forth, slayer of Demons",
    "section": "Fair Dealing",
    "text": "Fair Dealing\n\nAny of the trademarks, service marks, collective marks, design rights or similar rights that are mentioned, used, or cited in this article are the property of their respective owners. They are used here as fair dealing for the purpose of education in accordance with section 29 of the Copyright Act and do not infringe copyright."
  },
  {
    "objectID": "posts/2022-06-16_projectile-motion/index.html",
    "href": "posts/2022-06-16_projectile-motion/index.html",
    "title": "On motion",
    "section": "",
    "text": "Projectile motion describes the motion of an object launched into the air whose trajectory after launch is influenced only by the force of gravity and for which air resistance is negligible. Projectile motion was first accurately described by Galileo Galilei in his book Two New Sciences, published in 1638. In what he dubbed compound motion, Galileo demonstrated that projectile motion can be broken down into independent horizontal and vertical components that can be analyzed separately to describe an object’s trajectory. He used this principle to prove that the trajectory of an object in projectile motion will always follow a curve in the shape of a parabola.\n\n\n\n\n\nProjectile motion of an object launched at the same height and velocity but different angles. The symmetrical U-shaped curve of each trajectory is known as a parabola.\n\n\n\n\nGalileo used an inclined plane to demonstrate his principle of compound motion. I’m going to use R."
  },
  {
    "objectID": "posts/2022-06-16_projectile-motion/index.html#post-inspiration",
    "href": "posts/2022-06-16_projectile-motion/index.html#post-inspiration",
    "title": "On motion",
    "section": "Post Inspiration",
    "text": "Post Inspiration\nI recently finished Outer Wilds, an open world adventure game set in a strange, constantly evolving solar system trapped in an endless time loop. It’s a great game and I really enjoyed solving the mysteries of the solar system. The reason I bring it up here is that, on top of being a great game, the physics realism of Outer Wilds is something that really stood out to me.\nFor instance, at the start of the game there’s a museum exhibit consisting of a low table and three gently rolling balls that appear to move around on their own.\n\n\nEmphasis on gently. Wait for it…\n\nBut the balls are not actually moving on their own. Their motion is being affected by the moon’s gravity. As the placard beside them explains: “As it orbits our planet, the Attlerock’s gravity pulls on objects from different directions. In fact, it’s pulling on you right now!” This isn’t just flavour text—the game is actually simulating planetary gravity.\nOuter Wilds uses its physics realism to great effect in other parts of the game (ever wondered what it’s like to stand on planet while it breaks apart into a black hole?), and experiencing it all got me curious: What kinds of physics simulations could I do in the two-dimensional space of a plot? After some research and reading I landed on projectile motion."
  },
  {
    "objectID": "posts/2022-06-16_projectile-motion/index.html#prerequisites",
    "href": "posts/2022-06-16_projectile-motion/index.html#prerequisites",
    "title": "On motion",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(ggh4x)\nlibrary(formattable)\nlibrary(emojifont)\nlibrary(glue)\n\nI’ll be simulating data for my plot by turning the equations for projectile motion into R functions. You can download this data with the Data Source link in the appendix. The sources I used for the equations can also be found in the appendix."
  },
  {
    "objectID": "posts/2022-06-16_projectile-motion/index.html#simulate",
    "href": "posts/2022-06-16_projectile-motion/index.html#simulate",
    "title": "On motion",
    "section": "Simulate",
    "text": "Simulate\nThe equations for projectile motion use a common set of variables which are listed below. The equations assume that the force of air resistance is negligible\n\n\\begin{align*}\nV &\\leftarrow \\textrm{initial velocity}, \\\\\nV_x &\\leftarrow \\textrm{horizontal velocity}, \\\\\nV_y &\\leftarrow \\textrm{vertical velocity}, \\\\\n\\alpha &\\leftarrow \\textrm{launch angle}, \\\\\nh &\\leftarrow \\textrm{initial height}, \\\\\nt &\\leftarrow \\textrm{time of flight}, \\\\\nd &\\leftarrow \\textrm{distance (range)}, \\\\\nh_{\\textrm{max}} &\\leftarrow \\textrm{maximum height}, \\\\\ng &\\leftarrow \\textrm{gravity}.\n\\end{align*}\n\n\nHorizontal and vertical velocity\nThe horizontal velocity, V_x, and vertical velocity, V_y, of an object moving in projectile motion are given by the equations\n\n\\begin{align*}\nV_x &= V \\times \\cos(\\alpha), \\textrm{ and} \\\\\nV_y &= V \\times \\sin(\\alpha),\n\\end{align*}\n\nwhere V is the initial velocity and \\alpha is the launch angle. Horizontal and vertical velocity can be computed in R with the following functions.\n\nvelocity_x <- function(velocity, angle) {\n  # Degrees need to be converted to radians in cos() since that is what the\n  # function uses\n  velocity * cos(angle * (pi/180))\n}\n\nvelocity_y <- function(velocity, angle) {\n  # Degrees need to be converted to radians in sin() since that is what the\n  # function uses\n  velocity * sin(angle * (pi/180))\n}\n\n\n\nTime of flight\nThe time of flight, t, of an object moving in projectile motion is given by the equation\n\nt = \\left(V_y + \\sqrt{V_y^2 + 2 \\times g \\times h}\\right) \\div g,\n\nwhere V_y is the vertical velocity, g is the force of gravity, and h, is the initial height the object is launched from. Time of flight is the time from when the object is launched to the time the object reaches the surface. It can be computed in R with the following function.\n\nflight_time <- function(velocity_y, height, gravity = 9.80665) {\n  ( velocity_y + sqrt(velocity_y^2 + 2 * gravity * height) ) / gravity\n}\n\n\n\nDistance (range)\nThe distance, d, or range travelled by an object moving in projectile motion is given by the equation\n\nd = V_x \\times t,\n\nwhere V_x is the horizontal velocity and t is the time of flight. The range of the projectile is the total horizontal distance travelled during the time of flight. It can be computed in R with the following function.\n\ndistance <- function(velocity_x, velocity_y, height, gravity = 9.80665) {\n  velocity_x * ( velocity_y + sqrt(velocity_y^2 + 2 * gravity * height) ) /\n  gravity\n}\n\n\n\nMaximum height\nThe maximum height, h_{\\textrm{max}}, reached by an object moving in projectile motion is given by the equation\n\nh_{\\textrm{max}} = h + V_y^2 \\div (2 \\times g),\n\nwhere h is the initial height, V_y is the vertical velocity, and g is the force of gravity. The maximum height is reached when V_y = 0. It can be computed in R with the following function.\n\nheight_max <- function(velocity_y, height, gravity = 9.80665) {\n  height + velocity_y^2 / (2 * gravity)\n}\n\n\n\nProjectile motion calculator\nNow to wrap all the components into a single function that will calculate the result for each component based on a set of parameters given to it. These results can then be used to determine the position and velocity of the projectile at any point in time during its trajectory, which I want to return as a data frame that can be used for plotting.\n\n#' nframes and fps can be used to animate the trajectory as close to real time as possible.\n#' There will be some rounding error though so it won't be exactly the same as the flight\n#' time.\nprojectile_motion <- function(velocity, angle, height, gravity = 9.80665, nframes = 30) {\n  \n  # Velocity components\n  vx <- velocity_x(velocity, angle)\n  vy <- velocity_y(velocity, angle)\n  # Flight components\n  t  <- flight_time(vy, height, gravity)\n  d  <- distance(vx, vy, height, gravity)\n  # Max height components\n  hm <- height_max(vy, height, gravity)\n  th <- vy / gravity\n  hd <- vx * th\n  \n  # Calculate the position of the projectile in 2D space at a given point in\n  # time to approximate its trajectory over time\n  x_pos <- map_dbl(seq(0, t, length = nframes), ~ {\n    vx * .x\n  })\n  \n  y_pos <- map_dbl(seq(0, t, length = nframes), ~ {\n    height + ( vy * .x + 0.5 * -gravity * .x^2 )\n  })\n  \n  # Calculate the vertical velocity of the projectile at a given point in time\n  vy_t  <- map_dbl(seq(0, t, length = nframes), ~ {\n    vy - gravity * .x\n  })\n  \n  trajectory <- data.frame(\n    x = x_pos,\n    y = y_pos,\n    vx = vx,\n    vy = vy_t,\n    second = seq(0, t, length = nframes)\n  )\n  \n  # Return a list with all calculated values\n  list(\n    velocity_x = vx,\n    velocity_y = vy,\n    flight_time = t,\n    distance = d,\n    max_height = hm,\n    max_height_time = th,\n    max_height_dist = hd,\n    trajectory = trajectory,\n    nframes = nframes,\n    fps = nframes/t\n  )\n  \n}"
  },
  {
    "objectID": "posts/2022-06-16_projectile-motion/index.html#wrangle",
    "href": "posts/2022-06-16_projectile-motion/index.html#wrangle",
    "title": "On motion",
    "section": "Wrangle",
    "text": "Wrangle\nInstead of jumping straight into a visualization, I want to play around with the output of projectile_motion(). First to show what its output looks like, and second to explore the interesting ways it can be extended through wrangling.\n\nA simple trajectory\nFirst off, demonstrating projectile_motion() and its output. The function takes five arguments:\n\nvelocity in metres per second,\nangle in degrees,\nheight in metres per second,\ngravity in metres per second (this defaults to Earth’s gravity, 9.80665 m/s), and\nnframes which represents how many points in time to record in the trajectory data frame.\n\n\nprojectile_motion(\n  velocity = 11.4,\n  angle = 52.1,\n  height = 18,\n  nframes = 10\n)\n\n#> $velocity_x\n#> [1] 7.002851\n#> \n#> $velocity_y\n#> [1] 8.995559\n#> \n#> $flight_time\n#> [1] 3.041533\n#> \n#> $distance\n#> [1] 21.29941\n#> \n#> $max_height\n#> [1] 22.12578\n#> \n#> $max_height_time\n#> [1] 0.9172917\n#> \n#> $max_height_dist\n#> [1] 6.423657\n#> \n#> $trajectory\n#>            x            y       vx          vy    second\n#> 1   0.000000 1.800000e+01 7.002851   8.9955586 0.0000000\n#> 2   2.366601 2.048003e+01 7.002851   5.6814194 0.3379481\n#> 3   4.733201 2.184005e+01 7.002851   2.3672802 0.6758963\n#> 4   7.099802 2.208006e+01 7.002851  -0.9468589 1.0138444\n#> 5   9.466402 2.120007e+01 7.002851  -4.2609981 1.3517926\n#> 6  11.833003 1.920007e+01 7.002851  -7.5751373 1.6897407\n#> 7  14.199604 1.608006e+01 7.002851 -10.8892765 2.0276889\n#> 8  16.566204 1.184005e+01 7.002851 -14.2034156 2.3656370\n#> 9  18.932805 6.480029e+00 7.002851 -17.5175548 2.7035852\n#> 10 21.299405 3.552714e-15 7.002851 -20.8316940 3.0415333\n#> \n#> $nframes\n#> [1] 10\n#> \n#> $fps\n#> [1] 3.287815\n\n\nThe function returns calculations for each of the projectile motion equations I covered above, as well as some additional output that can be used for plotting and animation:\n\nmax_height_time and max_height_dist give the time (s) and distance (m) it takes for the projectile to reach its maximum height.\ntrajectory gives the horizontal and vertical position and velocity at a given moment during the projectile’s trajectory.\nfps gives the number of frames per second that are needed to animate the trajectory in real time based on nframes. Because it is impossible to have a fraction of a frame there will be variance in how closely an animation’s duration matches the actual time of flight based on the value of nframes.\n\n\n\nLaunching a projectile on different planets\nGiven the inspiration for this post, a space themed simulation seems appropriate. Here I want to test how the gravity of each planet in our solar system influences projectile motion, given a projectile is launched with the same velocity, angle, and height.\nFirst I need to construct a named vector of the gravity of each planet in our solar system. NASA provides these values came as ratios of each planet’s gravity relative to Earth, so I had to multiply each one by Earth’s gravity to get the units correct.\n\n# All values are in metres per second\nplanets <- c(\n  mercury = 3.7069137,\n  venus   = 8.8946315,\n  earth   = 9.80665,\n  moon    = 1.6279039,\n  mars    = 3.697107,\n  jupiter = 23.143694,\n  saturn  = 8.9828914,\n  uranus  = 8.7181118,\n  neptune = 10.983448,\n  pluto   = 0.6962721\n)\n\nThen I can create a named list of projectile motion calculations, one for each planet. Each planet has its own list of output from projectile_motion(), so the resulting list of projectile motion calculations is actually a list of lists. This can be tidied into a tibble to make it easier to work with.\n\n# Calculate projectile motion for each planet, given the same velocity,\n# angle, and height\nplanets_pm <- map(planets, ~{\n  projectile_motion(\n    velocity = 20,\n    angle = 45,\n    height = 35,\n    gravity = .x,\n    nframes = 100)\n})\n\n# Tidying the list of lists into a tibble makes it easier to work with. Note\n# that the trajectory column is a list column since it contains the trajectory\n# data frame for each planet.\nplanets_df <- planets_pm %>%\n  enframe() %>%\n  unnest_wider(value) %>%\n  rename(planet = name)\n\nplanets_trajectory <- planets_df %>%\n  select(planet, trajectory) %>%\n  unnest(trajectory) %>% \n  mutate(planet = factor(planet, levels = names(planets)))"
  },
  {
    "objectID": "posts/2022-06-16_projectile-motion/index.html#visualize",
    "href": "posts/2022-06-16_projectile-motion/index.html#visualize",
    "title": "On motion",
    "section": "Visualize",
    "text": "Visualize\nNow for visualization. First I’ll plot a simple trajectory, then a projectile launched on different planets.\n\nA simple trajectory\nThis is the same simple trajectory I showed the output for earlier, only with more frames to make the animation smoother.\n\nsimple_trajectory <- projectile_motion(\n  velocity = 11.4,\n  angle = 52.1,\n  height = 18,\n  nframes = 100\n)\n\n# Assign the data frame and max height parameters to objects to make the plot\n# code easier to read\ndf <- simple_trajectory$trajectory\nmax_height_dist <- simple_trajectory$max_height_dist\nmax_height_time <- simple_trajectory$max_height_time\nmax_height <- simple_trajectory$max_height\n\nI’m going to build the plot for this simple trajectory up in chunks to make the code easier to understand. The foundation of the plot is fairly standard. The only unusual thing here are the group aesthetics in geom_line() and geom_point(). These are used to tell gganimate which rows in the data correspond to the same graphic element.\n\np <- ggplot(df, aes(x = x, y = y)) +\n  geom_line(\n    aes(group = 1),\n    linetype = \"dashed\",\n    colour = \"red\",\n    alpha = 0.5\n  ) +\n  geom_point(aes(group = 1), size = 2)\n\nFor the data I simulated, the projectile starts with a positive vertical velocity. However, at its maximum height, the vertical velocity of the projectile becomes 0 m/s for a brief moment, as it stops rising and starts falling. This happens Because gravity is constantly influencing the vertical velocity of the projectile. This is an important and interesting piece of information I want to communicate in my plot. This can be accomplished subtly by displaying the vertical velocity of the projectile at each point in time, or more overtly using a text annotation. I’m going to do both.\nFirst the text annotation. I’m using geom_curve() to draw an arrow between the annotation and the point at which the projectile is at its maximum height, and geom_text() to draw the annotation. I’ve supplied each geom with its own data frame containing a second column whose sole value corresponds to the time the projectile reaches its maximum height. This will control when the annotation appears in the animation. I’ve also given the pair a different group aesthetic from geom_line() and geom_point().\n\np <- p +\n  geom_curve(\n    data = data.frame(\n      second = max_height_time\n    ),\n    aes(\n      xend = max_height_dist,\n      yend = max_height + 0.2,\n      x = max_height_dist + 2,\n      y = max_height + 2,\n      group = 2\n    ),\n    curvature = 0.45,\n    angle = 105,\n    ncp = 15,\n    arrow = arrow(length = unit(0.1,\"cm\"), type = \"closed\")\n  ) +\n  geom_text(\n    data = data.frame(\n      second = max_height_time\n    ),\n    aes(\n      x = max_height_dist + 2.4,\n      y = max_height + 2,\n      group = 2\n    ),\n    hjust = \"left\",\n    lineheight = 1,\n    family = \"serif\",\n    label = str_c(\n      \"At its maximum height, the vertical velocity \\n\", \n      \"of the projectile is 0 m/s for a brief moment, \\n\",\n      \"as it stops rising and starts falling.\"\n    )\n  )\n\nSecond the vertical velocity. I’m displaying this in the plot’s subtitle along with the time elapsed since the projectile was launched. The displayed values are updated each frame using the value returned by the expression enclosed in glue braces for a frame. The variable frame_along is made available by gganimate::transition_along() (see below) and gives the position on the along-dimension (time in seconds in this case) that a frame corresponds to. Here I’m using frame_along to display the elapsed time, and to index the data frame df for the vertical velocity at a given second. The latter is a slight workaround because vy cannot be accessed directly in the glue braces.\n\np <- p +\n  labs(\n    title = str_c(\n      \"Projectile motion of an object launched with \",\n      #\" <br> \",\n      \"a speed of 11.4 m/s at an angle of 52.1°\"\n    ),\n    subtitle = str_c(\n      \"Time: \",\n      \"{formattable(frame_along, digits = 2, format = 'f')}s\",\n      \"\\n\",\n      \"Vertical velocity = \",\n      \"{formattable(df$vy[df$second == frame_along], digits = 2, format = 'f')}\",\n      \" m/s\"\n    ),\n    x = \"Distance (m)\",\n    y = \"Height (m)\",\n    caption = \"Graphic: Michael McCarthy\"\n  )\n\nNow for theming. I wanted something minimalistic with a scientific feel—the classic theme paired with truncated axes courtesy of ggh4x does this nicely. Finally, I originally planned to use element_markdown() from ggtext to enable markdown text in the subtitle of the plot so that vertical velocity could be written like \\textrm{Velocity}_Y; however, this caused issues with the text spacing when rendering the animation to video, so I opted not to.1\n\np <- p +\n  guides(x = \"axis_truncated\", y = \"axis_truncated\") +\n  theme_classic(base_family = \"serif\")\n\nAnd finally, the animation code. Yes, that’s it. Animations can be tweaked and spiced up with other functions in gganimate, but I ran into issues making the ones I wanted to use work with transition_reveal().\n\n\nJust a note: The behaviour of transition_reveal() shown here was broken in v1.0.8 of gganimate.\n\nanim <- p +\n  transition_reveal(second)\n\nanim\n\n\n\n\n\n\n\nLaunching a projectile on different planets\nNow to test how the gravity of each planet in our solar system influences projectile motion. As a reminder, I already simulated the projectile motion data in planets_trajectory, so now it’s just a matter of plotting it.\nSince the simulation is space themed, the plot should be too. Instead of using a simple point to represent the projectile, I’m going to use Font Awesome’s rocket icon by way of the emojifont package. To make it extra, I’ll also add propulsion and rotation to the rocket’s animation.\n\n# Make it so the propulsion is only present for first half of animation, so it\n# looks like the rockets are launching.\nrocket_propulsion <- planets_trajectory %>%\n  group_by(planet) %>%\n  mutate(retain = rep(c(TRUE, FALSE), each = 50)) %>%\n  ungroup() %>%\n  mutate(x = case_when(\n    retain == FALSE ~ NA_real_,\n    TRUE ~ x\n  ))\n\nThe plotting code is mostly boilerplate, but I’ve added comments to highlight a few noteworthy points.\n\np <- ggplot(planets_trajectory, aes(x = x, y = y)) +\n  geom_line(\n    aes(colour = planet, group = planet),\n    linetype = \"dashed\",\n    alpha = 0.5,\n    # Change the key glyph in the legend to a point, to represent a planet\n    key_glyph = \"point\"\n  ) +\n  geom_point(\n    data = rocket_propulsion,\n    aes(group = planet),\n    colour = \"orange\"\n  ) +\n  # Change the angle at different frames to rotate the rocket\n  geom_text(\n    aes(colour = planet, group = planet, label = fontawesome(\"fa-rocket\")),\n    family='fontawesome-webfont',\n    angle = rep(seq(0, 45, length = 100), 10),\n    size = 6,\n    # There is no rocket key glyph, so override this too\n    key_glyph = \"point\"\n  ) +\n  scale_color_manual(\n    values = c(\n      \"#97979F\",\n      \"#BBB7AB\",\n      \"#8CB1DE\",\n      \"#DAD9D7\",\n      \"#E27B58\",\n      \"#C88B3A\",\n      \"#C5AB6E\",\n      \"#93B8BE\",\n      \"#6081FF\",\n      \"#4390BA\"\n    )\n  ) +\n  labs(\n    title = str_c(\n      \"projectile motion of an object launched on different planets in our solar system\"\n    ),\n    x = \"distance (m)\",\n    y = \"height (m)\",\n    caption = \"graphic: michael mccarthy\"\n  ) +\n  guides(\n    x = \"axis_truncated\",\n    y = \"axis_truncated\",\n    colour = guide_legend(title.vjust = .7, nrow = 1, label.position = \"bottom\")\n  ) +\n  theme_classic(base_family = \"mono\") +\n  theme(\n    text = element_text(colour = \"white\"),\n    axis.text = element_text(colour = \"white\"),\n    rect = element_rect(fill = \"black\"),\n    panel.background = element_rect(fill = \"black\"),\n    axis.ticks = element_line(colour = \"white\"),\n    axis.line = element_line(colour = \"white\"),\n    legend.position = \"top\",\n    legend.justification = \"left\"\n  )\n\nFinally, the animation code. The shadow_wake() function is applied to the orange points used for rocket propulsion to really sell the effect.\n\nanim <- p +\n  transition_reveal(second) +\n  shadow_wake(wake_length = 0.1, size = 2, exclude_layer = c(1, 3))"
  },
  {
    "objectID": "posts/2022-06-16_projectile-motion/index.html#section",
    "href": "posts/2022-06-16_projectile-motion/index.html#section",
    "title": "On motion",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "posts/2022-06-16_projectile-motion/index.html#michael-mccarthy",
    "href": "posts/2022-06-16_projectile-motion/index.html#michael-mccarthy",
    "title": "On motion",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "posts/2022-06-16_projectile-motion/index.html#comments",
    "href": "posts/2022-06-16_projectile-motion/index.html#comments",
    "title": "On motion",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/2022-06-16_projectile-motion/index.html#session-info",
    "href": "posts/2022-06-16_projectile-motion/index.html#session-info",
    "title": "On motion",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2022-12-24\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.280 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.0.10  2022-09-01 [1] CRAN (R 4.2.0)\n emojifont   * 0.5.5   2021-04-20 [1] CRAN (R 4.2.0)\n forcats     * 0.5.2   2022-08-19 [1] CRAN (R 4.2.0)\n formattable * 0.2.1   2021-01-07 [1] CRAN (R 4.2.0)\n gganimate   * 1.0.7   2020-10-15 [1] CRAN (R 4.2.2)\n ggh4x       * 0.2.3   2022-11-09 [1] CRAN (R 4.2.0)\n ggplot2     * 3.4.0   2022-11-04 [1] CRAN (R 4.2.0)\n glue        * 1.6.2   2022-02-24 [1] CRAN (R 4.2.0)\n here        * 1.0.1   2020-12-13 [1] CRAN (R 4.2.0)\n purrr       * 0.3.5   2022-10-06 [1] CRAN (R 4.2.0)\n readr       * 2.1.3   2022-10-01 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.2.0)\n tibble      * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyr       * 1.2.1   2022-09-08 [1] CRAN (R 4.2.0)\n tidyverse   * 1.3.2   2022-07-18 [1] CRAN (R 4.2.0)\n\n [1] /Users/Michael/Library/R/x86_64/4.2/library/__tidytales\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-05-10_r-developers-github/index.html",
    "href": "posts/2023-05-10_r-developers-github/index.html",
    "title": "We are, we are, on the cruise! We R!",
    "section": "",
    "text": "In my previous post I explored R package developer statistics on CRAN, focusing on developers with package authorship. However, because many of the packages published on CRAN are hosted on open source repositories (like GitHub), many R packages have additional contributors who do not appear in the list of authors.\n\n\n\n\n\n\n\n\nKonrad Rudolph @klmr@mastodon.social\n\n\n\n@mccarthymg Very interesting (though not surprising)!One caveat is that most big packages have many more contributors than declared authors, because people often only submit a handful of patches, and are consequently not added you the authors list (to be clear, I don't think that's a problem; just something to keep in mind).\n\n3:12 PM - May 4, 2023 (UTC)\n\n\n\n\n\nAlthough this information isn’t available through CRAN, GitHub has a REST API that we can use to get data on the contributors of R packages hosted on GitHub. We can use this to answer new questions about R package contributions by the wider R community.\nAgain, if you just want to see the stats, you can skip to the R contributor statistics section. Otherwise follow along to see how I retrieved and wrangled the data into a usable state. I’d also like to give a big thanks to everyone who engaged with me on my previous post (and some of the stuff in-between); you all inspired this post and some of the approaches I chose to take below."
  },
  {
    "objectID": "posts/2023-05-10_r-developers-github/index.html#prerequisites",
    "href": "posts/2023-05-10_r-developers-github/index.html#prerequisites",
    "title": "We are, we are, on the cruise! We R!",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nlibrary(tidyverse)\nlibrary(stringi)\nlibrary(gh)\nlibrary(gt)\nlibrary(ggrepel)\nlibrary(scales)\nlibrary(igraph)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(palettes)\n\nI’ll be using the CRAN package repository data returned by tools::CRAN_package_db() again in this post, this time to get the following DESCRIPTION fields from each package: BugReports and URL. Each of these fields can contain the URL for a package’s GitHub repository, so I’m taking both for better coverage.\n\n\nSince this data will change over time, here’s when tools::CRAN_package_db() was run for reference: 2023-05-06.\n\ncran_pkgs_db <- tools::CRAN_package_db() |>\n  select(\n    package = Package,\n    authors = Author,\n    authors_r = `Authors@R`,\n    bug_reports = BugReports,\n    url = URL\n  ) |>\n  as_tibble()\n\nI’ve also selected the Author and Authors@R fields again so I can rerun the author extraction code from the previous post. This will let us ask more interesting questions later on. I just need package and author names, so the code is slightly different from the previous post.\nI’ve hidden the code since there’s no need to reexplain it—all that matters is that this returns a two column data frame called cran_authors, with columns package and person.\n\n\nCode\n# Helper functions ------------------------------------------------------------\n\n# Get names from \"person\" objects in the Authors@R field\nauthors_r <- function(x) {\n  code <- str_replace_all(x, \"\\\\<U\\\\+000a\\\\>\", \"\\n\")\n  persons <- eval(parse(text = code))\n  person <- str_trim(format(persons, include = c(\"given\", \"family\")))\n  tibble(person = person)\n}\n\n# Get names from character strings in the Author field\npersons_roles <- r\"((\\'|\\\")*[A-Z]([A-Z]+|(\\'[A-Z])?[a-z]+|\\.)(?:(\\s+|\\-)[A-Z]([a-z]+|\\.?))*(?:(\\'?\\s+|\\-)[a-z][a-z\\-]+){0,2}(\\s+|\\-)[A-Z](\\'?[A-Za-z]+(\\'[A-Za-z]+)?|\\.)(?:(\\s+|\\-)[A-Za-z]([a-z]+|\\.))*(\\'|\\\")*(?:\\s*\\[(.*?)\\])?)\"\nperson_objects <- r\"(person\\((.*?)\\))\"\n\nauthors <- function(x) {\n  persons <- x |>\n    str_replace_all(\"\\\\n|\\\\<U\\\\+000a\\\\>|\\\\n(?=[:upper:])\", \" \") |>\n    str_replace_all(\"\\\\.\", \"\\\\. \") |>\n    str_remove_all(\",(?= \\\\[)\") |>\n    str_extract_all(paste0(persons_roles, \"|\", person_objects)) |>\n    unlist() |>\n    str_replace_all(\"\\\\s+\", \" \")\n\n  tibble(person = persons) |>\n    mutate(\n      person = str_remove(\n        str_remove(person, \"\\\\s*\\\\[(.*?)\\\\]\"),\n        \"^('|\\\")|('|\\\")$\"\n      )\n    )\n}\n\n# Get data used in the previous post ------------------------------------------\ncran_authors <- cran_pkgs_db |>\n  select(package, authors, authors_r) |>\n  as_tibble() |>\n  mutate(\n    across(c(authors, authors_r), \\(.x) stri_trans_general(.x, \"latin-ascii\")),\n    persons = if_else(\n      is.na(authors_r),\n      map(authors, \\(.x) authors(.x)),\n      map(authors_r, \\(.x) authors_r(.x))\n    )\n  ) |>\n  select(-c(authors, authors_r)) |>\n  unnest(persons) |>\n  # Some unicode characters from authors_r were sneaking through despite the\n  # first normalization call. Also making everything title case to improve\n  # joining results later.\n  mutate(person = str_to_title(stri_trans_general(person, \"latin-ascii\")))"
  },
  {
    "objectID": "posts/2023-05-10_r-developers-github/index.html#wrangle",
    "href": "posts/2023-05-10_r-developers-github/index.html#wrangle",
    "title": "We are, we are, on the cruise! We R!",
    "section": "Wrangle",
    "text": "Wrangle\nI’ll be using the gh package to query GitHub’s REST API. However, before any queries can be done, I need to figure out which packages in cran_pkgs_db have GitHub repositories and what their URLs are.\nFinding the packages with GitHub repositories is simple—it’s just a matter of filtering cran_pkgs_db down to the subset of packages with GitHub’s base URL.\n\ngh_base_url <- r\"(https:\\/\\/github.com\\/)\"\n\ncran_gh_repos <- cran_pkgs_db |>\n  filter(str_detect(bug_reports, gh_base_url) | str_detect(url, gh_base_url))\n\nglimpse(cran_gh_repos)\n\n#> Rows: 9,182\n#> Columns: 5\n#> $ package     <chr> \"AATtools\", \"abbreviate\", \"abclass\", \"abctools\", \"abdiv\", …\n#> $ authors     <chr> \"Sercan Kahveci [aut, cre]\", \"Sigbert Klinke [aut, cre]\", …\n#> $ authors_r   <chr> \"person(\\\"Sercan\\\", \\\"Kahveci\\\", email = \\\"sercan.kahveci@…\n#> $ bug_reports <chr> \"https://github.com/Spiritspeak/AATtools/issues\", NA, \"htt…\n#> $ url         <chr> NA, \"https://github.com/sigbertklinke/abbreviate (developm…\n\n\nOf the 19483 packages on CRAN, 9182 have a GitHub URL in the BugReports or URL field.\nNext step: get the repository URLs. Again, regular expressions are our friend. This one is a simple search for the owner and repository name following the GitHub base URL in the form of owner/repo, following GitHub’s rules for the allowable characters in user and repository names. After getting that it’s just a matter of tidying the data up a bit more to make it query-ready.\n\nowner_repo <-\n  r\"((?<=https:\\/\\/github.com\\/)([A-Za-z0-9\\-]{1,39}\\/[A-Za-z0-9\\-\\_\\.]+))\"\n\ncran_gh_repos <- cran_gh_repos |>\n  mutate(\n    # Get the owner and repo name from a GitHub URL in the form: owner/repo\n    across(c(bug_reports, url), \\(.x) str_extract(.x, owner_repo)),\n    repo_url = case_when(\n      # There shouldn't be a need to check for both columns being NA, since\n      # at least one column should have the info we want due to the filter()\n      # call at the start of this pipe.\n      is.na(bug_reports) & !is.na(url) ~ url,\n      !is.na(bug_reports) & is.na(url) ~ bug_reports,\n      bug_reports == url ~ bug_reports\n    ),\n    # We just need everything before or after the / to get the owner and repo.\n    owner = str_extract(repo_url, \".+(?=\\\\/)\"),\n    repo = str_extract(repo_url, \"(?<=\\\\/).+\")\n  ) |>\n  select(package, owner, repo) |>\n  # Despite the previous comment, some packages only include their username\n  # or organization in the URL, which we can't make use of and which return\n  # NA. These should be omitted. There aren't that many (< 100).\n  na.omit()\n\ncran_gh_repos\n\n#> # A tibble: 9,115 × 3\n#>    package        owner         repo          \n#>    <chr>          <chr>         <chr>         \n#>  1 AATtools       Spiritspeak   AATtools      \n#>  2 abbreviate     sigbertklinke abbreviate    \n#>  3 abclass        wenjie2wang   abclass       \n#>  4 abctools       dennisprangle abctools      \n#>  5 abdiv          kylebittinger abdiv         \n#>  6 abess          abess-team    abess         \n#>  7 abjutils       abjur         abjutils      \n#>  8 ABM            junlingm      ABM           \n#>  9 abstr          a-b-street    abstr         \n#> 10 AcademicThemes hwarden162    AcademicThemes\n#> # … with 9,105 more rows\n\n\nThe GitHub REST API has a rate limit of 5000 requests per hour for authenticated users, so the repositories have to be queried in chunks instead of doing them all in one go. To accomodate this, cran_gh_repos can be split into a list of data frames that can be mapped over when calling the API. I’ve chosen groups of 4000 here just to avoid accidentally hitting the limit.\n\nn_per_group <- 4000\nn_groups <- ceiling(nrow(cran_gh_repos)/n_per_group)\n\nquery_groups <- cran_gh_repos |>\n  mutate(\n    # The result of the rep() call needs to be subset to the max number of\n    # observations to avoid a recycling error with vctrs.\n    query_group = paste0(\n      \"group_\", rep(1:n_groups, each = n_per_group)[1:nrow(cran_gh_repos)]\n    )\n  ) |>\n  group_by(query_group) |>\n  nest() |>\n  deframe()\n\nquery_groups\n\n#> $group_1\n#> # A tibble: 4,000 × 3\n#>    package        owner         repo          \n#>    <chr>          <chr>         <chr>         \n#>  1 AATtools       Spiritspeak   AATtools      \n#>  2 abbreviate     sigbertklinke abbreviate    \n#>  3 abclass        wenjie2wang   abclass       \n#>  4 abctools       dennisprangle abctools      \n#>  5 abdiv          kylebittinger abdiv         \n#>  6 abess          abess-team    abess         \n#>  7 abjutils       abjur         abjutils      \n#>  8 ABM            junlingm      ABM           \n#>  9 abstr          a-b-street    abstr         \n#> 10 AcademicThemes hwarden162    AcademicThemes\n#> # … with 3,990 more rows\n#> \n#> $group_2\n#> # A tibble: 4,000 × 3\n#>    package         owner          repo           \n#>    <chr>           <chr>          <chr>          \n#>  1 letsR           macroecology   letsR          \n#>  2 levitate        lewinfox       levitate       \n#>  3 LexFindR        maglab-uconn   LexFindR       \n#>  4 lexicon         trinker        lexicon        \n#>  5 LexisNexisTools JBGruber       LexisNexisTools\n#>  6 LexisPlotR      ottlngr        LexisPlotR     \n#>  7 lexRankr        AdamSpannbauer lexRankr       \n#>  8 LFApp           fpaskali       LFApp          \n#>  9 lfc             erhard-lab     lfc            \n#> 10 lfda            terrytangyuan  lfda           \n#> # … with 3,990 more rows\n#> \n#> $group_3\n#> # A tibble: 1,115 × 3\n#>    package      owner         repo        \n#>    <chr>        <chr>         <chr>       \n#>  1 stringb      petermeissner stringb     \n#>  2 stringdist   markvanderloo stringdist  \n#>  3 stringfish   traversc      stringfish  \n#>  4 stringi      gagolews      stringi     \n#>  5 stringr      tidyverse     stringr     \n#>  6 stringstatic rossellhayes  stringstatic\n#>  7 stringx      gagolews      stringx     \n#>  8 strip        paulponcet    strip       \n#>  9 striprtf     kota7         striprtf    \n#> 10 stRoke       agdamsbo      stRoke      \n#> # … with 1,105 more rows\n\n\nWe’re now ready to get the contributors for each package. The code here is based on the code used to thank contributors in the R for Data Science book, so if you want to understand the general idea I’d recommend running that code. Otherwise this is just an extension of that code, using list columns, rectangling, and error handling to make it work over multiple repositories.\n\ncontributors_all <- query_groups |>\n  map_dfr(\n    \\(query_group) {\n      # All packages will be queried within the group one-by-one. We just\n      # need package names, usernames, and commit numbers; everything else\n      # can be dropped.\n      query_result <- query_group |>\n        rowwise() |>\n        mutate(\n          contributors_json = list(\n            # Some package repos don't exist anymore (404), which triggers an\n            # error. This needs to be handled in order for the rest of the\n            # queries to continue running.\n            tryCatch(\n              expr = {\n                gh::gh(\n                  \"/repos/:owner/:repo/contributors\",\n                  owner = owner,\n                  repo = repo,\n                  .limit = Inf\n                )\n              },\n              error = \\(unused_argument) {\n                # For consistency with `gh()`, return a list with the keys we\n                # want but with NA values.\n                list(list(login = NA_character_, contributions = NA_integer_))\n              }\n            )\n          ),\n          contributors = list(tibble(\n            login = map_chr(contributors_json, \"login\"),\n            n_commits = map_int(contributors_json, \"contributions\")\n          ))\n        ) |>\n        select(-c(owner, repo, contributors_json)) |>\n        unnest(contributors)\n\n      # Once a group is done, rest for an hour to avoid the rate limit.\n      Sys.sleep(3600)\n      \n      query_result\n    }\n  )\n\ncontributors_all\n\n#> # A tibble: 35,939 × 3\n#>    package    login          n_commits\n#>    <chr>      <chr>              <int>\n#>  1 AATtools   Spiritspeak           76\n#>  2 abbreviate sigbertklinke          4\n#>  3 abclass    wenjie2wang          297\n#>  4 abctools   dennisprangle         50\n#>  5 abdiv      kylebittinger        136\n#>  6 abess      Mamba413            1113\n#>  7 abess      oooo26               816\n#>  8 abess      Jiang-Kangkang       312\n#>  9 abess      zaza0209             249\n#> 10 abess      bbayukari             97\n#> # … with 35,929 more rows\n\n\nAs you can see from the output above, contributors are returned as logins rather than names; so the API will need to be queried a second time to get the names corresponding to each login. However, to avoid redundancy and reduce the total number of queries, we should only take the unique login names from the contributor data.\n\n# We only want real people, not bots or other automated tools. Most bots have a\n# common naming scheme for their login, but there are exceptions. May as well\n# drop the NA entries here too, since it's convenient.\ncontributors_all <- contributors_all |>\n  filter(\n    !(\n      login %in% c(\n        \"actions-user\",\n        \"cran-robot\",\n        \"fossabot\",\n        \"ImgBotApp\",\n        \"traviscibot\",\n        NA_character_\n      ) |\n      str_detect(login, \"\\\\[bot\\\\]$|\\\\-bot$\")\n    ) \n  )\n\ncontributors_unique <- tibble(login = unique(contributors_all$login))\n\ncontributors_unique\n\n#> # A tibble: 15,318 × 1\n#>    login         \n#>    <chr>         \n#>  1 Spiritspeak   \n#>  2 sigbertklinke \n#>  3 wenjie2wang   \n#>  4 dennisprangle \n#>  5 kylebittinger \n#>  6 Mamba413      \n#>  7 oooo26        \n#>  8 Jiang-Kangkang\n#>  9 zaza0209      \n#> 10 bbayukari     \n#> # … with 15,308 more rows\n\n\nWe also need to make query groups for contributors_unique to avoid hitting the rate limit, since logins need to be queried one-by-one.\n\n# We need to make query groups here too to avoid hitting the rate limit.\nn_per_group <- 4000\nn_groups <- ceiling(nrow(contributors_unique)/n_per_group)\n\nquery_groups <- contributors_unique |>\n  mutate(\n    query_group = paste0(\n      \"group_\", rep(1:n_groups, each = n_per_group)[1:nrow(contributors_unique)]\n    )\n  ) |>\n  group_by(query_group) |>\n  nest() |>\n  deframe()\n\nquery_groups\n\n#> $group_1\n#> # A tibble: 4,000 × 1\n#>    login         \n#>    <chr>         \n#>  1 Spiritspeak   \n#>  2 sigbertklinke \n#>  3 wenjie2wang   \n#>  4 dennisprangle \n#>  5 kylebittinger \n#>  6 Mamba413      \n#>  7 oooo26        \n#>  8 Jiang-Kangkang\n#>  9 zaza0209      \n#> 10 bbayukari     \n#> # … with 3,990 more rows\n#> \n#> $group_2\n#> # A tibble: 4,000 × 1\n#>    login      \n#>    <chr>      \n#>  1 eipi10     \n#>  2 fh-kpikhart\n#>  3 agable-vt  \n#>  4 hs3180     \n#>  5 klmedeiros \n#>  6 mnfn       \n#>  7 shea-parkes\n#>  8 shearerp   \n#>  9 teramonagi \n#> 10 peekxc     \n#> # … with 3,990 more rows\n#> \n#> $group_3\n#> # A tibble: 4,000 × 1\n#>    login        \n#>    <chr>        \n#>  1 tlamadon     \n#>  2 NickSalkowski\n#>  3 duncandoo    \n#>  4 chroetz      \n#>  5 jtoloe       \n#>  6 kindlychung  \n#>  7 mhofert      \n#>  8 nachocab     \n#>  9 sietse       \n#> 10 tibastral    \n#> # … with 3,990 more rows\n#> \n#> $group_4\n#> # A tibble: 3,318 × 1\n#>    login        \n#>    <chr>        \n#>  1 liamblake    \n#>  2 lgaborini    \n#>  3 perluna      \n#>  4 mikemc       \n#>  5 VictorSuarezL\n#>  6 asmae-toumi  \n#>  7 fxdlmatt     \n#>  8 leondap      \n#>  9 LurN         \n#> 10 Rhan43       \n#> # … with 3,308 more rows\n\n\nAgain, this query is just a generalization of the R for Data Science contributors code. Here we’re getting users’ real names from their login names, then tidying up.\n\ncontributors_tidy <- query_groups |>\n  map_dfr(\n    \\(query_group) {\n      # All logins will be queried within the group one-by-one.\n      query_result <- query_group |>\n        rowwise() |>\n        mutate(\n          users_json = list(\n            # Some user login names don't exist anymore (404), which triggers\n            # an error. This needs to be handled in order for the rest of the\n            # queries to continue running.\n            tryCatch(\n              expr = {\n                gh::gh(\"/users/:username\", username = login)\n              },\n              error = \\(unused_argument) {\n                # For consistency with `gh()`, return a list with the keys we\n                # want but with NA values.\n                list(list(login = login, name = NA_character_))\n              }\n            )\n          ),\n          # Some users don't have a name filled out. To keep it from being\n          # empty, use their login name instead.\n          name = pluck(users_json, \"name\", .default = login)\n        ) |>\n        # Rowwise evaluation is no longer needed and I don't want active after\n        # this code chunk.\n        ungroup() |>\n        # Letters with accents, etc. should be normalized so that they match\n        # the normalized names in `cran_authors` from the previous post.\n        mutate(name = str_trim(stri_trans_general(name, \"latin-ascii\"))) |>\n        select(name, login)\n\n      # Once a group is done, rest for an hour to avoid the rate limit.\n      # Since this is our last query, there's a redundant hour of waiting\n      # after the last query group is finished. This is easy to fix, but I'm\n      # only running this code once *ever*, so it's even easier not to.\n      Sys.sleep(3600)\n      \n      query_result\n    }\n  )\n\ncontributors_tidy\n\n#> # A tibble: 15,318 × 2\n#>    name           login         \n#>    <chr>          <chr>         \n#>  1 Spiritspeak    Spiritspeak   \n#>  2 Sigbert Klinke sigbertklinke \n#>  3 Wenjie Wang    wenjie2wang   \n#>  4 Dennis Prangle dennisprangle \n#>  5 Kyle Bittinger kylebittinger \n#>  6 Jin Zhu        Mamba413      \n#>  7 Junhao Huang   oooo26        \n#>  8 Jiang-Kangkang Jiang-Kangkang\n#>  9 Liyuan Hu      zaza0209      \n#> 10 bbayukari      bbayukari     \n#> # … with 15,308 more rows\n\n\n\nTidying GitHub query results\n\n\n\nWith the names associated with each login in contributors_tidy, I can add those to contributors_all.\n\ncontributors_all <- contributors_all |>\n  left_join(contributors_tidy) |>\n  relocate(name, .before = login) |>\n  # Make all names title case to improve the success rates of joining names\n  # with `cran_authors` in the next code chunk.\n  mutate(name = str_to_title(name))\n\ncontributors_all\n\n#> # A tibble: 35,532 × 4\n#>    package    name           login          n_commits\n#>    <chr>      <chr>          <chr>              <int>\n#>  1 AATtools   Spiritspeak    Spiritspeak           76\n#>  2 abbreviate Sigbert Klinke sigbertklinke          4\n#>  3 abclass    Wenjie Wang    wenjie2wang          297\n#>  4 abctools   Dennis Prangle dennisprangle         50\n#>  5 abdiv      Kyle Bittinger kylebittinger        136\n#>  6 abess      Jin Zhu        Mamba413            1113\n#>  7 abess      Junhao Huang   oooo26               816\n#>  8 abess      Jiang-Kangkang Jiang-Kangkang       312\n#>  9 abess      Liyuan Hu      zaza0209             249\n#> 10 abess      Bbayukari      bbayukari             97\n#> # … with 35,522 more rows\n\n\nEven better, since we know the authors of each package from the work I did in the previous post, we can identify which contributors have authorship in the DESCRIPTION of each package. This one is pretty simple: Just use filtering joins to get the subset of contributors with or without a matching package-name combination in cran_authors (using semi_join() and anti_join(), respectively), add an indicator column for package authorship, then join the two data frames back together.\n\n\nAs I discussed in the previous post, there is some degree of measurement error here—so at the person level there will be some errors, but they aren’t bad enough to prevent us from getting an idea of how things are at the population level. There’s also a small bit of new measurement error introduced here, because not every login has a name associated with it, and some people use different names on GitHub and in the DESCRIPTION of the R packages they’re authors on.\n\ncontributors_authors <- contributors_all |>\n  semi_join(cran_authors, by = join_by(package, name == person)) |>\n  mutate(package_author = TRUE)\n\ncontributors_contributors <- contributors_all |>\n  anti_join(cran_authors, by = join_by(package, name == person)) |>\n  mutate(package_author = FALSE)\n\ncontributors_all <- contributors_authors |>\n  bind_rows(contributors_contributors) |>\n  arrange(package) |>\n  # If a package has a single contributor it's reasonable to assume they have\n  # authorship of the package. This fixes some misses in the joining method\n  # where people use different names on GitHub and in the `DESCRIPTION`.\n  group_by(package) |>\n  mutate(package_author = if (n() == 1) TRUE else package_author) |>\n  ungroup()\n\ncontributors_all\n\n#> # A tibble: 35,532 × 5\n#>    package  name                login              n_commits package_author\n#>    <chr>    <chr>               <chr>                  <int> <lgl>         \n#>  1 AATtools Spiritspeak         Spiritspeak               76 TRUE          \n#>  2 ABM      Junling Ma          junlingm                  89 TRUE          \n#>  3 ACDC     Bjorn Tore Kopperud kopperud                 115 TRUE          \n#>  4 ACDC     Sebastian Hohna     hoehna                    27 TRUE          \n#>  5 ACDC     Andy Magee          afmagee                   31 FALSE         \n#>  6 ACDC     Jeremy Andreoletti  Jeremy-Andreoletti         8 FALSE         \n#>  7 ACEP     Agustin Nieto       agusnieto77              218 TRUE          \n#>  8 ACEP     Sismos              HDyCSC                     1 FALSE         \n#>  9 ACNE     Henrik Bengtsson    HenrikBengtsson           55 TRUE          \n#> 10 ACNE     Mortizest           mortizest                  5 FALSE         \n#> # … with 35,522 more rows\n\n\nGoing full circle, we can then update contributors_tidy to include some summary statistics for each contributor, including the number of packages they have contributed to that they have authorship on.\n\ncontributors_tidy <- contributors_all |>\n  group_by(name, login) |>\n  summarise(\n    n_packages = n(),\n    n_commits = sum(n_commits, na.rm = TRUE),\n    n_authorships = sum(package_author, na.rm = TRUE)\n  ) |>\n  ungroup() |>\n  arrange(desc(n_packages), desc(n_commits))\n\ncontributors_tidy\n\n#> # A tibble: 15,318 × 5\n#>    name              login          n_packages n_commits n_authorships\n#>    <chr>             <chr>               <int>     <int>         <int>\n#>  1 Hadley Wickham    hadley                261     31758           108\n#>  2 Jeroen Ooms       jeroen                243     12425            81\n#>  3 Maelle Salmon     maelle                194      2432            32\n#>  4 Jim Hester        jimhester             179     10756            38\n#>  5 Dirk Eddelbuettel eddelbuettel          170     12748            58\n#>  6 Kirill Muller     krlmlr                167     25380            49\n#>  7 Davis Vaughan     DavisVaughan          123      7096            27\n#>  8 Michael Chirico   MichaelChirico        122      1252             9\n#>  9 Romain Francois   romainfrancois        116      7202            21\n#> 10 Gabor Csardi      gaborcsardi           113     14760            72\n#> # … with 15,308 more rows\n\n\nWe can also summarize contributors_all to get package-level summary statistics.\n\npackages_tidy <- contributors_all |>\n  group_by(package) |>\n  summarise(\n    n_contributors = n(),\n    n_authors = sum(package_author, na.rm = TRUE),\n    n_commits = sum(n_commits, na.rm = TRUE)\n  ) |>\n  ungroup() |>\n  arrange(desc(n_contributors), desc(n_commits))\n\npackages_tidy\n\n#> # A tibble: 8,972 × 4\n#>    package  n_contributors n_authors n_commits\n#>    <chr>             <int>     <int>     <int>\n#>  1 mlflow              456         3      3794\n#>  2 xgboost             412         7      5998\n#>  3 arrow               356        10     12493\n#>  4 fs                  315         3      4521\n#>  5 ggplot2             284         9      4993\n#>  6 lightgbm            272         6      3241\n#>  7 dplyr               254         5      7592\n#>  8 mlpack              231        80     23813\n#>  9 duckdb              207         1     24937\n#> 10 h2o                 162        17     23155\n#> # … with 8,962 more rows\n\n\n\n\nTidying graphs\nFinally, we can make a graph from contributors_all to look at collaboration networks between packages and people. This is fairly straightforward to do since contributors_all is already structured as a bipartite graph, where its nodes are divided into two disjoint and independent sets—in this case packages and people. A bipartite projection can then be applied to this graph to get (1) a network of people that have contributed to the same packages, and (2) a network of packages that have contributors in common.\n\n\nNote that both these networks are undirected. For the packages network this makes sense, but the contributors network could theoretically be turned into a directed network based on the authorship status of a contributor. I’ve decided against doing that for this post as it would make the contributors network—which is already large and complex—even larger and more complex.\n\ncontributors_bipartite <- contributors_all |>\n  # The next three steps turn `contributors_all` into a bipartite graph, then\n  # the fourth applies the bipartite projection. This returns a list of two\n  # graphs, hence the use of `map()` in the fifth step.\n  select(package, login) |>\n  table() |>\n  graph_from_incidence_matrix() |>\n  bipartite_projection() |>\n  # The `as_tbl_graph()` function comes from the tidygraph package, which I\n  # explain below.\n  map(as_tbl_graph, directed = FALSE)\n\ncontributors_bipartite\n\n#> $proj1\n#> # A tbl_graph: 8972 nodes and 259674 edges\n#> #\n#> # An undirected simple graph with 1936 components\n#> #\n#> # Node Data: 8,972 × 1 (active)\n#>   name      \n#>   <chr>     \n#> 1 AATtools  \n#> 2 abbreviate\n#> 3 abclass   \n#> 4 abctools  \n#> 5 abdiv     \n#> 6 abess     \n#> # … with 8,966 more rows\n#> #\n#> # Edge Data: 259,674 × 3\n#>    from    to weight\n#>   <int> <int>  <dbl>\n#> 1     2   165      1\n#> 2     2  5601      1\n#> 3     2  6844      1\n#> # … with 259,671 more rows\n#> \n#> $proj2\n#> # A tbl_graph: 15318 nodes and 714831 edges\n#> #\n#> # An undirected simple graph with 1936 components\n#> #\n#> # Node Data: 15,318 × 1 (active)\n#>   name                            \n#>   <chr>                           \n#> 1 001ben                          \n#> 2 0liver0815                      \n#> 3 0tertra                         \n#> 4 0UmfHxcvx5J7JoaOhFSs5mncnisTJJ6q\n#> 5 0x00b1                          \n#> 6 0x26res                         \n#> # … with 15,312 more rows\n#> #\n#> # Edge Data: 714,831 × 3\n#>    from    to weight\n#>   <int> <int>  <dbl>\n#> 1     1   455      1\n#> 2     1   652      1\n#> 3     1   860      1\n#> # … with 714,828 more rows\n\n\nSome of the summary data from before can also be added to the nodes of the projections. Here I’m taking advantage of the tidygraph package so this can be done using the familiar functions from the dplyr package. The workflow is exactly the same, except that the activate() function needs to be called first to declare whether subsequent functions should be applied to the nodes or edges dataframe.\nFirst the contributors network:\n\ncontributors_graph <- contributors_bipartite$proj2 |>\n  activate(nodes) |>\n  rename(login = name) |>\n  left_join(contributors_tidy) |>\n  select(-login) |>\n  # Get some descriptive measures of the nodes for use later.\n  mutate(\n    degree = centrality_degree(),\n    strength = centrality_degree(weights = weight),\n    neighbours = local_ave_degree(weights = weight),\n    component = group_components()\n  )\n\ncontributors_graph\n\n#> # A tbl_graph: 15318 nodes and 714831 edges\n#> #\n#> # An undirected simple graph with 1936 components\n#> #\n#> # Node Data: 15,318 × 8 (active)\n#>   name              n_packages n_commits n_autho… degree streng… neighb… compon…\n#>   <chr>                  <int>     <int>    <int>  <dbl>   <dbl>   <dbl>   <int>\n#> 1 001ben                     2         2        0    109     113   737.        1\n#> 2 Oliver Boix                1        18        0     12      12    61.2       1\n#> 3 Nick Meulemeester          1         4        0      1       1     1       236\n#> 4 Michaja Pehl               5        22        1     26      65    27.6       1\n#> 5 Allen Goodman              1         3        1      5       5     5        26\n#> 6 0x26res                    1         7        0    355     355   415.        1\n#> # … with 15,312 more rows\n#> #\n#> # Edge Data: 714,831 × 3\n#>    from    to weight\n#>   <int> <int>  <dbl>\n#> 1     1   455      1\n#> 2     1   652      1\n#> 3     1   860      1\n#> # … with 714,828 more rows\n\n\nThen the packages network:\n\npackages_graph <- contributors_bipartite$proj1 |>\n  activate(nodes) |>\n  rename(package = name) |>\n  left_join(packages_tidy) |>\n  mutate(\n    degree = centrality_degree(),\n    strength = centrality_degree(weights = weight),\n    neighbours = local_ave_degree(weights = weight),\n    component = group_components()\n  )\n\npackages_graph\n\n#> # A tbl_graph: 8972 nodes and 259674 edges\n#> #\n#> # An undirected simple graph with 1936 components\n#> #\n#> # Node Data: 8,972 × 8 (active)\n#>   package    n_contributors n_authors n_commits degree strength neighbo… compon…\n#>   <chr>               <int>     <int>     <int>  <dbl>    <dbl>    <dbl>   <int>\n#> 1 AATtools                1         1        76      0        0   NaN        415\n#> 2 abbreviate              1         1         4      3        3     3.33       1\n#> 3 abclass                 1         1       297     12       12   214.         1\n#> 4 abctools                1         1        50      1        1     1        154\n#> 5 abdiv                   1         1       136      0        0   NaN        416\n#> 6 abess                  13         3      2675      7        8     6.12       1\n#> # … with 8,966 more rows\n#> #\n#> # Edge Data: 259,674 × 3\n#>    from    to weight\n#>   <int> <int>  <dbl>\n#> 1     2   165      1\n#> 2     2  5601      1\n#> 3     2  6844      1\n#> # … with 259,671 more rows"
  },
  {
    "objectID": "posts/2023-05-10_r-developers-github/index.html#r-contributor-statistics",
    "href": "posts/2023-05-10_r-developers-github/index.html#r-contributor-statistics",
    "title": "We are, we are, on the cruise! We R!",
    "section": "R contributor statistics",
    "text": "R contributor statistics\nPhew, that was… a lot. A lot of wrangling. To summarize, we now have four different objects that cover different aspects about R package contributions on GitHub:\n\ncontributors_tidy: A data frame with summary statistics for each contributor.\npackages_tidy: A data frame with summary statistics for each package.\npackages_graph: A graph of packages that have contributors in common.\ncontributors_graph: A graph of people that have contributed to the same packages.\n\nLike in the previous post, it’s worth reiterating that there is some degree of measurement error here. It’s going to vary by package and person; some of it can be attributable to my code not dealing with certain edge cases well, but other errors are just inherent to doing this analysis in an programmatic way.\nFor example, if we look at which individuals haven’t authored an R package on CRAN, we immediately see three obvious false negatives with Jenny, Daniel, and Will, who are all prolific authors who have most certainly published on CRAN, and whose packages I use regularly. The reason they all have zero authorships is because they use different names on GitHub and in the DESCRIPTION of the R packages they’re authors on.\n\nfilter(contributors_tidy, n_authorships == 0)\n\n#> # A tibble: 9,485 × 5\n#>    name                   login         n_packages n_commits n_authorships\n#>    <chr>                  <chr>              <int>     <int>         <int>\n#>  1 Jennifer (Jenny) Bryan jennybc               87      4877             0\n#>  2 Salim B                salim-b               65       136             0\n#>  3 Daniel                 strengejacke          28     16006             0\n#>  4 The Gitter Badger      gitter-badger         22        23             0\n#>  5 Eitsupi                eitsupi               19       225             0\n#>  6 Max Held               maxheld83             19       159             0\n#>  7 Steven (Siwei) Ye      stevenysw             19        65             0\n#>  8 Bgoodri                bgoodri               18      2417             0\n#>  9 Will Landau            wlandau-lilly         16      3341             0\n#> 10 Jonathan               jmcphers              16       917             0\n#> # … with 9,475 more rows\n\n\nGiven that almost two-thirds of contributors have no authorships, and there’s no way of knowing what proportion of these are true positives or false negatives, I wouldn’t take any stats about authorship too seriously at the individual or population level. I’ve kept these stats in the post because I think they make for a nice demonstration of taking data from different sources to answer new questions; but they’re obviously flawed. Measurement errors like this can be dealt with, but they require manual inspection, which has a high time cost that I wasn’t prepared to pay for this post.\nIt’s also worth a reminder that not every R package on CRAN has a GitHub repository associated with it. Here’s a quick survey of the difference in population sizes. About half the R packages on CRAN have a GitHub repository, and there about half as many GitHub contributors as there are CRAN authors.\n\ntibble(\n  github_packages = nrow(packages_tidy),\n  cran_packages = nrow(distinct(cran_authors, package)),\n  github_contributors = nrow(contributors_tidy),\n  cran_authors = nrow(distinct(cran_authors, person))\n)\n\n#> # A tibble: 1 × 4\n#>   github_packages cran_packages github_contributors cran_authors\n#>             <int>         <int>               <int>        <int>\n#> 1            8972         19432               15318        29384\n\n\nIt’s hard to say exactly how much overlap there is between the people in the GitHub contributors group and the CRAN authors group (given the discussion above), but I’d guess it’s fairly high. For certain, about one-third of contributors are CRAN authors:\n\nlength(base::intersect(contributors_tidy$name, cran_authors$person))\n\n#> [1] 5339\n\n\nAnyways, keeping all that in mind, here’s my exploration of R package contributions in the wider R community.\n\nContributor statistics\nSimilar to my previous post, we can look at some basic person-level stats—this time the number of R packages a person has contributed to and their total commits across packages. Funnily enough, the distribution for the number of R packages a person has contributed to is very similar to the distribution of CRAN package authorships we saw in my previous post. Thus, around 90% have contributed to three packages, and only around 10% have contributed to four or more packages.\n\ncontributors_tidy |>\n  mutate(\n    n_pkgs_fct = case_when(\n      n_packages == 1 ~ \"One\",\n      n_packages == 2 ~ \"Two\",\n      n_packages == 3 ~ \"Three\",\n      n_packages >= 4 ~ \"Four+\"\n    ),\n    n_pkgs_fct = factor(n_pkgs_fct, levels = c(\"One\", \"Two\", \"Three\", \"Four+\"))\n  ) |>\n  ggplot(aes(x = n_pkgs_fct)) +\n    geom_bar() +\n    scale_y_continuous(\n      sec.axis = sec_axis(\n        trans = \\(.x) .x / nrow(contributors_tidy),\n        name = \"Percent of sample\",\n        labels = label_percent(),\n        breaks = c(0, .05, .10, .15, .70)\n      )\n    ) +\n    labs(\n      x = \"Package contributions\",\n      y = \"People\"\n    )\n\n\n\n\nLikewise, around 54% of people have only made 10 or less commits across the packages they’ve contributed to, and around 80% have made 100 or less.\n\nn_less_than_10 <- nrow(contributors_tidy[contributors_tidy$n_commits <= 10,])\nn_less_than_100 <- nrow(contributors_tidy[contributors_tidy$n_commits <= 100,])\n\ncontributors_tidy |>\n  summarise(\n    n_people = n(),\n    percent_less_than_10 = n_less_than_10 / n_people * 100,\n    percent_less_than_100 = n_less_than_100 / n_people * 100\n  )\n\n#> # A tibble: 1 × 3\n#>   n_people percent_less_than_10 percent_less_than_100\n#>      <int>                <dbl>                 <dbl>\n#> 1    15318                 54.2                  79.4\n\n\nFor both these subgroups, we can also see that the majority of people in them do not have any CRAN package authorships; but that as the number of commits increases so does the proportion of people who have CRAN package authorship. Moreover, from the histogram below we see that almost 25% have only made a single commit.\n\nggplot(contributors_tidy, aes(x = n_commits, fill = n_authorships > 0)) +\n  geom_histogram() +\n  scale_x_log10() +\n  scale_y_continuous(\n    sec.axis = sec_axis(\n      trans = \\(.x) .x / nrow(contributors_tidy),\n      name = \"Percent of sample\",\n      labels = label_percent()\n    )\n  ) +\n  labs(\n    x = \"Commits across packages\",\n    y = \"People\",\n    fill = \"CRAN authorship\"\n  )\n\n\n\n\nThe number of R packages a person has contributed to and their total commits across packages are obviously related (e.g., every new package a person contributes to is a new commit), so it makes sense to look at these together too. To avoid overplotting, I’ve used a hex bin plot instead of scatterplot; and to give an idea of the distributions for each variable I’ve added a jittered rug to both axes. There’s a high amount of variability here, but around 76% of people have made 100 or less commits across one to three packages.\n\ncontributors_tidy |>\n  ggplot(aes(x = n_packages, y = n_commits)) +\n    annotate(\n      \"text\",\n      x = 32,\n      y = 8,\n      hjust = 0,\n      # I calculated this value using the same methods as above, but hard coded\n      # it here just to avoid some ugly code.\n      label = \"Around 76% of people\\nare in the shaded region.\"\n    ) +\n    annotate(\n      \"curve\",\n      x = 64, xend = 3.5,\n      y = 3, yend = 1.5,\n      curvature = -0.1,\n      arrow = arrow(type = \"closed\")\n    ) +\n    annotate(\n      \"rect\",\n      xmin = 0.9, xmax = 3.45,\n      ymin = 0.8, ymax = 100,\n      alpha = 0.5\n    ) +\n    # Note that geom_hex was borked in ggplot2 v3.4.0, so make sure to have\n    # v3.4.1 or later if you use this in your own projects. See:\n    # https://ggplot2.tidyverse.org/news/index.html#bug-fixes-3-4-1\n    geom_hex(bins = 30) +\n    geom_rug(\n      alpha = 0.01, position = position_jitter(width = .1, height = .1)\n    ) +\n    scale_x_continuous(\n      trans = \"log2\", breaks = breaks_log(n = 7, base = 2)\n    ) +\n    scale_y_continuous(\n      trans = \"log10\"\n    ) +\n    scale_fill_viridis_c(\n      trans = \"log10\"\n    ) +\n    labs(\n      x = \"Packages contributed to\",\n      y = \"Commits across packages\",\n      fill = \"Count\"\n    ) +\n    theme(panel.grid.minor = element_blank())\n\n\n\n\nThe previous plot used log scales to avoid almost all the data being compressed in the bottom-left corner. However, this makes the true scale of the data less visually intuitive, so here’s a scatterplot of this data without any scaling. I’ve shaded the 76% of people region here too (this time in yellow; it’s very small in the bottom-left corner), which makes it clear that the number of package contributions and commits made by a typical contributor are orders of magnitude smaller than those of the most prolific contributors.\n\n\nI’ve also labelled some of the most prolific people and added the number of CRAN package authorships each person has to the colour scale (since it wasn’t be used otherwise). For the authorships, I’ve coloured them grey if the number of authorships was zero. I also stopped the highest bin at 30 or more authorships, since that seemed like a good cut point for this data.\n\ncontributors_tidy |>\n  mutate(n_authorships = if_else(n_authorships == 0, NA_integer_, n_authorships)) |>\n  ggplot(aes(x = n_packages, y = n_commits, colour = n_authorships)) +\n    geom_point(size = 0.5) +\n    geom_text_repel(\n      aes(label = name),\n      data = filter(contributors_tidy, n_packages >= 100 | n_commits >= 10000),\n      min.segment.length = 0,\n      max.overlaps = Inf\n    ) +\n    annotate(\n      \"rect\",\n      xmin = 1, xmax = 3,\n      ymin = 1, ymax = 100,\n      fill = \"#FDE725\", colour = \"#FDE725\"\n    ) +\n    # This is a slightly hack-ish way to provide a manual fill to a binned\n    # colour scale: https://stackoverflow.com/a/67273672/16844576.\n    binned_scale(\n      aesthetics = \"colour\",\n      scale_name = \"coloursteps\",\n      palette = function(x) pal_ramp(viridis_palettes$viridis, 6),\n      na.value = \"darkgrey\",\n      limits = c(1, max(contributors_tidy$n_authorships)),\n      breaks = c(1, 3, 10, 20, 30),\n      guide = \"coloursteps\"\n    ) +\n    labs(\n      x = \"Packages contributed to\",\n      y = \"Commits across packages\",\n      colour = \"Package\\nauthorships\"\n    )\n\n\n\n\nJust as we found last time with CRAN package authorships, the vast range in package contributions and commits is mostly driven by people who do R package development as part of their job, which can be seen by browsing through the top entries in the interactive gt table below.\n\n\nCode\ncontributors_tidy |>\n  select(-login) |>\n  gt() |>\n  tab_header(\n    title = \"R Developer Contributions\",\n    subtitle = \"CRAN Package Contributions on GitHub\"\n  ) |>\n  text_transform(\n    \\(.x) str_to_title(str_replace_all(.x, \"_\", \" \")),\n    locations = cells_column_labels()\n  ) |>\n  opt_interactive(use_sorting = FALSE, use_filters = TRUE)\n\n\n\n\n\n\nR Developer Contributions\nCRAN Package Contributions on GitHub\n\n\n\n\n\n\n\n\nContributor collaboration networks\nThe graph of 15318 contributors that have contributed to the same packages is disconnected, meaning that it can be broken down into a number of subgraphs (called components) that are completely isolated from one another. This is the opposite of a connected graph, where every node is reachable from every other.\n\nis_connected(contributors_graph)\n\n#> [1] FALSE\n\n\nA census of the components shows that the contributor collaboration network is dominated by one giant component that contains around 80% of the contributors in the network, which indicates that the vast majority of people can be reached through common package contributions. However, around 8% of the nodes in the network consist of single contributors that don’t share common package contributions with any other contributors, meaning that there’s also a decent chunk of solo package authors who haven’t published or contributed to any other CRAN packages, or had anyone contribute to their package.\n\ncontributors_graph |>\n  as_tibble() |>\n  group_by(component) |>\n  summarise(n = n()) |>\n  group_by(n_contributors = n) |>\n  summarise(n_components = n()) |>\n  ungroup() |>\n  mutate(\n    percent_of_network = n_components * n_contributors / vcount(contributors_graph)\n  ) |>\n  arrange(desc(percent_of_network))\n\n#> # A tibble: 16 × 3\n#>    n_contributors n_components percent_of_network\n#>             <int>        <int>              <dbl>\n#>  1          12270            1           0.801   \n#>  2              1         1349           0.0881  \n#>  3              2          352           0.0460  \n#>  4              3          121           0.0237  \n#>  5              4           52           0.0136  \n#>  6              5           29           0.00947 \n#>  7              7            7           0.00320 \n#>  8              6            8           0.00313 \n#>  9              9            5           0.00294 \n#> 10             10            3           0.00196 \n#> 11             15            2           0.00196 \n#> 12             11            2           0.00144 \n#> 13              8            2           0.00104 \n#> 14             14            1           0.000914\n#> 15             13            1           0.000849\n#> 16             12            1           0.000783\n\n\nGiven that the network is dominated by one giant component, and the next largest set of components don’t have any connections at all, it makes sense to focus on the giant component.\n\n# Components are ordered by size, so the largest component is the first one.\ncontributors_graph <- filter(contributors_graph, component == 1)\n\nHere’s a plot of the giant component. It looks like a big hairball, which makes it hard to fully interpret, but we can at least see some structure. For example, it’s clear that not all contributors are connected, that there is a wide range in the number of common package contributions that exist between people (based on the node sizes here), and that there are possibly some natural communities of people who tend to contribute to the same packages.\n\ncontributors_graph |>\n  ggraph() +\n    # It's important to use geom_edge_link0() instead of geom_edge_link();\n    # otherwise plotting takes forever since the latter draws 100 points\n    # along each edge so it they be used make edges with gradients.\n    geom_edge_link0(\n      aes(edge_width = weight),\n      colour = \"grey\",\n      alpha = 0.5\n    ) + \n    geom_node_point(\n      aes(size = strength),\n      fill = \"white\",\n      shape = 21\n    ) +\n    theme_graph() +\n    theme(legend.position = \"none\")\n\n\n\n\nWe can use descriptive statistics to augment our understanding of the graph beyond what can be learned by plotting it.\nFirst let’s look at the distributions of degree and strength. Here the degree of each contributor represents the total number of neighbours a contributor has; and the strength of each node represents the total number of common packages a person and their neighbours have contributed to.\nThe (log) degree range is quite wide, and it makes it clear that although most contributors have relatively few neighbours, there is also a segment that have a lot of neighbours.\n\ncontributors_graph |>\n  as_tibble() |>\n  ggplot(aes(x = degree)) +\n    geom_histogram() +\n    scale_x_log10()\n\n\n\n\nThe (log) strength range is much greater than the (log) degree range, but the distributions themselves look similar.\n\ncontributors_graph |>\n  as_tibble() |>\n  ggplot(aes(x = strength)) +\n    geom_histogram() +\n    scale_x_log10()\n\n\n\n\nTo augment the strength distribution above, the scatterplot below shows the strength of each node plotted against the average strength of the neighbours of each node. This plot suggests that there is a tendency for contributors with higher strengths to link with similar contributors, while contributors with lower strengths tend to link with contributors of both lower and higher strengths.\n\ncontributors_graph |>\n  as_tibble() |>\n  ggplot(aes(x = strength, y = neighbours)) +\n    geom_point(alpha = 0.05) +\n    scale_x_continuous(trans = \"log10\") +\n    scale_y_continuous(trans = \"log10\") +\n    labs(\n      x = \"Node strength\",\n      y = \"Average neighbour strength\"\n    )\n\n\n\n\nAnother way to make the giant component easier to interpret is to to try to detect communities of contributors who have contributed to the same packages, then contract and simplify the graph so that each node summarizes a community and each edge summarizes community interactions. There are a lot of algorithms available to do this; here I’ve chosen the Louvain algorithm, which was created in part to help visualize the structure of large networks using this communities as nodes approach.\n\ncontributors_communities_graph <- contributors_graph |>\n  mutate(community = group_louvain()) |>\n  convert(to_contracted, community, simplify = TRUE) |>\n  mutate(n_nodes = unlist(map(.tidygraph_node_index, length))) |>\n  select(-c(.orig_data, .tidygraph_node_index)) |>\n  activate(edges) |>\n  mutate(weight = unlist(map(.tidygraph_edge_index, length))) |>\n  select(-c(.orig_data, .tidygraph_edge_index)) |>\n  activate(nodes)\n\ncontributors_communities_graph\n\n#> # A tbl_graph: 139 nodes and 269 edges\n#> #\n#> # An undirected simple graph with 1 component\n#> #\n#> # Node Data: 139 × 2 (active)\n#>   community n_nodes\n#>       <int>   <int>\n#> 1         1    5531\n#> 2        12     256\n#> 3        28      26\n#> 4         6     401\n#> 5         2     843\n#> 6        17     117\n#> # … with 133 more rows\n#> #\n#> # Edge Data: 269 × 3\n#>    from    to weight\n#>   <int> <int>  <int>\n#> 1     1     2    953\n#> 2     1     3     28\n#> 3     1     4   9088\n#> # … with 266 more rows\n\n\nThis is now much easier to visualize. Here we can see there is one giant community with thousands of contributors in it that has connections to every other community, several large communities with hundreds of contributors in them that have relatively high connections with the giant community, and many more communities with less than 100 contributors whose members have relatively low amounts of common package contributions with other communities.\n\ncontributors_communities_graph |>\n  ggraph(layout = \"stress\") +\n    geom_edge_link0(\n      aes(edge_width = weight),\n      colour = \"grey\",\n      alpha = 0.5\n    ) + \n    geom_node_point(\n      aes(fill = n_nodes, size = n_nodes),\n      shape = 21\n    ) +\n    geom_node_text(\n      aes(label = n_nodes, filter = n_nodes >= 100)\n    ) +\n    scale_fill_viridis_c(begin = 0.5, option = \"G\") +\n    scale_size(range = c(0.5, 25)) +\n    theme_graph() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\nPackage statistics\nThe summary statistics for packages are similar to those for people, so I’ll follow a similar exploration workflow here. Starting with the number of contributors to the 8972 packages with GitHub repositories, it’s apparent that around 45% of CRAN packages are single author packages with no contributors, and that around 82% of packages have one to four contributors.\n\npackages_tidy |>\n  mutate(\n    n_ctbs_fct = case_when(\n      n_contributors == 1 ~ \"One\",\n      n_contributors == 2 ~ \"Two\",\n      n_contributors == 3 ~ \"Three\",\n      n_contributors == 4 ~ \"Four\",\n      n_contributors >= 5 ~ \"Five+\",\n    ),\n    n_ctbs_fct = factor(\n      n_ctbs_fct, levels = c(\"One\", \"Two\", \"Three\", \"Four\", \"Five+\")\n    )\n  ) |>\n  ggplot(aes(x = n_ctbs_fct)) +\n    geom_bar() +\n    scale_y_continuous(\n      sec.axis = sec_axis(\n        trans = \\(.x) .x / nrow(packages_tidy),\n        name = \"Percent of sample\",\n        labels = label_percent(),\n        breaks = c(0, .05, .10, .15, .20, .45)\n      )\n    ) +\n    labs(\n      x = \"Contributors\",\n      y = \"Count\"\n    )\n\n\n\n\nIt’s also apparent that as the number of contributors increases, the proportion of packages whose only contributors are their authors decreases. This isn’t surprising at all—but I will emphasize that the exact proportions shown here aren’t accurate due to the measurement error between people’s names on GitHub and DESCRIPTION I discussed earlier. I’ve added a tolerance of one to try to account for this, so the proportions here slightly favour false positives over false negatives, but to what extent it’s hard to say.\n\npackages_tidy |>\n  ggplot(aes(x = n_contributors, fill = n_contributors - 1 <= n_authors)) +\n    geom_histogram() +\n    scale_x_log10() +\n    scale_y_continuous(\n      sec.axis = sec_axis(\n        trans = \\(.x) .x / nrow(packages_tidy),\n        name = \"Percent of sample\",\n        labels = label_percent(),\n        breaks = c(0, .05, .10, .15, .20, .45)\n      )\n    ) +\n    labs(\n      x = \"Contributors\",\n      y = \"Count\",\n      fill = \"All contributors\\nare authors\"\n    )\n\n\n\n\nThe number of contributors an R package has and the total commits to that package are obviously related (e.g., every new contributor is a new commit), so it makes sense to look at these together too. I’ve used a hex bin plot again to avoid overplotting, with jittered rugs to give an idea of the distributions for each variable. This one’s particularly aesthetically pleasing, with two salient points of interest: First, package commits follow a log-normal distribution; and second, although there’s a high amount of variability, around 75% of packages have between one and four contributors, with 10 to 1000 commits shared between them.\n\npackages_tidy |>\n  # Around 35 packages have 0 commits, so they shouldn't be included here.\n  # I tried following up on these and some of them led to 404 pages, which is\n  # weird because those should have been skipped when getting data from the\n  # GitHub REST API. Some others seemed to be caused by people changing their\n  # usernames. \n  filter(n_commits != 0) |>\n  ggplot(aes(x = n_contributors, y = n_commits)) +\n    annotate(\n      \"text\",\n      x = 32,\n      y = 8,\n      hjust = 0,\n      label = \"Around 75% of people\\nare in the shaded region.\"\n    ) +\n    annotate(\n      \"curve\",\n      x = 64, xend = 4,\n      y = 3, yend = 6,\n      curvature = -0.35,\n      arrow = arrow(type = \"closed\")\n    ) +\n    annotate(\n      \"rect\",\n      xmin = 0.9, xmax = 4,\n      ymin = 10, ymax = 1000,\n      alpha = 0.5\n    ) +\n    geom_hex(bins = 30) +\n    geom_rug(\n      alpha = 0.01, position = position_jitter(width = .1, height = .1)\n    ) +\n    scale_x_continuous(\n      trans = \"log2\", breaks = breaks_log(n = 7, base = 2)\n    ) +\n    scale_y_continuous(trans = \"log10\") +\n    scale_fill_viridis_c(trans = \"log2\") +\n    labs(\n      x = \"Contributors\",\n      y = \"Commits\",\n      fill = \"Count\"\n    ) +\n    theme(panel.grid.minor = element_blank())\n\n\n\n\nAnd here’s the same thing as a scatterplot on the original scale. Unsurprisingly, the packages with the most activity here are largely made by Posit, with a few community favourites as well like brms. Notably, the extreme outliers here—like duckdb and xgboost—are so far apart from everything else because they are cross-language tools that use the monorepo philosophy for project management, so their GitHub activity goes beyond the confines of the R community.\n\npackages_tidy |>\n  ggplot(aes(x = n_contributors, y = n_commits, colour = n_authors)) +\n    geom_point(alpha = 0.1) +\n    geom_text_repel(\n      aes(label = package),\n      data = filter(packages_tidy, n_contributors >= 100 | n_commits >= 5000),\n      min.segment.length = 0,\n      max.overlaps = Inf,\n      force = 3\n    ) +\n    scale_x_continuous(limits = c(0, NA)) +\n    scale_colour_viridis_c(\n      trans = \"log2\", na.value = \"grey\", end = 0.9\n    ) +\n    labs(\n      x = \"Contributors\",\n      y = \"Commits\",\n      colour = \"Authors\"\n    )\n\n\n\n\nFinally, so you can explore the packages_tidy data yourself, here’s another interactive gt table.\n\n\nCode\npackages_tidy |>\n  gt() |>\n  tab_header(\n    title = \"R Package Contributions\",\n    subtitle = \"CRAN Package Activity on GitHub\"\n  ) |>\n  text_transform(\n    \\(.x) str_to_title(str_replace_all(.x, \"_\", \" \")),\n    locations = cells_column_labels()\n  ) |>\n  opt_interactive(use_sorting = FALSE, use_filters = TRUE)\n\n\n\n\n\n\nR Package Contributions\nCRAN Package Activity on GitHub\n\n\n\n\n\n\n\n\nPackage collaboration networks\nThe graph of 8972 packages that have contributors in common is also disconnected, and can be broken down into a number of components that are completely isolated from one another.\n\nis_connected(packages_graph)\n\n#> [1] FALSE\n\n\nA census of the components shows that the package collaboration network is dominated by one giant component that contains around 70% of the packages in the network, which indicates that the vast majority of packages can be reached through common contributors. However, around 17% of the nodes in the network consist of single packages that don’t share common contributors with any other packages, meaning that there’s also a good chunk of the network whose package contributor(s) haven’t engaged with the wider R community at all.\n\npackages_graph |>\n  as_tibble() |>\n  group_by(component) |>\n  summarise(n = n()) |>\n  group_by(n_packages = n) |>\n  summarise(n_components = n()) |>\n  ungroup() |>\n  mutate(\n    percent_of_network = n_components * n_packages / vcount(packages_graph)\n  ) |>\n  arrange(desc(percent_of_network))\n\n#> # A tibble: 15 × 3\n#>    n_packages n_components percent_of_network\n#>         <int>        <int>              <dbl>\n#>  1       6265            1            0.698  \n#>  2          1         1522            0.170  \n#>  3          2          261            0.0582 \n#>  4          3           85            0.0284 \n#>  5          4           30            0.0134 \n#>  6          5           12            0.00669\n#>  7          7            6            0.00468\n#>  8          6            6            0.00401\n#>  9          9            4            0.00401\n#> 10          8            4            0.00357\n#> 11         22            1            0.00245\n#> 12         17            1            0.00189\n#> 13         16            1            0.00178\n#> 14         15            1            0.00167\n#> 15         12            1            0.00134\n\n\nGiven that the network is dominated by one giant component, and the next largest set of components don’t have any connections at all, it makes sense to focus on the giant component.\n\n# Components are ordered by size, so the largest component is the first one.\npackages_graph <- filter(packages_graph, component == 1)\n\nHere’s a plot of the giant component. It looks like another hairball, but we can still see some structure. For example, it’s clear that not all packages are connected, and that there is a wide range in the number of common contributors that exist between packages (based on the node sizes here).\n\npackages_graph |>\n  ggraph() +\n    # It's important to use geom_edge_link0() instead of geom_edge_link();\n    # otherwise plotting takes forever since the latter draws 100 points\n    # along each edge so it they be used make edges with gradients.\n    geom_edge_link0(\n      aes(edge_width = weight),\n      colour = \"grey\",\n      alpha = 0.5\n    ) + \n    geom_node_point(\n      aes(size = strength),\n      fill = \"white\",\n      shape = 21\n    ) +\n    theme_graph() +\n    theme(legend.position = \"none\")\n\n\n\n\nWe can use descriptive statistics to augment our understanding of the graph beyond what can be learned by plotting it.\nFirst let’s look at the distributions of degree and strength. Here the degree of each package represents the total number of neighbours a package has; and the strength of each node represents the total number of common contributors a package has with all its neighbours.\nThe (log) degree range is quite wide, and it looks like the distribution has two or three peaks.\n\npackages_graph |>\n  as_tibble() |>\n  ggplot(aes(x = degree)) +\n    geom_histogram() +\n    scale_x_log10()\n\n\n\n\nThe (log) strength range is about twice as wide as the (log) degree range (it’s hard to tell on the log scale; this statement is based on a check before the transformation), but it still looks like there could be two or three peaks here.\n\npackages_graph |>\n  as_tibble() |>\n  ggplot(aes(x = strength)) +\n    geom_histogram() +\n    scale_x_log10()\n\n\n\n\nTo augment the strength distribution above, the scatterplot below shows the strength of each node plotted against the average strength of the neighbours of each node. This plot suggests that there is a tendency for packages with higher strengths to link with similar packages, while packages with lower strengths tend to link with packages of both lower and higher strengths.\n\npackages_graph |>\n  as_tibble() |>\n  ggplot(aes(x = strength, y = neighbours)) +\n    geom_point(alpha = 0.05) +\n    scale_x_continuous(trans = \"log10\") +\n    scale_y_continuous(trans = \"log10\") +\n    labs(\n      x = \"Node strength\",\n      y = \"Average neighbour strength\"\n    )\n\n\n\n\nFinally, we can use community detection to contract and simplify the graph of the giant component.\n\npackages_communities_graph <- packages_graph |>\n  mutate(community = group_louvain()) |>\n  convert(to_contracted, community, simplify = TRUE) |>\n  mutate(n_nodes = unlist(map(.tidygraph_node_index, length))) |>\n  select(-c(.orig_data, .tidygraph_node_index)) |>\n  activate(edges) |>\n  mutate(weight = unlist(map(.tidygraph_edge_index, length))) |>\n  select(-c(.orig_data, .tidygraph_edge_index)) |>\n  activate(nodes)\n\npackages_communities_graph\n\n#> # A tbl_graph: 41 nodes and 172 edges\n#> #\n#> # An undirected simple graph with 1 component\n#> #\n#> # Node Data: 41 × 2 (active)\n#>   community n_nodes\n#>       <int>   <int>\n#> 1        38       4\n#> 2        17      24\n#> 3        14      63\n#> 4         4     599\n#> 5         9     419\n#> 6         7     536\n#> # … with 35 more rows\n#> #\n#> # Edge Data: 172 × 3\n#>    from    to weight\n#>   <int> <int>  <int>\n#> 1     1     4      1\n#> 2     2     4     13\n#> 3     2     9      8\n#> # … with 169 more rows\n\n\nHere we can see there are several large communities with hundreds of packages in them that have relatively high amounts of common contributors, and many more communities with less than 100 packages that have relatively low amounts of common contributors.\n\npackages_communities_graph |>\n  ggraph(layout = \"stress\") +\n    geom_edge_link0(\n      aes(edge_width = weight),\n      colour = \"grey\",\n      alpha = 0.5\n    ) + \n    geom_node_point(\n      aes(fill = n_nodes, size = n_nodes),\n      shape = 21\n    ) +\n    geom_node_text(\n      aes(label = n_nodes, filter = n_nodes >= 100)\n    ) +\n    scale_fill_viridis_c(begin = 0.5, option = \"G\") +\n    scale_size(range = c(0.5, 15)) +\n    theme_graph() +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/2023-05-10_r-developers-github/index.html#conclusion",
    "href": "posts/2023-05-10_r-developers-github/index.html#conclusion",
    "title": "We are, we are, on the cruise! We R!",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\nIt turns out there’s a lot you can do with a simple data set like this by taking advantage of summary statistics and a bit of graph theory—and this post only scratched the surface. Some other interesting explorations include looking at networks for tidyverse packages, the difference in commits between the top and bottom contributors of R packages, and so forth. I’ve included a copy of the contributors_all data in the appendix below in case you want to explore this data yourself.\nI don’t have a grand unifying conclusion for this one, but I do have some thoughts. R is a great language. I think it’s currently the best language for doing data science, and will continue to be in the years to come. The R Core Team’s recent improvements to R have been great, with new additions like the native pipe (|>), the new function shorthand (\\()), and raw strings (r\"()\"); and as I covered in my series on reproducible data science, the number of R packages published on CRAN each year continues to grow at a steady rate, and most existing packages are under active development. We also see signs of R’s growth in its increased popularity for scientific training and research, in industry leaders shifting their data science stacks to R, and in the continuing success of companies like Posit. So R isn’t going away any time soon.\nHowever, as we’ve discovered from this post and the previous one, the community that has made R so great on the software development side is quite small in the grand scheme of things, with maybe (if we’re being generous) around 5% of all R users being directly involved in R package development,1 and even less of us being involved in open-source package development. We’ve also seen that, although there’s a lot of collaboration happening in the R package development space, around 41% of all CRAN packages have a single author,2 around 45% of open-source CRAN packages have a single author and no contributors, and around 8% of open-source R package authors have not collaborated with anyone at all (whether through making contributions to others’ packages, or accepting contributions to their own packages). So a lot of people develop R packages (mostly) alone, many of whom are scientists developing software for their own research.\nThere isn’t anything inherently wrong with working alone or in a small team—it might even be preferable in many cases—but it makes me wonder whether these developers have plans for how their packages will be maintained after they stop working on them. Do they have someone to pass on the torch to? Is it sufficient that the source code is available (whether by the author or a GitHub CRAN mirror), so a motivated individual can fork the project and elect themselves the new maintainer? If the latter is sufficient, what about for R packages without an open license? One of the great things about R is that our community has created packages for almost anything you can imagine, even if it’s some weird niche thing that doesn’t seem like it has a practical use case (until you work on a project where this weird niche thing is the exact solution to your problem). It would be a shame if those we lost those packages in the future.\nSimilarly, this made wonder about the sustainability of the R language itself. The number of people who contribute to R is considerably smaller than the number of people who contribute to R packages—including the 26 current and former members of the R Core Team, only 148 people have contributed to the R language (and for anyone wondering, no you don’t need to count everyone by hand; just use strsplit() on the acknowledgement paragraphs and get the length of the vector). Fortunately, there are initiatives in place to keep development of R sustainable (and more diverse and inclusive!), which Heather Turner and Ella Kaye gave a nice talk on recently.\nAnyways, just some food for thought. What do you think?\nIf you enjoyed learning about what’s going on in the R package development space, I found some related work in the course of writing that you might also like. In no particular order:\n\nA First Survey on the Diversity of the R Community\nMapping useRs\nRStudio R Community Survey\nThe next million R users\nThe Impressive Growth of R\nCode longevity of the R programming language"
  },
  {
    "objectID": "posts/2023-05-10_r-developers-github/index.html#section",
    "href": "posts/2023-05-10_r-developers-github/index.html#section",
    "title": "We are, we are, on the cruise! We R!",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "posts/2023-05-10_r-developers-github/index.html#michael-mccarthy",
    "href": "posts/2023-05-10_r-developers-github/index.html#michael-mccarthy",
    "title": "We are, we are, on the cruise! We R!",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "posts/2023-05-10_r-developers-github/index.html#comments",
    "href": "posts/2023-05-10_r-developers-github/index.html#comments",
    "title": "We are, we are, on the cruise! We R!",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/2023-05-10_r-developers-github/index.html#session-info",
    "href": "posts/2023-05-10_r-developers-github/index.html#session-info",
    "title": "We are, we are, on the cruise! We R!",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2023-05-20\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.475 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n dplyr        * 1.1.0   2023-01-29 [1] CRAN (R 4.2.0)\n forcats      * 0.5.2   2022-08-19 [1] CRAN (R 4.2.0)\n ggplot2      * 3.4.2   2023-04-03 [1] CRAN (R 4.2.0)\n ggraph       * 2.1.0   2022-10-09 [1] CRAN (R 4.2.0)\n ggrepel      * 0.9.3   2023-02-03 [1] CRAN (R 4.2.0)\n gh           * 1.3.1   2022-09-08 [1] CRAN (R 4.2.0)\n graphlayouts * 1.0.0   2023-05-01 [1] CRAN (R 4.2.0)\n gt           * 0.9.0   2023-03-31 [1] CRAN (R 4.2.0)\n igraph       * 1.4.2   2023-04-07 [1] CRAN (R 4.2.0)\n palettes     * 0.1.0   2022-12-19 [1] CRAN (R 4.2.0)\n purrr        * 0.3.5   2022-10-06 [1] CRAN (R 4.2.0)\n readr        * 2.1.3   2022-10-01 [1] CRAN (R 4.2.0)\n scales       * 1.2.1   2022-08-20 [1] CRAN (R 4.2.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi      * 1.7.8   2022-07-11 [1] CRAN (R 4.2.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.2.0)\n tibble       * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidygraph    * 1.2.3   2023-02-01 [1] CRAN (R 4.2.0)\n tidyr        * 1.2.1   2022-09-08 [1] CRAN (R 4.2.0)\n tidyverse    * 1.3.2   2022-07-18 [1] CRAN (R 4.2.0)\n\n [1] /Users/Michael/Library/R/x86_64/4.2/library/__tidytales\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-05-10_r-developers-github/index.html#data",
    "href": "posts/2023-05-10_r-developers-github/index.html#data",
    "title": "We are, we are, on the cruise! We R!",
    "section": "Data",
    "text": "Data\n\nDownload the data used in this post."
  },
  {
    "objectID": "posts/2021-06-19_distill/index.html",
    "href": "posts/2021-06-19_distill/index.html",
    "title": "What’s he building in there?",
    "section": "",
    "text": "Note: This blog is no longer built with Distill. It is now built with Quarto, which is in many ways a successor to Distill."
  },
  {
    "objectID": "posts/2021-06-19_distill/index.html#overview",
    "href": "posts/2021-06-19_distill/index.html#overview",
    "title": "What’s he building in there?",
    "section": "Overview",
    "text": "Overview\nTidy Tales is built using Distill for R Markdown, a web publishing format optimized for scientific and technical communication. I chose Distill for Tidy Tales over other R website formats for its simplicity and style. My personal website uses blogdown, a more customizable but also more complicated format, which I did not want or need for Tidy Tales. That said, I did encounter some restrictions that I wanted to overcome while customizing Tidy Tales using the tools provided by Distill. This post covers some tips and tricks to get around these restrictions."
  },
  {
    "objectID": "posts/2021-06-19_distill/index.html#page-and-article-metadata",
    "href": "posts/2021-06-19_distill/index.html#page-and-article-metadata",
    "title": "What’s he building in there?",
    "section": "Page and Article Metadata",
    "text": "Page and Article Metadata\nDistill comes equipped with a number of features to automatically enable richer sharing of article links on the web using article metadata. However, some of these features are not available for non-article pages on distill blogs (such as link preview images for the home page), and the automatic behaviour of these features limits how much they can be customized. Both of these limitations can be overcome using the metathis package by Garrick Aden-Buie.\nTom Mock has a great blog post diving into how metadata can be used to customize how links from a distill blog appear on social media. It’s a great resource and I followed it to add metadata and preview images to the home and about pages of Tidy Tales.\nHere is what the index.Rmd file for the Tidy Tales home page looks like.\n---\ntitle: \"Wrangling, Visualizing, Modelling, Communicating data\"\nsite: distill::distill_website\nlisting: posts\n---\n\n```{r, include=FALSE, results='asis'}\nlibrary(metathis)\n\nmeta() %>%\n  meta_social(\n    title = \"Tidy Tales\",\n    description = \"Wrangling, Visualizing, Modelling, Communicating data\",\n    url = \"https://tidytales.ca\",\n    image = \"https://tidytales.ca/inst/images/twittercard.png\",\n    image_alt = \"Tidy Tales logo\",\n    og_type = \"website\",\n    twitter_card_type = \"summary\",\n    twitter_site = NULL\n  )\n```\nWhen the site is built distill will automatically generate metadata for the home page, and the metathis code in index.Rmd will generate additional metadata for the home page. Here is what it looks like in HTML.\n<!-- Generated by distill -->\n<meta property=\"og:title\" content=\"Tidy Tales | Michael McCarthy: Wrangling, Visualizing, Modelling, Communicating data\">\n<meta property=\"og:type\" content=\"article\">\n<meta property=\"og:locale\" content=\"en_US\">\n<meta property=\"og:site_name\" content=\"Tidy Tales | Michael McCarthy\">\n<meta property=\"twitter:card\" content=\"summary\">\n<meta property=\"twitter:title\" content=\"Tidy Tales | Michael McCarthy: Wrangling, Visualizing, Modelling, Communicating data\">\n<meta property=\"twitter:site\" content=\"@propertidytales\">\n<meta property=\"twitter:creator\" content=\"@mccarthymg\">\n\n<!-- Generated by metathis -->\n<meta property=\"og:locale\" content=\"en_US\">\n<meta name=\"twitter:title\" content=\"Tidy Tales\">\n<meta name=\"twitter:description\" content=\"Wrangling, Visualizing, Modelling, Communicating data\">\n<meta name=\"twitter:url\" content=\"https://tidytales.ca\">\n<meta name=\"twitter:image:src\" content=\"https://tidytales.ca/inst/images/twittercard.png\">\n<meta name=\"twitter:image:alt\" content=\"Tidy Tales logo\">\n<meta name=\"twitter:card\" content=\"summary\">\n<meta property=\"og:title\" content=\"Tidy Tales\">\n<meta property=\"og:description\" content=\"Wrangling, Visualizing, Modelling, Communicating data\">\n<meta property=\"og:url\" content=\"https://tidytales.ca\">\n<meta property=\"og:image\" content=\"https://tidytales.ca/inst/images/twittercard.png\">\n<meta property=\"og:image:alt\" content=\"Tidy Tales logo\">\n<meta property=\"og:type\" content=\"website\">\nThere is some overlap between the <meta> tags generated by distill and metathis, however, the metadata tags generated by metathis seem to take precedence over those automatically generated by distill. For example, the Twitter card for the Tidy Tales home page displays “Tidy Tales” as its title, rather than “Tidy Tales | Michael McCarthy: Wrangling, Visualizing, Modelling, Communicating data”.\n\nArticle Metadata\nThe ability to override some of the metadata generated by distill using metathis is hacky, but it also affords more customization for distill blogs. One trick I’m taking full advantage of with this is to have unique preview images between posts on Tidy Tales and their social cards. Distill allows you to specify a preview image for a post using the preview chunk option.\n```{r, preview=TRUE}\nlibrary(ggplot2)\nggplot(diamonds, aes(carat, price)) +\n  geom_smooth()\n```\nThis preview image will be used alongside post listings and in social cards. However, if a different image is specified in metathis::meta_social() that image will be used in social cards but the preview image specified in the post chunk will still be used alongside post listings. I’m using this on Tidy Tales to have branded images for social cards and plain images for post listings. Here’s an example of the branded social card image from my first post.\n\n\n\n\n\nThe branded social card image for my first post. Copy the post link into a tweet to see it in action."
  },
  {
    "objectID": "posts/2021-06-19_distill/index.html#utterances-comments",
    "href": "posts/2021-06-19_distill/index.html#utterances-comments",
    "title": "What’s he building in there?",
    "section": "Utterances Comments",
    "text": "Utterances Comments\nDistill only supports Disqus comments officially. I did not want to use Disqus comments on Tidy Tales because it would add bloat to my posts, and because I do not want a third-party data mining and tracking Tidy Tales readers. Utterances is a lightweight alternative that uses GitHub issues for comments. Miles McBain shared an HTML script on his blog to add Utterances to a distill blog.\nHere is what the script for Tidy Tales looks like.\n<script>\n document.addEventListener(\"DOMContentLoaded\", function () {\n   if (!/posts/.test(location.pathname)) {\n     return;\n   }\n\n   var script = document.createElement(\"script\");\n   script.src = \"https://utteranc.es/client.js\";\n   script.setAttribute(\"repo\", \"mccarthy-m-g/tidytales\");\n   script.setAttribute(\"issue-term\", \"title\");\n   script.setAttribute(\"crossorigin\", \"anonymous\");\n   script.setAttribute(\"label\", \"utterances\");\n   script.setAttribute(\"theme\", \"github-light\");\n\n   /* wait for article to load, append script to article element */\n   var observer = new MutationObserver(function (mutations, observer) {\n     var article = document.querySelector(\"details.comment-section\");\n     if (article) {\n       observer.disconnect();\n       /* HACK: article scroll */\n       article.setAttribute(\"style\", \"overflow-y: hidden\");\n       article.appendChild(script);\n     }\n   });\n\n   observer.observe(document.body, { childList: true });\n });\n</script>\nThe script uses JavaScript to inject the Utterances <iframe> into the end of the first HTML Element within the document that matches the CSS selector specified in document.querySelector(). By default, the script shared by Miles will place the comment section at the end of a distill post’s body. Since Utterances comments sections are not collapsible this presents a problem though, as more comments are made readers will have to scroll further and further to reach a post’s appendix.\nTo overcome this on Tidy Tales I created new CSS selectors that use the <details> tag, so readers can show and hide the comments section as they please, and added a brightness and opacity filter to the selector for the Utterances <iframe> to make it fit into the Tidy Tales colour scheme better. I also wanted my comments section to be in the appendix of my posts rather than the body.\nd-appendix details.comment-section {\n  color: var(--dark-shade-alpha);\n  font-family: var(--heading-font);\n  font-size: 15px !important;\n}\n\nd-appendix details.comment-section summary:after {\n  content: \"Show\";\n}\n\nd-appendix details[open].comment-section summary:after {\n  content: \"Hide\";\n}\n\n.utterances {\n  filter: brightness(95%) opacity(85%);\n}\nThe above HTML and CSS is applied to all Tidy Tales posts using the theme and includes parameters in _site.yml, so to add Utterances to a post I only need to include the following in the R Markdown file for a post as an appendix header.\n## Comments {.appendix}\n\n<details open class=\"comment-section\">\n   <summary>\n   </summary>\n</details>"
  },
  {
    "objectID": "posts/2021-06-19_distill/index.html#post-templates",
    "href": "posts/2021-06-19_distill/index.html#post-templates",
    "title": "What’s he building in there?",
    "section": "Post Templates",
    "text": "Post Templates\nFinally, and most importantly, I am using an R Markdown template for new Tidy Tales posts so I don’t need to copy and paste all of my customizations into the YAML and body of every post I create. This is easily done using the create_post_from_template() function in the distilltools package by Ella Kaye. And it will be even easier after the Pull Request I’m working on with Ella adds an RStudio addin for creating new posts from a template to the package.\n\n\nElla also created the awesome Distill Club hex sticker I used for this post’s preview image.\n\n\n\n\n\nPreview of the new post from template RStudio addin for distilltools."
  },
  {
    "objectID": "posts/2021-06-19_distill/index.html#community-tips-and-tricks",
    "href": "posts/2021-06-19_distill/index.html#community-tips-and-tricks",
    "title": "What’s he building in there?",
    "section": "Community Tips and Tricks",
    "text": "Community Tips and Tricks\nDistill has a great community of users supporting each other to build and customize their sites. John Paul Helveston has put together the Distillery—a distill blog about building distill websites and blogs—to collect tips and tricks from the community and to showcase their distill websites and blogs. Check it out if you’re thinking of joining the Distill Club!"
  },
  {
    "objectID": "posts/2021-06-19_distill/index.html#section",
    "href": "posts/2021-06-19_distill/index.html#section",
    "title": "What’s he building in there?",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "posts/2021-06-19_distill/index.html#michael-mccarthy",
    "href": "posts/2021-06-19_distill/index.html#michael-mccarthy",
    "title": "What’s he building in there?",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "posts/2021-06-19_distill/index.html#comments",
    "href": "posts/2021-06-19_distill/index.html#comments",
    "title": "What’s he building in there?",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/2021-06-19_distill/index.html#session-info",
    "href": "posts/2021-06-19_distill/index.html#session-info",
    "title": "What’s he building in there?",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2022-12-21\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.280 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n here        * 1.0.1   2020-12-13 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n\n [1] /Users/Michael/Library/R/x86_64/4.2/library/__tidytales\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html",
    "href": "posts/2022-09-29_borderlands/index.html",
    "title": "Tales from the Borderlands",
    "section": "",
    "text": "Borderlands is a an action role-playing first-person looter shooter video game franchise set in a space western science-fiction universe. The games have a dramatic comic book art style that I want to capture in my plot.\n\n\n\n\n\nIn-game screenshot from Borderlands 3.\n\n\n\n\nGearbox, the developers of Borderlands, have explained that this art style is achieved using “hand-drawn textures, scanned in and coloured in Photoshop, combined with software that draws graphic novel-style outlines around characters and objects, sharpens shadows to look more like something an artist might create, and even draws lines on hills and inclines. Finally the character models are all revamped with more exaggerated proportions, creating the appearance of a detailed comic book in motion.”\nSome of these are not relevant to plotting, but two are:\n\nDrawing graphic novel-style outlines\nUsing hand-drawn textures\n\n\n\nThe in-game menus in Borderlands 3 provide a great design reference for plot theming.\n\n\n\n\n\nThe ECHO-3 in-game menu in Borderlands 3.\n\n\n\n\n\n\n\n\n\n\n\nLoot in Borderlands (guns, grenades, shields, etc.) are colour categorized by the rarity with which they can be found in containers or dropped by defeated enemies. From left to right the categories are: Common, Uncommon, Rare, Epic, Legendary.\n\n\n\n\nI want to translate these design elements to my plot like so:\n\nThe Compacta Bold font can be used for plot text.\nThe blue background and light blue UI highlights can be used for the plot background and axes, respectively.\nThe white header text and the blue text can be used for different textual elements of the plot.\nThe yellow colour can be used for the plot title (this is the colour used for the game’s titles).\nThe loot rarity colours can be used for grouping data in plot geoms.\nAll elements should have a black outline.\n\nApplying these elements to my plot will help it fit the Borderlands aesthetic."
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html#prerequisites",
    "href": "posts/2022-09-29_borderlands/index.html#prerequisites",
    "title": "Tales from the Borderlands",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(lubridate)\nlibrary(magick)\nlibrary(ggdist)\n\nI’ll be using Steam player data for my plot. The data contains statistics for the average and peak number of players playing a variety of games each month from July 2012 to February 2022. You can download this data with the Data Source code in the appendix, or from Tidy Tuesday with tidytuesdayR::tt_load(\"2021-03-16\").\n\n# Load the weekly data\ngames <- read_csv(here(\"data\", \"2021-03-16_games.csv\"))\ngames"
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html#wrangle",
    "href": "posts/2022-09-29_borderlands/index.html#wrangle",
    "title": "Tales from the Borderlands",
    "section": "Wrangle",
    "text": "Wrangle\nI only want data from the mainline Borderlands titles for my plot, so let’s get those.\n\n# Filter to mainline Borderlands titles available in the data. The first game\n# is not available in the dataset so filtering based on the title and digit\n# works fine here.\nborderlands <- games %>%\n  filter(str_detect(gamename, \"Borderlands[[:space:]][[:digit:]]\"))\n\nborderlands\n\n\n\n  \n\n\n\nNow to explore the data.\n\n# Summarize how much data exists for each Borderlands title\nborderlands %>%\n  group_by(gamename) %>%\n  summarise(count = n())\n\n\n\n  \n\n\n\nBorderlands 2 was released on Steam in September 2012 and Borderlands 3 was released in March 2020, which explains the discrepancy in how much data exists between the two. One way to make them more comparable is to filter the Borderlands 2 data down to only its first year of release.\n\n# Wrangle date data into a date-time object to prepare for filtering\nborderlands <- borderlands %>%\n  mutate(date = glue(\"{year}-{month}\"),\n         date = parse_date_time(date, \"ym\"),\n         .after = gamename)\n\n# Filter Borderlands 2 data down to only its first year of release to make\n# comparisons with Borderlands 3 more appropriate. There is no need to filter\n# by date for Borderlands 3 since only its first year of data are available in\n# the dataset.\nborderlands <- borderlands %>%\n  filter(gamename == \"Borderlands 2\" &\n         date %within% interval(ymd(\"2012--09-01\"), ymd(\"2013--08-01\")) |\n         gamename == \"Borderlands 3\") \n\nborderlands\n\n\n\n  \n\n\n\nNow there is monthly data for the first year of release for each game. I want to compare how the two games performed against each other in their first year. This will give some insight on how the player stats changed over time within and between the games. Creating a new variable counting the number of months since release is a clean way to do this. I could also stick with nominal months, but using a count variable will make the comparison between the games more apparent in my plot.\n\n# This code is sufficient since the data is in reverse chronological order.\nborderlands <- borderlands %>%\n  group_by(gamename) %>%\n  mutate(since_release = 11:0, .after = month)\n\nborderlands\n\n\n\n  \n\n\n\nFinally, I need to decide how to relate the five rarity colours from Figure @ref(fig:loot-rarity) to the player stats for the first year of release for each game. Since there are five levels, cutoffs based on quantiles could work.\n\nborderlands %>%\n  summarise(quantile = quantile(peak))\n\n\n\n  \n\n\n\nRather than following the quantiles exactly, I’ve picked some cutoffs that look like they would work well for both games. An alternative approach would be to assign cutoffs per game, in which case the exact quantiles could be used.\n\nborderlands <- borderlands %>%\n  mutate(rarity = case_when(\n    between(peak, 0, 19999) ~ \"white\",\n    between(peak, 20000, 39999) ~ \"green\",\n    between(peak, 40000, 59999) ~ \"blue\",\n    between(peak, 60000, 79999) ~ \"purple\",\n    between(peak, 80000, 150000) ~ \"orange\"\n  ))"
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html#visualize",
    "href": "posts/2022-09-29_borderlands/index.html#visualize",
    "title": "Tales from the Borderlands",
    "section": "Visualize",
    "text": "Visualize\nThere are two obvious ways to visualize this data: A time series line graph, or a bar graph. I’m going to use a bar graph, mainly so I can group the bars using the loot rarity colours I mentioned earlier. A line graph would be a better choice for communication though.\nThe ggplot2 package doesn’t support outlines for the plot elements such as titles, axis lines, or strips—only some plot geoms support outlines. Because of this, I need to create two plots: An outline plot, and a coloured plot. Then I can combine the two plots with the magick package to create the outline effect.\nThe outline plot will look like this. Nothing too exciting.\n\noutline_plot <- ggplot(borderlands, aes(since_release, peak)) +\n  facet_wrap(vars(gamename)) +\n  labs(\n    x = \"Months Since Release\",\n    y = \"Peak Player Count\",\n    title = \"Peak players in Borderlands drop faster\\nthan common loot\",\n    caption = \"Source: Steam / Graphic: Michael McCarthy\"\n  ) +\n  theme_bw() +\n  theme(\n    text = element_text(family = \"Compacta Bold\", colour = \"black\"),\n    axis.text = element_text(colour = \"black\"),\n    axis.line = element_blank(),\n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(colour = \"black\", fill = NA),\n    strip.background = element_rect(fill = \"white\", colour = \"black\")\n  )\n\noutline_plot\n\n\n\n\nThe coloured plot is the same thing… but with colour. I’m adding a pink background here so the white text is visible, and so there is a distinct colour I can detect to make the background transparent later.\n\nblue <- \"#08283c\"\nlight_blue <- \"#115190\"\nbaby_blue <- \"#a7e5ff\"\nindigo <- \"#cef8ff\"\n\ncolour_plot <- outline_plot +\n  theme(\n    text = element_text(colour = \"white\"),\n    # Axis\n    axis.text = element_text(colour = indigo),\n    axis.ticks = element_line(colour = light_blue),\n    # Panel\n    panel.grid.major.y = element_line(colour = light_blue),\n    panel.grid.minor.y = element_line(colour = light_blue),\n    panel.border = element_rect(colour = light_blue, fill = NA),\n    panel.background = element_rect(fill = blue, colour = light_blue),\n    # Plot\n    plot.title = element_text(colour = \"#fff01a\"),\n    plot.background = element_rect(fill = \"pink\"),\n    # Strip\n    strip.text = element_text(colour = baby_blue),\n    strip.background = element_rect(fill = \"#00378f\", colour = light_blue)\n  )\n\ncolour_plot\n\n\n\n\nNow for some image magic! First I need to turn the outline ggplot into an image to prepare for post-processing. I’ll save the image to a temporary file since it’s only an intermediate step. Note here that the image is really big, and I’ve scaled up the sizes of plot elements accordingly; this is needed to make the outlines look good, since the post-processing involves detecting the edges of elements.\n\n# Create plot used for the outline\nfile <- tempfile(fileext = '.png')\nragg::agg_png(file, width = 1920, height = 1200, res = 300, units = \"px\", scaling = 0.5)\noutline_plot +\n  geom_col(fill = \"white\") +\n  stat_ccdfinterval(fill = \"white\", point_alpha = 0) +\n  theme(\n    # Axis\n    axis.title = element_text(size = 36),\n    axis.text = element_text(size = 28),\n    axis.text.x = element_text(margin = margin(5, 0, 5, 0, \"pt\")),\n    axis.text.y = element_text(margin = margin(0, 5, 0, 5, \"pt\")),\n    axis.line = element_line(size = 0),\n    axis.ticks = element_line(size = 2),\n    axis.ticks.length = unit(10, \"pt\"),\n    # Panel\n    panel.border = element_rect(size = 0),\n    panel.background = element_rect(colour = \"black\", size = 5),\n    panel.spacing = unit(3, \"lines\"),\n    # Plot\n    plot.title = element_text(size = 56),\n    plot.margin = unit(c(40, 40, 40, 40), \"pt\"),\n    # Strip\n    strip.text = element_text(size = 36, margin = margin(0.5,0,0.5,0, \"cm\")),\n    strip.background = element_rect(size = 5),\n    # Caption\n    plot.caption = element_text(size = 24)\n  )\ninvisible(dev.off())\n\nFor the actual post-processing, I detect the edges of all the plot elements, then dilate them outwards. Finally the white areas in the plot are made transparent, so all that’s left is the black outlines. To demonstrate, I’ve created a blank white image here and flattened the outline plot on top of it.\n\nplot_outline_layer <- image_read(file) %>%\n  image_convert(type=\"Grayscale\") %>%\n  image_negate() %>%\n  image_threshold(\"white\", \"5%\") %>%\n  image_morphology('EdgeOut', \"Diamond\", iterations = 6) %>%\n  image_morphology('Dilate', \"Diamond\", iterations = 1) %>%\n  image_negate() %>%\n  image_transparent(\"white\", fuzz = 7)\n\nimage_flatten(c(image_blank(1920, 1200, color = \"white\"), plot_outline_layer))\n\n\n\n\nNext the colour plot, which just needs to be scaled up with the bars added to it, then saved to a temporary file. Here I’ve used CCDF bars with a gradient, courtesy of the ggdist package, going from black to colour to match the gradients in the ECHO-3 in-game menu in Borderlands 3. It’s a bit tacky, and there isn’t an easy way to add gradients to any other plot elements, but it fits the theme.\n\nfile <- tempfile(fileext = '.png')\nragg::agg_png(file, width = 1920, height = 1200, res = 300, units = \"px\", scaling = 0.5)\ncolour_plot +\n  # First a solid fill column\n  geom_col(aes(fill = rarity)) +\n  # Then use a ccdfinterval to create a vertical gradient over top the solid\n  # fill\n  stat_ccdfinterval(\n    aes(fill = rarity, fill_ramp = stat(y)),\n    fill_type = \"gradient\",\n    show.legend = FALSE,\n    point_alpha = 0\n  ) +\n  scale_fill_identity() +\n  scale_fill_ramp_continuous(\n    from = \"black\",\n    range = c(0.8, 1),\n    limits = c(0, 15000)\n  ) +\n  expand_limits(y = 0) +\n  # Finally add a black outline over top of everything\n  geom_col(fill = NA, colour = \"black\", size = 1) +\n  theme(\n    # Axis\n    axis.title = element_text(size = 36),\n    axis.text = element_text(size = 28),\n    axis.text.x = element_text(margin = margin(5, 0, 5, 0, \"pt\")),\n    axis.text.y = element_text(margin = margin(0, 5, 0, 5, \"pt\")),\n    axis.line = element_line(size = 0),\n    axis.ticks = element_line(size = 2),\n    axis.ticks.length = unit(10, \"pt\"),\n    # Panel\n    panel.border = element_rect(size = 0),\n    panel.background = element_rect(size = 5),\n    panel.spacing = unit(3, \"lines\"),\n    # Plot\n    plot.title = element_text(size = 56),\n    plot.margin = unit(c(40, 40, 40, 40), \"pt\"),\n    plot.background = element_rect(fill = \"pink\"),\n    # Strip\n    strip.text = element_text(size = 36, margin = margin(0.5,0,0.5,0, \"cm\")),\n    strip.background = element_rect(size = 5),\n    # Caption\n    plot.caption = element_text(size = 24)\n  )\ninvisible(dev.off())\n\nplot_fill_layer <- image_read(file)\n\nplot_fill_layer\n\n\n\n\nFinally, the outline and fill layers can be combined, and the background made transparent. I think the outline effect is actually pretty convincing.\n\nplot_layer <- image_composite(plot_fill_layer, plot_outline_layer) %>% \n  image_transparent(\"pink\", fuzz = 7)\n\nplot_layer\n\n\n\n\nAnd the background image can be added for the final composite. To make it stand out less, I’ve overlaid a solid black frame with 50% opacity.\n\nbackground_layer <- image_read(\n  here(\"posts\", \"2022-09-29_borderlands\", \"images\", \"plot-background.png\")\n) %>% \n  image_colorize(50, \"black\")\n\nfinal_graphic <- image_composite(background_layer, plot_layer)\n\nThis plot isn’t going to win any awards (unless it’s for an ugly plots contest), but it does show that you can do some pretty cool programmatic image processing of your plots (or any other images) with the magick package."
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html#final-graphic",
    "href": "posts/2022-09-29_borderlands/index.html#final-graphic",
    "title": "Tales from the Borderlands",
    "section": "Final Graphic",
    "text": "Final Graphic"
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html#section",
    "href": "posts/2022-09-29_borderlands/index.html#section",
    "title": "Tales from the Borderlands",
    "section": "",
    "text": "Michael McCarthy\n\nThanks for reading! I’m Michael, the voice behind Tidy Tales. I am an award winning data scientist and R programmer with the skills and experience to help you solve the problems you care about. You can learn more about me, my consulting services, and my other projects on my personal website."
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html#michael-mccarthy",
    "href": "posts/2022-09-29_borderlands/index.html#michael-mccarthy",
    "title": "Tales from the Borderlands",
    "section": "Michael McCarthy",
    "text": "Michael McCarthy"
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html#comments",
    "href": "posts/2022-09-29_borderlands/index.html#comments",
    "title": "Tales from the Borderlands",
    "section": "Comments",
    "text": "Comments"
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html#session-info",
    "href": "posts/2022-09-29_borderlands/index.html#session-info",
    "title": "Tales from the Borderlands",
    "section": "Session Info",
    "text": "Session Info\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Mojave 10.14.6\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Vancouver\n date     2022-12-21\n pandoc   2.14.0.3 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)\n quarto   1.2.280 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.0.10  2022-09-01 [1] CRAN (R 4.2.0)\n forcats     * 0.5.2   2022-08-19 [1] CRAN (R 4.2.0)\n ggdist      * 3.2.0   2022-07-19 [1] CRAN (R 4.2.0)\n ggplot2     * 3.4.0   2022-11-04 [1] CRAN (R 4.2.0)\n glue        * 1.6.2   2022-02-24 [1] CRAN (R 4.2.0)\n here        * 1.0.1   2020-12-13 [1] CRAN (R 4.2.0)\n lubridate   * 1.9.0   2022-11-06 [1] CRAN (R 4.2.0)\n magick      * 2.7.3   2021-08-18 [1] CRAN (R 4.2.0)\n purrr       * 0.3.5   2022-10-06 [1] CRAN (R 4.2.0)\n readr       * 2.1.3   2022-10-01 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.2.0)\n tibble      * 3.1.8   2022-07-22 [1] CRAN (R 4.2.0)\n tidyr       * 1.2.1   2022-09-08 [1] CRAN (R 4.2.0)\n tidyverse   * 1.3.2   2022-07-18 [1] CRAN (R 4.2.0)\n timechange  * 0.1.1   2022-11-04 [1] CRAN (R 4.2.0)\n\n [1] /Users/Michael/Library/R/x86_64/4.2/library/__tidytales\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html#data",
    "href": "posts/2022-09-29_borderlands/index.html#data",
    "title": "Tales from the Borderlands",
    "section": "Data",
    "text": "Data\n\nDownload the data used in this post."
  },
  {
    "objectID": "posts/2022-09-29_borderlands/index.html#fair-dealing",
    "href": "posts/2022-09-29_borderlands/index.html#fair-dealing",
    "title": "Tales from the Borderlands",
    "section": "Fair Dealing",
    "text": "Fair Dealing\n\nAny of the trademarks, service marks, collective marks, design rights or similar rights that are mentioned, used, or cited in this article are the property of their respective owners. They are used here as fair dealing for the purpose of education in accordance with section 29 of the Copyright Act and do not infringe copyright."
  }
]